{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aee8de2b",
   "metadata": {},
   "source": [
    "### In this notebook we perform federated learning\n",
    "\n",
    "In federated learning each base station has access only to it's private dataset, however they collaborate together to train a model that has satifactory results on data from any other base station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a3a97e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "parent = Path(os.path.abspath(\"\")).resolve().parents[0]\n",
    "if parent not in sys.path:\n",
    "    sys.path.insert(0, str(parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "eed1269d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import random\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c1c1ca7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.utils.data_utils import read_data, generate_time_lags, time_to_feature, handle_nans, to_Xy, \\\n",
    "    to_torch_dataset, to_timeseries_rep, assign_statistics, \\\n",
    "    to_train_val, scale_features, get_data_by_area, remove_identifiers, get_exogenous_data_by_area, handle_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f79e4b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.utils.train_utils import train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a7c779c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.models.mlp import MLP\n",
    "from ml.models.rnn import RNN\n",
    "from ml.models.lstm import LSTM\n",
    "from ml.models.gru import GRU\n",
    "from ml.models.cnn import CNN\n",
    "from ml.models.rnn_autoencoder import DualAttentionAutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "67d7baf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.fl.defaults import create_regression_client\n",
    "from ml.fl.client_proxy import SimpleClientProxy\n",
    "from ml.fl.server.server import Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c4606af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    data_path='../dataset/full_dataset.csv', # dataset\n",
    "    data_path_test=['../dataset/upstream_test.csv'], # test dataset\n",
    "    test_size=0.3, # validation size\n",
    "    targets=['temp','pH','DissolvedOxygen','Conductivity'], # the target\n",
    "    num_lags=10, # the number of past observations to feed as input\n",
    "\n",
    "    identifier='District', # the column name that identifies a bs\n",
    "\n",
    "    nan_constant=0, # the constant to transform nan values\n",
    "    x_scaler='minmax', # x_scaler\n",
    "    y_scaler='minmax', # y_scaler\n",
    "    outlier_detection=None, # whether to perform flooring and capping\n",
    "\n",
    "    \n",
    "    criterion='mse', # optimization criterion, mse or l1\n",
    "    fl_rounds=5, # the number of federated rounds\n",
    "    fraction=1., # the percentage of available client to consider for random selection\n",
    "    aggregation=\"avg\", # federated aggregation algorithm\n",
    "    epochs=3, # the number of maximum local epochs\n",
    "    lr=0.001, # learning rate\n",
    "    optimizer='adam', # the optimizer, it can be sgd or adam\n",
    "    batch_size=128, # the batch size to use\n",
    "    local_early_stopping=False, # whether to use early stopping\n",
    "    local_patience=50, # patience value for the early stopping parameter (if specified)\n",
    "    max_grad_norm=0.0, # whether to clip grad norm\n",
    "    reg1=0.0, # l1 regularization\n",
    "    reg2=0.0, # l2 regularization\n",
    "\n",
    "    cuda=True, # whether to use gpu\n",
    "    \n",
    "    seed=0, # reproducibility\n",
    "\n",
    "    assign_stats=None, # whether to use statistics as exogenous data, [\"mean\", \"median\", \"std\", \"variance\", \"kurtosis\", \"skew\"]\n",
    "    use_time_features=False # whether to use datetime features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "727c20ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script arguments: Namespace(aggregation='avg', assign_stats=None, batch_size=128, criterion='mse', cuda=True, data_path='../dataset/full_dataset.csv', data_path_test=['../dataset/upstream_test.csv'], epochs=3, fl_rounds=5, fraction=1.0, identifier='District', local_early_stopping=False, local_patience=50, lr=0.001, max_grad_norm=0.0, nan_constant=0, num_lags=10, optimizer='adam', outlier_detection=None, reg1=0.0, reg2=0.0, seed=0, targets=['temp', 'pH', 'DissolvedOxygen', 'Conductivity'], test_size=0.3, use_time_features=False, x_scaler='minmax', y_scaler='minmax')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Script arguments: {args}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d36c3d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if args.cuda and torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "28dbb608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection specification\n",
    "if args.outlier_detection is not None:\n",
    "    outlier_columns = ['Conductivity', 'Turbidity', 'pH', 'DissolvedOxygen']\n",
    "    outlier_kwargs = {\"upstream\": (10, 90), \"midstream\": (10, 90), \"downstream\": (5, 95)}\n",
    "    args.outlier_columns = outlier_columns\n",
    "    args.outlier_kwargs = outlier_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c7015c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_all():\n",
    "    # ensure reproducibility\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "581869c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3a510771",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "022d3623",
   "metadata": {},
   "source": [
    "### The pre-processing method is almost equivalent to centralized learning. The only difference is that the scaling operations are performed individually on each base station. In contrast, in centralized learning the scaling is performed by considering the combined data from all base stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1b304bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preprocessing():\n",
    "    \"\"\"Preprocess a given .csv\"\"\"\n",
    "    # read data\n",
    "    df = read_data(args.data_path)\n",
    "    # handle nans\n",
    "    df = handle_nans(train_data=df, constant=args.nan_constant,\n",
    "                     identifier=args.identifier)\n",
    "    # split to train/validation\n",
    "    train_data, val_data = to_train_val(df)\n",
    "    \n",
    "    # handle outliers (if specified)\n",
    "    if args.outlier_detection is not None:\n",
    "        train_data = handle_outliers(df=train_data, columns=args.outlier_columns,\n",
    "                                     identifier=args.identifier, kwargs=args.outlier_kwargs)\n",
    "    \n",
    "    # get X and y\n",
    "    X_train, X_val, y_train, y_val = to_Xy(train_data=train_data, val_data=val_data,\n",
    "                                          targets=args.targets)\n",
    "    \n",
    "    # scale X\n",
    "    X_train, X_val, x_scalers = scale_features(train_data=X_train, val_data=X_val,\n",
    "                                              scaler=args.x_scaler,\n",
    "                                              per_area=True, # the features are scaled locally\n",
    "                                              identifier=args.identifier)\n",
    "    # scale y\n",
    "    y_train, y_val, y_scalers = scale_features(train_data=y_train, val_data=y_val,\n",
    "                                              scaler=args.y_scaler, \n",
    "                                              per_area=True,\n",
    "                                              identifier=args.identifier)\n",
    "    \n",
    "    # generate time lags\n",
    "    X_train = generate_time_lags(X_train, args.num_lags)\n",
    "    X_val = generate_time_lags(X_val, args.num_lags)\n",
    "    y_train = generate_time_lags(y_train, args.num_lags, is_y=True)\n",
    "    y_val = generate_time_lags(y_val, args.num_lags, is_y=True)\n",
    "    \n",
    "    # get datetime features as exogenous data\n",
    "    date_time_df_train = time_to_feature(\n",
    "        X_train, args.use_time_features, identifier=args.identifier\n",
    "    )\n",
    "    date_time_df_val = time_to_feature(\n",
    "        X_val, args.use_time_features, identifier=args.identifier\n",
    "    )\n",
    "    \n",
    "    # get statistics as exogenous data\n",
    "    stats_df_train = assign_statistics(X_train, args.assign_stats, args.num_lags,\n",
    "                                       targets=args.targets, identifier=args.identifier)\n",
    "    stats_df_val = assign_statistics(X_val, args.assign_stats, args.num_lags, \n",
    "                                       targets=args.targets, identifier=args.identifier)\n",
    "    \n",
    "    # concat the exogenous features (if any) to a single dataframe\n",
    "    if date_time_df_train is not None or stats_df_train is not None:\n",
    "        exogenous_data_train = pd.concat([date_time_df_train, stats_df_train], axis=1)\n",
    "        # remove duplicate columns (if any)\n",
    "        exogenous_data_train = exogenous_data_train.loc[:, ~exogenous_data_train.columns.duplicated()].copy()\n",
    "        assert len(exogenous_data_train) == len(X_train) == len(y_train)\n",
    "    else:\n",
    "        exogenous_data_train = None\n",
    "    if date_time_df_val is not None or stats_df_val is not None:\n",
    "        exogenous_data_val = pd.concat([date_time_df_val, stats_df_val], axis=1)\n",
    "        exogenous_data_val = exogenous_data_val.loc[:, ~exogenous_data_val.columns.duplicated()].copy()\n",
    "        assert len(exogenous_data_val) == len(X_val) == len(y_val)\n",
    "    else:\n",
    "        exogenous_data_val = None\n",
    "        \n",
    "    return X_train, X_val, y_train, y_val, exogenous_data_train, exogenous_data_val, x_scalers, y_scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ad15f79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO logger 2024-06-02 23:30:33,416 | data_utils.py:383 | Observations info in upstream\n",
      "INFO logger 2024-06-02 23:30:33,417 | data_utils.py:384 | \tTotal number of samples:  4863\n",
      "INFO logger 2024-06-02 23:30:33,417 | data_utils.py:385 | \tNumber of samples for training: 3891\n",
      "INFO logger 2024-06-02 23:30:33,418 | data_utils.py:386 | \tNumber of samples for validation:  972\n",
      "INFO logger 2024-06-02 23:30:33,421 | data_utils.py:383 | Observations info in midstream\n",
      "INFO logger 2024-06-02 23:30:33,421 | data_utils.py:384 | \tTotal number of samples:  4930\n",
      "INFO logger 2024-06-02 23:30:33,422 | data_utils.py:385 | \tNumber of samples for training: 3944\n",
      "INFO logger 2024-06-02 23:30:33,422 | data_utils.py:386 | \tNumber of samples for validation:  986\n",
      "INFO logger 2024-06-02 23:30:33,424 | data_utils.py:383 | Observations info in downstream\n",
      "INFO logger 2024-06-02 23:30:33,424 | data_utils.py:384 | \tTotal number of samples:  4920\n",
      "INFO logger 2024-06-02 23:30:33,424 | data_utils.py:385 | \tNumber of samples for training: 3936\n",
      "INFO logger 2024-06-02 23:30:33,425 | data_utils.py:386 | \tNumber of samples for validation:  984\n",
      "INFO logger 2024-06-02 23:30:33,427 | data_utils.py:389 | Observations info using all data\n",
      "INFO logger 2024-06-02 23:30:33,428 | data_utils.py:390 | \tTotal number of samples:  14713\n",
      "INFO logger 2024-06-02 23:30:33,428 | data_utils.py:391 | \tNumber of samples for training: 11771\n",
      "INFO logger 2024-06-02 23:30:33,428 | data_utils.py:392 | \tNumber of samples for validation:  2942\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val, exogenous_data_train, exogenous_data_val, x_scalers, y_scalers = make_preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bf0982ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TotalNitrogen_lag-10</th>\n",
       "      <th>TotalPhosphorus_lag-10</th>\n",
       "      <th>AmmoniaNitrogen_lag-10</th>\n",
       "      <th>PermanganateIndex_lag-10</th>\n",
       "      <th>Turbidity_lag-10</th>\n",
       "      <th>Conductivity_lag-10</th>\n",
       "      <th>DissolvedOxygen_lag-10</th>\n",
       "      <th>pH_lag-10</th>\n",
       "      <th>temp_lag-10</th>\n",
       "      <th>TotalNitrogen_lag-9</th>\n",
       "      <th>...</th>\n",
       "      <th>TotalNitrogen_lag-1</th>\n",
       "      <th>TotalPhosphorus_lag-1</th>\n",
       "      <th>AmmoniaNitrogen_lag-1</th>\n",
       "      <th>PermanganateIndex_lag-1</th>\n",
       "      <th>Turbidity_lag-1</th>\n",
       "      <th>Conductivity_lag-1</th>\n",
       "      <th>DissolvedOxygen_lag-1</th>\n",
       "      <th>pH_lag-1</th>\n",
       "      <th>temp_lag-1</th>\n",
       "      <th>District</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-11-10 16:00:00</th>\n",
       "      <td>0.053548</td>\n",
       "      <td>0.487918</td>\n",
       "      <td>0.454747</td>\n",
       "      <td>0.202132</td>\n",
       "      <td>0.004535</td>\n",
       "      <td>0.185411</td>\n",
       "      <td>0.314404</td>\n",
       "      <td>0.227700</td>\n",
       "      <td>0.671883</td>\n",
       "      <td>0.053548</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054376</td>\n",
       "      <td>0.490246</td>\n",
       "      <td>0.469831</td>\n",
       "      <td>0.184912</td>\n",
       "      <td>0.007406</td>\n",
       "      <td>0.187004</td>\n",
       "      <td>0.316469</td>\n",
       "      <td>0.225352</td>\n",
       "      <td>0.661090</td>\n",
       "      <td>upstream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-10 20:00:00</th>\n",
       "      <td>0.053548</td>\n",
       "      <td>0.487918</td>\n",
       "      <td>0.454747</td>\n",
       "      <td>0.202132</td>\n",
       "      <td>0.005683</td>\n",
       "      <td>0.185266</td>\n",
       "      <td>0.278265</td>\n",
       "      <td>0.194836</td>\n",
       "      <td>0.663788</td>\n",
       "      <td>0.052996</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052996</td>\n",
       "      <td>0.490246</td>\n",
       "      <td>0.463620</td>\n",
       "      <td>0.191472</td>\n",
       "      <td>0.005454</td>\n",
       "      <td>0.189756</td>\n",
       "      <td>0.352091</td>\n",
       "      <td>0.262911</td>\n",
       "      <td>0.679978</td>\n",
       "      <td>upstream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-11 00:00:00</th>\n",
       "      <td>0.052996</td>\n",
       "      <td>0.489781</td>\n",
       "      <td>0.468944</td>\n",
       "      <td>0.179582</td>\n",
       "      <td>0.005683</td>\n",
       "      <td>0.185266</td>\n",
       "      <td>0.277233</td>\n",
       "      <td>0.194836</td>\n",
       "      <td>0.658392</td>\n",
       "      <td>0.054376</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057137</td>\n",
       "      <td>0.492574</td>\n",
       "      <td>0.459184</td>\n",
       "      <td>0.181222</td>\n",
       "      <td>0.004765</td>\n",
       "      <td>0.188742</td>\n",
       "      <td>0.325245</td>\n",
       "      <td>0.230047</td>\n",
       "      <td>0.666487</td>\n",
       "      <td>upstream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-11 04:00:00</th>\n",
       "      <td>0.054376</td>\n",
       "      <td>0.489781</td>\n",
       "      <td>0.468944</td>\n",
       "      <td>0.188602</td>\n",
       "      <td>0.004995</td>\n",
       "      <td>0.186714</td>\n",
       "      <td>0.323180</td>\n",
       "      <td>0.248826</td>\n",
       "      <td>0.677280</td>\n",
       "      <td>0.055756</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052720</td>\n",
       "      <td>0.488850</td>\n",
       "      <td>0.458740</td>\n",
       "      <td>0.187782</td>\n",
       "      <td>0.005396</td>\n",
       "      <td>0.187438</td>\n",
       "      <td>0.304595</td>\n",
       "      <td>0.206573</td>\n",
       "      <td>0.658392</td>\n",
       "      <td>upstream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-11 08:00:00</th>\n",
       "      <td>0.055756</td>\n",
       "      <td>0.488850</td>\n",
       "      <td>0.458740</td>\n",
       "      <td>0.192292</td>\n",
       "      <td>0.004995</td>\n",
       "      <td>0.186714</td>\n",
       "      <td>0.347444</td>\n",
       "      <td>0.248826</td>\n",
       "      <td>0.685375</td>\n",
       "      <td>0.055480</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052720</td>\n",
       "      <td>0.488850</td>\n",
       "      <td>0.458740</td>\n",
       "      <td>0.187782</td>\n",
       "      <td>0.005798</td>\n",
       "      <td>0.187004</td>\n",
       "      <td>0.293237</td>\n",
       "      <td>0.199530</td>\n",
       "      <td>0.655693</td>\n",
       "      <td>upstream</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     TotalNitrogen_lag-10  TotalPhosphorus_lag-10  \\\n",
       "time                                                                \n",
       "2020-11-10 16:00:00              0.053548                0.487918   \n",
       "2020-11-10 20:00:00              0.053548                0.487918   \n",
       "2020-11-11 00:00:00              0.052996                0.489781   \n",
       "2020-11-11 04:00:00              0.054376                0.489781   \n",
       "2020-11-11 08:00:00              0.055756                0.488850   \n",
       "\n",
       "                     AmmoniaNitrogen_lag-10  PermanganateIndex_lag-10  \\\n",
       "time                                                                    \n",
       "2020-11-10 16:00:00                0.454747                  0.202132   \n",
       "2020-11-10 20:00:00                0.454747                  0.202132   \n",
       "2020-11-11 00:00:00                0.468944                  0.179582   \n",
       "2020-11-11 04:00:00                0.468944                  0.188602   \n",
       "2020-11-11 08:00:00                0.458740                  0.192292   \n",
       "\n",
       "                     Turbidity_lag-10  Conductivity_lag-10  \\\n",
       "time                                                         \n",
       "2020-11-10 16:00:00          0.004535             0.185411   \n",
       "2020-11-10 20:00:00          0.005683             0.185266   \n",
       "2020-11-11 00:00:00          0.005683             0.185266   \n",
       "2020-11-11 04:00:00          0.004995             0.186714   \n",
       "2020-11-11 08:00:00          0.004995             0.186714   \n",
       "\n",
       "                     DissolvedOxygen_lag-10  pH_lag-10  temp_lag-10  \\\n",
       "time                                                                  \n",
       "2020-11-10 16:00:00                0.314404   0.227700     0.671883   \n",
       "2020-11-10 20:00:00                0.278265   0.194836     0.663788   \n",
       "2020-11-11 00:00:00                0.277233   0.194836     0.658392   \n",
       "2020-11-11 04:00:00                0.323180   0.248826     0.677280   \n",
       "2020-11-11 08:00:00                0.347444   0.248826     0.685375   \n",
       "\n",
       "                     TotalNitrogen_lag-9  ...  TotalNitrogen_lag-1  \\\n",
       "time                                      ...                        \n",
       "2020-11-10 16:00:00             0.053548  ...             0.054376   \n",
       "2020-11-10 20:00:00             0.052996  ...             0.052996   \n",
       "2020-11-11 00:00:00             0.054376  ...             0.057137   \n",
       "2020-11-11 04:00:00             0.055756  ...             0.052720   \n",
       "2020-11-11 08:00:00             0.055480  ...             0.052720   \n",
       "\n",
       "                     TotalPhosphorus_lag-1  AmmoniaNitrogen_lag-1  \\\n",
       "time                                                                \n",
       "2020-11-10 16:00:00               0.490246               0.469831   \n",
       "2020-11-10 20:00:00               0.490246               0.463620   \n",
       "2020-11-11 00:00:00               0.492574               0.459184   \n",
       "2020-11-11 04:00:00               0.488850               0.458740   \n",
       "2020-11-11 08:00:00               0.488850               0.458740   \n",
       "\n",
       "                     PermanganateIndex_lag-1  Turbidity_lag-1  \\\n",
       "time                                                            \n",
       "2020-11-10 16:00:00                 0.184912         0.007406   \n",
       "2020-11-10 20:00:00                 0.191472         0.005454   \n",
       "2020-11-11 00:00:00                 0.181222         0.004765   \n",
       "2020-11-11 04:00:00                 0.187782         0.005396   \n",
       "2020-11-11 08:00:00                 0.187782         0.005798   \n",
       "\n",
       "                     Conductivity_lag-1  DissolvedOxygen_lag-1  pH_lag-1  \\\n",
       "time                                                                       \n",
       "2020-11-10 16:00:00            0.187004               0.316469  0.225352   \n",
       "2020-11-10 20:00:00            0.189756               0.352091  0.262911   \n",
       "2020-11-11 00:00:00            0.188742               0.325245  0.230047   \n",
       "2020-11-11 04:00:00            0.187438               0.304595  0.206573   \n",
       "2020-11-11 08:00:00            0.187004               0.293237  0.199530   \n",
       "\n",
       "                     temp_lag-1  District  \n",
       "time                                       \n",
       "2020-11-10 16:00:00    0.661090  upstream  \n",
       "2020-11-10 20:00:00    0.679978  upstream  \n",
       "2020-11-11 00:00:00    0.666487  upstream  \n",
       "2020-11-11 04:00:00    0.658392  upstream  \n",
       "2020-11-11 08:00:00    0.655693  upstream  \n",
       "\n",
       "[5 rows x 91 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e5e37966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp</th>\n",
       "      <th>pH</th>\n",
       "      <th>DissolvedOxygen</th>\n",
       "      <th>Conductivity</th>\n",
       "      <th>District</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-11-10 16:00:00</th>\n",
       "      <td>0.679978</td>\n",
       "      <td>0.262911</td>\n",
       "      <td>0.352091</td>\n",
       "      <td>0.189756</td>\n",
       "      <td>upstream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-10 20:00:00</th>\n",
       "      <td>0.666487</td>\n",
       "      <td>0.230047</td>\n",
       "      <td>0.325245</td>\n",
       "      <td>0.188742</td>\n",
       "      <td>upstream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-11 00:00:00</th>\n",
       "      <td>0.658392</td>\n",
       "      <td>0.206573</td>\n",
       "      <td>0.304595</td>\n",
       "      <td>0.187438</td>\n",
       "      <td>upstream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-11 04:00:00</th>\n",
       "      <td>0.655693</td>\n",
       "      <td>0.199530</td>\n",
       "      <td>0.293237</td>\n",
       "      <td>0.187004</td>\n",
       "      <td>upstream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-11 08:00:00</th>\n",
       "      <td>0.652995</td>\n",
       "      <td>0.197183</td>\n",
       "      <td>0.290139</td>\n",
       "      <td>0.187438</td>\n",
       "      <td>upstream</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         temp        pH  DissolvedOxygen  Conductivity  \\\n",
       "time                                                                     \n",
       "2020-11-10 16:00:00  0.679978  0.262911         0.352091      0.189756   \n",
       "2020-11-10 20:00:00  0.666487  0.230047         0.325245      0.188742   \n",
       "2020-11-11 00:00:00  0.658392  0.206573         0.304595      0.187438   \n",
       "2020-11-11 04:00:00  0.655693  0.199530         0.293237      0.187004   \n",
       "2020-11-11 08:00:00  0.652995  0.197183         0.290139      0.187438   \n",
       "\n",
       "                     District  \n",
       "time                           \n",
       "2020-11-10 16:00:00  upstream  \n",
       "2020-11-10 20:00:00  upstream  \n",
       "2020-11-11 00:00:00  upstream  \n",
       "2020-11-11 04:00:00  upstream  \n",
       "2020-11-11 08:00:00  upstream  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "02cb4089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'upstream': MinMaxScaler(),\n",
       "  'midstream': MinMaxScaler(),\n",
       "  'downstream': MinMaxScaler()},\n",
       " {'upstream': MinMaxScaler(),\n",
       "  'midstream': MinMaxScaler(),\n",
       "  'downstream': MinMaxScaler()})"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scalers, y_scalers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892a26b1",
   "metadata": {},
   "source": [
    "### Postprocessing in a same manner with centalized learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ff4401b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_postprocessing(X_train, X_val, y_train, y_val, exogenous_data_train, exogenous_data_val, x_scalers, y_scalers):\n",
    "    \"\"\"Make data ready to be fed into ml algorithms\"\"\"\n",
    "    # if there are more than one specified areas, get the data per area\n",
    "    if X_train[args.identifier].nunique() != 1:\n",
    "        area_X_train, area_X_val, area_y_train, area_y_val = get_data_by_area(X_train, X_val,\n",
    "                                                                              y_train, y_val, \n",
    "                                                                              identifier=args.identifier)\n",
    "    else:\n",
    "        area_X_train, area_X_val, area_y_train, area_y_val = None, None, None, None\n",
    "\n",
    "    # Get the exogenous data per area.\n",
    "    if exogenous_data_train is not None:\n",
    "        exogenous_data_train, exogenous_data_val = get_exogenous_data_by_area(exogenous_data_train,\n",
    "                                                                              exogenous_data_val)\n",
    "    # transform to np\n",
    "    if area_X_train is not None:\n",
    "        for area in area_X_train:\n",
    "            tmp_X_train, tmp_y_train, tmp_X_val, tmp_y_val = remove_identifiers(\n",
    "                area_X_train[area], area_y_train[area], area_X_val[area], area_y_val[area])\n",
    "            tmp_X_train, tmp_y_train = tmp_X_train.to_numpy(), tmp_y_train.to_numpy()\n",
    "            tmp_X_val, tmp_y_val = tmp_X_val.to_numpy(), tmp_y_val.to_numpy()\n",
    "            area_X_train[area] = tmp_X_train\n",
    "            area_X_val[area] = tmp_X_val\n",
    "            area_y_train[area] = tmp_y_train\n",
    "            area_y_val[area] = tmp_y_val\n",
    "    \n",
    "    if exogenous_data_train is not None:\n",
    "        for area in exogenous_data_train:\n",
    "            exogenous_data_train[area] = exogenous_data_train[area].to_numpy()\n",
    "            exogenous_data_val[area] = exogenous_data_val[area].to_numpy()\n",
    "    \n",
    "    # remove identifiers from features, targets\n",
    "    X_train, y_train, X_val, y_val = remove_identifiers(X_train, y_train, X_val, y_val)\n",
    "    assert len(X_train.columns) == len(X_val.columns)\n",
    "    \n",
    "    num_features = len(X_train.columns) // args.num_lags\n",
    "    \n",
    "    # to timeseries representation\n",
    "    X_train = to_timeseries_rep(X_train.to_numpy(), num_lags=args.num_lags,\n",
    "                                            num_features=num_features)\n",
    "    X_val = to_timeseries_rep(X_val.to_numpy(), num_lags=args.num_lags,\n",
    "                                          num_features=num_features)\n",
    "    \n",
    "    if area_X_train is not None:\n",
    "        area_X_train = to_timeseries_rep(area_X_train, num_lags=args.num_lags,\n",
    "                                                     num_features=num_features)\n",
    "        area_X_val = to_timeseries_rep(area_X_val, num_lags=args.num_lags,\n",
    "                                                   num_features=num_features)\n",
    "    \n",
    "    # transform targets to numpy\n",
    "    y_train, y_val = y_train.to_numpy(), y_val.to_numpy()\n",
    "    \n",
    "    if exogenous_data_train is not None:\n",
    "        exogenous_data_train_combined, exogenous_data_val_combined = [], []\n",
    "        for area in exogenous_data_train:\n",
    "            exogenous_data_train_combined.extend(exogenous_data_train[area])\n",
    "            exogenous_data_val_combined.extend(exogenous_data_val[area])\n",
    "        exogenous_data_train_combined = np.stack(exogenous_data_train_combined)\n",
    "        exogenous_data_val_combined = np.stack(exogenous_data_val_combined)\n",
    "        exogenous_data_train[\"all\"] = exogenous_data_train_combined\n",
    "        exogenous_data_val[\"all\"] = exogenous_data_val_combined\n",
    "    return X_train, X_val, y_train, y_val, area_X_train, area_X_val, area_y_train, area_y_val, exogenous_data_train, exogenous_data_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "03064d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val, client_X_train, client_X_val, client_y_train, client_y_val, exogenous_data_train, exogenous_data_val = make_postprocessing(X_train, X_val, y_train, y_val, exogenous_data_train, exogenous_data_val, x_scalers, y_scalers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ee088254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['upstream', 'midstream', 'downstream'])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_X_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "aef6d703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['upstream', 'midstream', 'downstream'])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_X_val.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "dfe82601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Client: upstream\n",
      "X_train shape: (3881, 10, 9, 1), y_train shape: (3881, 4)\n",
      "X_val shape: (962, 10, 9, 1), y_val shape: (962, 4)\n",
      "\n",
      "Client: midstream\n",
      "X_train shape: (3934, 10, 9, 1), y_train shape: (3934, 4)\n",
      "X_val shape: (976, 10, 9, 1), y_val shape: (976, 4)\n",
      "\n",
      "Client: downstream\n",
      "X_train shape: (3926, 10, 9, 1), y_train shape: (3926, 4)\n",
      "X_val shape: (974, 10, 9, 1), y_val shape: (974, 4)\n"
     ]
    }
   ],
   "source": [
    "for client in client_X_train:\n",
    "    print(f\"\\nClient: {client}\")\n",
    "    print(f\"X_train shape: {client_X_train[client].shape}, y_train shape: {client_y_train[client].shape}\")\n",
    "    print(f\"X_val shape: {client_X_val[client].shape}, y_val shape: {client_y_val[client].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "922abedf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7c20f040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_dims(X_train, exogenous_data_train):\n",
    "    if args.model_name == \"mlp\":\n",
    "        input_dim = X_train.shape[1] * X_train.shape[2]\n",
    "    else:\n",
    "        input_dim = X_train.shape[2]\n",
    "    \n",
    "    if exogenous_data_train is not None:\n",
    "        if len(exogenous_data_train) == 1:\n",
    "            cid = next(iter(exogenous_data_train.keys()))\n",
    "            exogenous_dim = exogenous_data_train[cid].shape[1]\n",
    "        else:\n",
    "            exogenous_dim = exogenous_data_train[\"all\"].shape[1]\n",
    "    else:\n",
    "        exogenous_dim = 0\n",
    "    \n",
    "    return input_dim, exogenous_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2e866f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model: str,\n",
    "              input_dim: int,\n",
    "              out_dim: int,\n",
    "              lags: int = 10,\n",
    "              exogenous_dim: int = 0,\n",
    "              seed=0):\n",
    "    if model == \"mlp\":\n",
    "        model = MLP(input_dim=input_dim, layer_units=[256, 128, 64], num_outputs=out_dim)\n",
    "    elif model == \"rnn\":\n",
    "        model = RNN(input_dim=input_dim, rnn_hidden_size=128, num_rnn_layers=1, rnn_dropout=0.0,\n",
    "                    layer_units=[128], num_outputs=out_dim, matrix_rep=True, exogenous_dim=exogenous_dim)\n",
    "    elif model == \"lstm\":\n",
    "        model = LSTM(input_dim=input_dim, lstm_hidden_size=128, num_lstm_layers=1, lstm_dropout=0.0,\n",
    "                     layer_units=[128], num_outputs=out_dim, matrix_rep=True, exogenous_dim=exogenous_dim)\n",
    "    elif model == \"gru\":\n",
    "        model = GRU(input_dim=input_dim, gru_hidden_size=128, num_gru_layers=1, gru_dropout=0.0,\n",
    "                    layer_units=[128], num_outputs=out_dim, matrix_rep=True, exogenous_dim=exogenous_dim)\n",
    "    elif model == \"cnn\":\n",
    "        model = CNN(num_features=input_dim, lags=lags, exogenous_dim=exogenous_dim, out_dim=out_dim)\n",
    "    elif model == \"da_encoder_decoder\":\n",
    "        model = DualAttentionAutoEncoder(input_dim=input_dim, architecture=\"lstm\", matrix_rep=True)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Specified model is not implemented. Plese define your own model or choose one from ['mlp', 'rnn', 'lstm', 'gru', 'cnn', 'da_encoder_decoder']\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1e163843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 0\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "args.model_name = \"cnn\"\n",
    "\n",
    "input_dim, exogenous_dim = get_input_dims(X_train, exogenous_data_train)\n",
    "\n",
    "print(input_dim, exogenous_dim)\n",
    "\n",
    "model = get_model(model=args.model_name,\n",
    "                  input_dim=input_dim,\n",
    "                  out_dim=y_train.shape[1],\n",
    "                  lags=args.num_lags,\n",
    "                  exogenous_dim=exogenous_dim,\n",
    "                  seed=args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7e15bd45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (activation): ReLU()\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(16, 3), stride=(1, 1), padding=same)\n",
       "  (conv2): Conv2d(16, 16, kernel_size=(3, 5), stride=(1, 1), padding=same)\n",
       "  (conv3): Conv2d(16, 32, kernel_size=(4, 3), stride=(1, 1), padding=same)\n",
       "  (conv4): Conv2d(32, 32, kernel_size=(4, 3), stride=(1, 1), padding=same)\n",
       "  (pool): AvgPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0)\n",
       "  (fc): Linear(in_features=1440, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2813c1",
   "metadata": {},
   "source": [
    "### Fit function initiates the training process of every base station local model and then performs parameters aggregation on a central server for N specified federated epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a913bbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, X_train, y_train, X_val, y_val, \n",
    "        exogenous_data_train=None, exogenous_data_val=None, \n",
    "        idxs=[8, 3, 1, 2], # the indices of our targets in X\n",
    "        log_per=1,\n",
    "        client_creation_fn = None, # client specification\n",
    "        local_train_params=None, # local params\n",
    "        aggregation_params=None, # aggregation params\n",
    "        use_carbontracker=False\n",
    "       ):\n",
    "    # client creation definition\n",
    "    if client_creation_fn is None:\n",
    "        client_creation_fn = create_regression_client\n",
    "    # local params\n",
    "    if local_train_params is None:\n",
    "        local_train_params = {\n",
    "            \"epochs\": args.epochs, \"optimizer\": args.optimizer, \"lr\": args.lr,\n",
    "            \"criterion\": args.criterion, \"early_stopping\": args.local_early_stopping,\n",
    "            \"patience\": args.local_patience, \"device\": device\n",
    "        }\n",
    "    \n",
    "    train_loaders, val_loaders = [], []\n",
    "    \n",
    "    # get data per client\n",
    "    for client in X_train:\n",
    "        if client == \"all\":\n",
    "            continue\n",
    "        if exogenous_data_train is not None:\n",
    "            tmp_exogenous_data_train = exogenous_data_train[client]\n",
    "            tmp_exogenous_data_val = tmp_exogenous_data_val[client]\n",
    "        else:\n",
    "            tmp_exogenous_data_train = None\n",
    "            tmp_exogenous_data_val = None\n",
    "    \n",
    "        num_features = len(X_train[client][0][0])\n",
    "        \n",
    "        # to torch loader\n",
    "        train_loaders.append(\n",
    "            to_torch_dataset(\n",
    "                X_train[client], y_train[client],\n",
    "                num_lags=args.num_lags,\n",
    "                num_features=num_features,\n",
    "                exogenous_data=tmp_exogenous_data_train,\n",
    "                indices=idxs,\n",
    "                batch_size=args.batch_size,\n",
    "                shuffle=False\n",
    "            )\n",
    "        )\n",
    "        val_loaders.append(\n",
    "            to_torch_dataset(\n",
    "                X_val[client], y_val[client],\n",
    "                num_lags=args.num_lags,\n",
    "                exogenous_data=tmp_exogenous_data_val,\n",
    "                indices=idxs,\n",
    "                batch_size=args.batch_size,\n",
    "                shuffle=False\n",
    "            )\n",
    "            \n",
    "        )\n",
    "        \n",
    "    # create clients with their local data\n",
    "    cids = [k for k in X_train.keys() if k != \"all\"]\n",
    "    clients = [\n",
    "        client_creation_fn(\n",
    "            cid=cid, # client id\n",
    "            model=model, # the global model\n",
    "            train_loader=train_loader, # the local train loader\n",
    "            test_loader=val_loader, # the local val loader\n",
    "            local_params=local_train_params # local parameters\n",
    "        )\n",
    "        for cid, train_loader, val_loader in zip(cids, train_loaders, val_loaders)\n",
    "    ]\n",
    "    \n",
    "    # represent clients to server\n",
    "    client_proxies = [\n",
    "        SimpleClientProxy(cid, client) for cid, client in zip(cids, clients)\n",
    "    ]\n",
    "    \n",
    "    # represent the server\n",
    "    server = Server(\n",
    "        client_proxies=client_proxies, # the client representations\n",
    "        aggregation=args.aggregation, # the aggregation algorithm\n",
    "        aggregation_params=aggregation_params, # aggregation specific params\n",
    "        local_params_fn=None, # we can change the local params on demand\n",
    "    )\n",
    "    # Note that the client manager instance will be initialized automatically. You can define your own client manager.\n",
    "\n",
    "    # train with FL\n",
    "    model_params, history = server.fit(args.fl_rounds, args.fraction, use_carbontracker=use_carbontracker)\n",
    "    \n",
    "    params_dict = zip(model.state_dict().keys(), model_params)\n",
    "    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "    model = copy.deepcopy(model)\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "51240332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# federated local params\n",
    "local_train_params = {\"epochs\": args.epochs, \"optimizer\": args.optimizer, \"lr\": args.lr,\n",
    "                      \"criterion\": args.criterion, \"early_stopping\": args.local_early_stopping,\n",
    "                      \"patience\": args.local_patience, \"device\": device\n",
    "                      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b1054cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO logger 2024-06-02 23:30:34,041 | server.py:62 | Initializing client manager...\n",
      "INFO logger 2024-06-02 23:30:34,041 | server.py:69 | Registering clients...\n",
      "INFO logger 2024-06-02 23:30:34,042 | client_manager.py:66 | Registered client with id: upstream\n",
      "INFO logger 2024-06-02 23:30:34,043 | client_manager.py:66 | Registered client with id: midstream\n",
      "INFO logger 2024-06-02 23:30:34,043 | client_manager.py:66 | Registered client with id: downstream\n",
      "INFO logger 2024-06-02 23:30:34,044 | server.py:73 | Client manager initialized!\n",
      "INFO logger 2024-06-02 23:30:34,044 | server.py:55 | Aggregation algorithm: SimpleAvg()\n",
      "INFO logger 2024-06-02 23:30:34,044 | client_manager.py:88 | Parameter c=0.0. Sampled 1 client(s): ['midstream']\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 4 is out of bounds for axis 1 with size 4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[116], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m global_model, history \u001b[38;5;241m=\u001b[39m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_X_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_y_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_X_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_y_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_train_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_train_params\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[114], line 88\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(model, X_train, y_train, X_val, y_val, exogenous_data_train, exogenous_data_val, idxs, log_per, client_creation_fn, local_train_params, aggregation_params, use_carbontracker)\u001b[0m\n\u001b[0;32m     79\u001b[0m server \u001b[38;5;241m=\u001b[39m Server(\n\u001b[0;32m     80\u001b[0m     client_proxies\u001b[38;5;241m=\u001b[39mclient_proxies, \u001b[38;5;66;03m# the client representations\u001b[39;00m\n\u001b[0;32m     81\u001b[0m     aggregation\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39maggregation, \u001b[38;5;66;03m# the aggregation algorithm\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     aggregation_params\u001b[38;5;241m=\u001b[39maggregation_params, \u001b[38;5;66;03m# aggregation specific params\u001b[39;00m\n\u001b[0;32m     83\u001b[0m     local_params_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;66;03m# we can change the local params on demand\u001b[39;00m\n\u001b[0;32m     84\u001b[0m )\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Note that the client manager instance will be initialized automatically. You can define your own client manager.\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# train with FL\u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m model_params, history \u001b[38;5;241m=\u001b[39m \u001b[43mserver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfl_rounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfraction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_carbontracker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_carbontracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m params_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(model\u001b[38;5;241m.\u001b[39mstate_dict()\u001b[38;5;241m.\u001b[39mkeys(), model_params)\n\u001b[0;32m     91\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m OrderedDict({k: torch\u001b[38;5;241m.\u001b[39mTensor(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params_dict})\n",
      "File \u001b[1;32mD:\\Work\\Federated_Time_Series_Forecasting_Water_Quality\\ml\\fl\\server\\server.py:84\u001b[0m, in \u001b[0;36mServer.fit\u001b[1;34m(self, num_rounds, fraction, fraction_args, use_carbontracker)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run federated rounds for num_rounds rounds.\"\"\"\u001b[39;00m\n\u001b[0;32m     82\u001b[0m history \u001b[38;5;241m=\u001b[39m History()\n\u001b[1;32m---> 84\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_round\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfl_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m log(INFO, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting FL rounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     87\u001b[0m cb_tracker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Work\\Federated_Time_Series_Forecasting_Water_Quality\\ml\\fl\\server\\server.py:217\u001b[0m, in \u001b[0;36mServer.evaluate_round\u001b[1;34m(self, fl_round, history)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cid, client_proxy \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient_manager\u001b[38;5;241m.\u001b[39mall()\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 217\u001b[0m         num_train_instances, train_loss, train_eval_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mclient_proxy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglobal_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m                                                                                    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m         num_train_examples\u001b[38;5;241m.\u001b[39mappend(num_train_instances)\n\u001b[0;32m    221\u001b[0m         train_losses[cid] \u001b[38;5;241m=\u001b[39m train_loss\n",
      "File \u001b[1;32mD:\\Work\\Federated_Time_Series_Forecasting_Water_Quality\\ml\\fl\\client_proxy.py:43\u001b[0m, in \u001b[0;36mSimpleClientProxy.evaluate\u001b[1;34m(self, data, model, params, method, verbose)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: Optional[DataLoader] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     40\u001b[0m              model: Optional[Union[torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule, List[np\u001b[38;5;241m.\u001b[39mndarray]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     41\u001b[0m              params: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, method: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, verbose: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     42\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Global model evaluation.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Work\\Federated_Time_Series_Forecasting_Water_Quality\\ml\\fl\\torch_client.py:144\u001b[0m, in \u001b[0;36mTorchRegressionClient.evaluate\u001b[1;34m(self, data, model, params, method, verbose)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    142\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader\n\u001b[1;32m--> 144\u001b[0m loss, mse, rmse, mae, r2, nrmse \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcriterion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m metrics \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMSE\u001b[39m\u001b[38;5;124m\"\u001b[39m: mse, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRMSE\u001b[39m\u001b[38;5;124m\"\u001b[39m: rmse, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAE\u001b[39m\u001b[38;5;124m\"\u001b[39m: mae, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mR^2\u001b[39m\u001b[38;5;124m\"\u001b[39m: r2, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNRMSE\u001b[39m\u001b[38;5;124m\"\u001b[39m: nrmse}\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "File \u001b[1;32mD:\\Work\\Federated_Time_Series_Forecasting_Water_Quality\\ml\\utils\\train_utils.py:171\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(model, data, criterion, device)\u001b[0m\n\u001b[0;32m    169\u001b[0m y_true \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(y_true)\n\u001b[0;32m    170\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(y_pred)\n\u001b[1;32m--> 171\u001b[0m mse, rmse, mae, r2, nrmse \u001b[38;5;241m=\u001b[39m \u001b[43maccumulate_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m criterion \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mse, rmse, mae, r2, nrmse, y_pred\n",
      "File \u001b[1;32mD:\\Work\\Federated_Time_Series_Forecasting_Water_Quality\\ml\\utils\\helpers.py:95\u001b[0m, in \u001b[0;36maccumulate_metric\u001b[1;34m(y_true, y_pred, log_per_output, dims, return_all)\u001b[0m\n\u001b[0;32m     93\u001b[0m nrmses \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(dims)):\n\u001b[1;32m---> 95\u001b[0m     y_true_dim \u001b[38;5;241m=\u001b[39m \u001b[43my_true\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdims\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     96\u001b[0m     y_pred_dim \u001b[38;5;241m=\u001b[39m y_pred[:, dims[i]]\n\u001b[0;32m     97\u001b[0m     rmse_dim \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(mean_squared_error(y_true_dim, y_pred_dim))\n",
      "\u001b[1;31mIndexError\u001b[0m: index 4 is out of bounds for axis 1 with size 4"
     ]
    }
   ],
   "source": [
    "global_model, history = fit(\n",
    "    model,\n",
    "    client_X_train,\n",
    "    client_y_train, \n",
    "    client_X_val, \n",
    "    client_y_val, \n",
    "    local_train_params=local_train_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c913897",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb2ead6",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5906a435",
   "metadata": {},
   "outputs": [],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f682fd52-1565-4823-80bf-5529f13d58f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040bba0d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "a39106e1a9d6d153b7400628e7589ff266b5caee5b0db427f0903be982155882"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
