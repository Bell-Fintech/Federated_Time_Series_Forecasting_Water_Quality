{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8da638ca",
   "metadata": {},
   "source": [
    "### In this notebook we perform individual training.\n",
    "In individual learning each base station has access only to it's private dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f3d7d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "parent = Path(os.path.abspath(\"\")).resolve().parents[0]\n",
    "if parent not in sys.path:\n",
    "    sys.path.insert(0, str(parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01e170e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15abc428",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.utils.data_utils import read_data, generate_time_lags, time_to_feature, handle_nans, to_Xy, \\\n",
    "    to_torch_dataset, to_timeseries_rep, assign_statistics, \\\n",
    "    to_train_val, scale_features, get_data_by_area, remove_identifiers, get_exogenous_data_by_area, handle_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "350c9d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.utils.train_utils import train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4688fbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.models.mlp import MLP\n",
    "from ml.models.rnn import RNN\n",
    "from ml.models.lstm import LSTM\n",
    "from ml.models.gru import GRU\n",
    "from ml.models.cnn import CNN\n",
    "from ml.models.rnn_autoencoder import DualAttentionAutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3db1750",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    data_path='../dataset/full_dataset.csv', # dataset\n",
    "    data_path_test=['../dataset/ElBorn_test.csv'], # test dataset\n",
    "    test_size=0.2, # validation size \n",
    "    targets=['rnti_count', 'rb_down', 'rb_up', 'down', 'up'], # the target columns\n",
    "    num_lags=10, # the number of past observations to feed as input\n",
    "\n",
    "    \n",
    "    filter_bs=None, # whether to use a single bs for training. It will be changed dynamically\n",
    "    identifier='District', # the column name that identifies a bs\n",
    "\n",
    "    nan_constant=0, # the constant to transform nan values\n",
    "    x_scaler='minmax', # x_scaler\n",
    "    y_scaler='minmax', # y_scaler\n",
    "    outlier_detection=None, # whether to perform flooring and capping\n",
    "\n",
    "    \n",
    "    criterion='mse', # optimization criterion, mse or l1\n",
    "    epochs=150, # the number of maximum epochs\n",
    "    lr=0.001, # learning rate\n",
    "    optimizer='adam', # the optimizer, it can be sgd or adam\n",
    "    batch_size=128, # the batch size to use\n",
    "    early_stopping=True, # whether to use early stopping\n",
    "    patience=50, # patience value for the early stopping parameter (if specified)\n",
    "    max_grad_norm=0.0, # whether to clip grad norm\n",
    "    reg1=0.0, # l1 regularization\n",
    "    reg2=0.0, # l2 regularization\n",
    "    \n",
    "    plot_history=True, # plot loss history\n",
    "\n",
    "    cuda=True, # whether to use gpu\n",
    "    \n",
    "    seed=0, # reproducibility\n",
    "\n",
    "    assign_stats=None, # whether to use statistics as exogenous data, [\"mean\", \"median\", \"std\", \"variance\", \"kurtosis\", \"skew\"]\n",
    "    use_time_features=False # whether to use datetime features\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56a743e",
   "metadata": {},
   "source": [
    "> You can define the base station to perform train on the filter_bs parameter and use it in block 12 or you can define the base station to block 12 explicitly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "763c39ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script arguments: Namespace(assign_stats=None, batch_size=128, criterion='mse', cuda=True, data_path='../dataset/full_dataset.csv', data_path_test=['../dataset/ElBorn_test.csv'], early_stopping=True, epochs=150, filter_bs=None, identifier='District', lr=0.001, max_grad_norm=0.0, nan_constant=0, num_lags=10, optimizer='adam', outlier_detection=None, patience=50, plot_history=True, reg1=0.0, reg2=0.0, seed=0, targets=['rnti_count', 'rb_down', 'rb_up', 'down', 'up'], test_size=0.2, use_time_features=False, x_scaler='minmax', y_scaler='minmax')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Script arguments: {args}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da3431ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if args.cuda and torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06bb4aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection specification\n",
    "if args.outlier_detection is not None:\n",
    "    outlier_columns = ['rb_down', 'rb_up', 'down', 'up']\n",
    "    outlier_kwargs = {\"ElBorn\": (10, 90), \"LesCorts\": (10, 90), \"PobleSec\": (5, 95)}\n",
    "    args.outlier_columns = outlier_columns\n",
    "    args.outlier_kwargs = outlier_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ac1d35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_all():\n",
    "    # ensure reproducibility\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea3ddd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00543376",
   "metadata": {},
   "source": [
    "### The preprocessing pipeline performed here for the base station specified in filter_bs argument\n",
    "Preprocessing inlcudes:\n",
    "1. NaNs Handling\n",
    "2. Outliers Handling\n",
    "3. Scaling Data\n",
    "4. Generating time lags\n",
    "5. Generating and importing exogenous data as features (time, statistics) (if applied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35bc6b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preprocessing(filter_bs=None):\n",
    "    \"\"\"Preprocess a given .csv\"\"\"\n",
    "    # read data\n",
    "    df = read_data(args.data_path, filter_data=filter_bs)\n",
    "    # handle nans\n",
    "    df = handle_nans(train_data=df, constant=args.nan_constant,\n",
    "                     identifier=args.identifier)\n",
    "    # split to train/validation\n",
    "    train_data, val_data = to_train_val(df)\n",
    "    \n",
    "    # handle outliers (if specified)\n",
    "    if args.outlier_detection is not None:\n",
    "        train_data = handle_outliers(df=train_data, columns=args.outlier_columns,\n",
    "                                     identifier=args.identifier, kwargs=args.outlier_kwargs)\n",
    "    \n",
    "    # get X and y\n",
    "    X_train, X_val, y_train, y_val = to_Xy(train_data=train_data, val_data=val_data,\n",
    "                                          targets=args.targets)\n",
    "    \n",
    "    # scale X\n",
    "    X_train, X_val, x_scaler = scale_features(train_data=X_train, val_data=X_val,\n",
    "                                             scaler=args.x_scaler, identifier=args.identifier)\n",
    "    # scale y\n",
    "    y_train, y_val, y_scaler = scale_features(train_data=y_train, val_data=y_val,\n",
    "                                             scaler=args.y_scaler, identifier=args.identifier)\n",
    "    \n",
    "    # generate time lags\n",
    "    X_train = generate_time_lags(X_train, args.num_lags)\n",
    "    X_val = generate_time_lags(X_val, args.num_lags)\n",
    "    y_train = generate_time_lags(y_train, args.num_lags, is_y=True)\n",
    "    y_val = generate_time_lags(y_val, args.num_lags, is_y=True)\n",
    "    \n",
    "    # get datetime features as exogenous data\n",
    "    date_time_df_train = time_to_feature(\n",
    "        X_train, args.use_time_features, identifier=args.identifier\n",
    "    )\n",
    "    date_time_df_val = time_to_feature(\n",
    "        X_val, args.use_time_features, identifier=args.identifier\n",
    "    )\n",
    "    \n",
    "    # get statistics as exogenous data\n",
    "    stats_df_train = assign_statistics(X_train, args.assign_stats, args.num_lags,\n",
    "                                       targets=args.targets, identifier=args.identifier)\n",
    "    stats_df_val = assign_statistics(X_val, args.assign_stats, args.num_lags, \n",
    "                                       targets=args.targets, identifier=args.identifier)\n",
    "    \n",
    "    # concat the exogenous features (if any) to a single dataframe\n",
    "    if date_time_df_train is not None or stats_df_train is not None:\n",
    "        exogenous_data_train = pd.concat([date_time_df_train, stats_df_train], axis=1)\n",
    "        # remove duplicate columns (if any)\n",
    "        exogenous_data_train = exogenous_data_train.loc[:, ~exogenous_data_train.columns.duplicated()].copy()\n",
    "        assert len(exogenous_data_train) == len(X_train) == len(y_train)\n",
    "    else:\n",
    "        exogenous_data_train = None\n",
    "    if date_time_df_val is not None or stats_df_val is not None:\n",
    "        exogenous_data_val = pd.concat([date_time_df_val, stats_df_val], axis=1)\n",
    "        exogenous_data_val = exogenous_data_val.loc[:, ~exogenous_data_val.columns.duplicated()].copy()\n",
    "        assert len(exogenous_data_val) == len(X_val) == len(y_val)\n",
    "    else:\n",
    "        exogenous_data_val = None\n",
    "        \n",
    "    return X_train, X_val, y_train, y_val, exogenous_data_train, exogenous_data_val, x_scaler, y_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66fc93eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO logger 2024-04-27 20:23:10,438 | data_utils.py:28 | Reading LesCorts's data...\n",
      "INFO logger 2024-04-27 20:23:10,469 | data_utils.py:395 | \tTotal number of samples:  6892\n",
      "INFO logger 2024-04-27 20:23:10,470 | data_utils.py:396 | \tNumber of samples for training: 5514\n",
      "INFO logger 2024-04-27 20:23:10,470 | data_utils.py:397 | \tNumber of samples for validation:  1378\n"
     ]
    }
   ],
   "source": [
    "# here exogenous_data_train and val are None.\n",
    "X_train, X_val, y_train, y_val, exogenous_data_train, exogenous_data_val, x_scaler, y_scaler = make_preprocessing(\n",
    "    filter_bs=\"LesCorts\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b99c6f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rb_up_var_lag-10</th>\n",
       "      <th>rb_up_lag-10</th>\n",
       "      <th>rb_down_var_lag-10</th>\n",
       "      <th>rb_down_lag-10</th>\n",
       "      <th>mcs_up_var_lag-10</th>\n",
       "      <th>mcs_up_lag-10</th>\n",
       "      <th>mcs_down_var_lag-10</th>\n",
       "      <th>mcs_down_lag-10</th>\n",
       "      <th>rnti_count_lag-10</th>\n",
       "      <th>up_lag-10</th>\n",
       "      <th>...</th>\n",
       "      <th>rb_down_var_lag-1</th>\n",
       "      <th>rb_down_lag-1</th>\n",
       "      <th>mcs_up_var_lag-1</th>\n",
       "      <th>mcs_up_lag-1</th>\n",
       "      <th>mcs_down_var_lag-1</th>\n",
       "      <th>mcs_down_lag-1</th>\n",
       "      <th>rnti_count_lag-1</th>\n",
       "      <th>up_lag-1</th>\n",
       "      <th>down_lag-1</th>\n",
       "      <th>District</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-12 17:32:00</th>\n",
       "      <td>1.083682e-08</td>\n",
       "      <td>0.036054</td>\n",
       "      <td>1.961025e-08</td>\n",
       "      <td>0.109670</td>\n",
       "      <td>0.493544</td>\n",
       "      <td>0.431702</td>\n",
       "      <td>0.459601</td>\n",
       "      <td>0.160269</td>\n",
       "      <td>0.210509</td>\n",
       "      <td>0.017705</td>\n",
       "      <td>...</td>\n",
       "      <td>1.974816e-08</td>\n",
       "      <td>0.114193</td>\n",
       "      <td>0.572030</td>\n",
       "      <td>0.400005</td>\n",
       "      <td>0.460896</td>\n",
       "      <td>0.164719</td>\n",
       "      <td>0.212757</td>\n",
       "      <td>0.018888</td>\n",
       "      <td>0.131219</td>\n",
       "      <td>LesCorts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 17:34:00</th>\n",
       "      <td>1.061491e-08</td>\n",
       "      <td>0.030426</td>\n",
       "      <td>1.644211e-08</td>\n",
       "      <td>0.098938</td>\n",
       "      <td>0.527483</td>\n",
       "      <td>0.476772</td>\n",
       "      <td>0.392094</td>\n",
       "      <td>0.131085</td>\n",
       "      <td>0.200300</td>\n",
       "      <td>0.018282</td>\n",
       "      <td>...</td>\n",
       "      <td>1.791601e-08</td>\n",
       "      <td>0.105544</td>\n",
       "      <td>0.576015</td>\n",
       "      <td>0.445876</td>\n",
       "      <td>0.427184</td>\n",
       "      <td>0.145817</td>\n",
       "      <td>0.192432</td>\n",
       "      <td>0.014424</td>\n",
       "      <td>0.113276</td>\n",
       "      <td>LesCorts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 17:36:00</th>\n",
       "      <td>9.109702e-09</td>\n",
       "      <td>0.038901</td>\n",
       "      <td>1.761309e-08</td>\n",
       "      <td>0.106045</td>\n",
       "      <td>0.501063</td>\n",
       "      <td>0.471773</td>\n",
       "      <td>0.453949</td>\n",
       "      <td>0.152725</td>\n",
       "      <td>0.221468</td>\n",
       "      <td>0.019501</td>\n",
       "      <td>...</td>\n",
       "      <td>1.766410e-08</td>\n",
       "      <td>0.101075</td>\n",
       "      <td>0.640010</td>\n",
       "      <td>0.448971</td>\n",
       "      <td>0.387407</td>\n",
       "      <td>0.130386</td>\n",
       "      <td>0.173137</td>\n",
       "      <td>0.011194</td>\n",
       "      <td>0.101414</td>\n",
       "      <td>LesCorts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 17:38:00</th>\n",
       "      <td>5.813802e-09</td>\n",
       "      <td>0.026050</td>\n",
       "      <td>1.668480e-08</td>\n",
       "      <td>0.098968</td>\n",
       "      <td>0.499865</td>\n",
       "      <td>0.450949</td>\n",
       "      <td>0.382516</td>\n",
       "      <td>0.129036</td>\n",
       "      <td>0.183768</td>\n",
       "      <td>0.012594</td>\n",
       "      <td>...</td>\n",
       "      <td>1.963081e-08</td>\n",
       "      <td>0.114777</td>\n",
       "      <td>0.534812</td>\n",
       "      <td>0.414812</td>\n",
       "      <td>0.476978</td>\n",
       "      <td>0.171880</td>\n",
       "      <td>0.205545</td>\n",
       "      <td>0.023585</td>\n",
       "      <td>0.141225</td>\n",
       "      <td>LesCorts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 17:40:00</th>\n",
       "      <td>7.246522e-09</td>\n",
       "      <td>0.028981</td>\n",
       "      <td>1.721707e-08</td>\n",
       "      <td>0.102669</td>\n",
       "      <td>0.535319</td>\n",
       "      <td>0.429488</td>\n",
       "      <td>0.412994</td>\n",
       "      <td>0.139577</td>\n",
       "      <td>0.191964</td>\n",
       "      <td>0.013056</td>\n",
       "      <td>...</td>\n",
       "      <td>1.895823e-08</td>\n",
       "      <td>0.107131</td>\n",
       "      <td>0.547422</td>\n",
       "      <td>0.456544</td>\n",
       "      <td>0.448805</td>\n",
       "      <td>0.157300</td>\n",
       "      <td>0.181567</td>\n",
       "      <td>0.022127</td>\n",
       "      <td>0.125069</td>\n",
       "      <td>LesCorts</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 111 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     rb_up_var_lag-10  rb_up_lag-10  rb_down_var_lag-10  \\\n",
       "time                                                                      \n",
       "2019-01-12 17:32:00      1.083682e-08      0.036054        1.961025e-08   \n",
       "2019-01-12 17:34:00      1.061491e-08      0.030426        1.644211e-08   \n",
       "2019-01-12 17:36:00      9.109702e-09      0.038901        1.761309e-08   \n",
       "2019-01-12 17:38:00      5.813802e-09      0.026050        1.668480e-08   \n",
       "2019-01-12 17:40:00      7.246522e-09      0.028981        1.721707e-08   \n",
       "\n",
       "                     rb_down_lag-10  mcs_up_var_lag-10  mcs_up_lag-10  \\\n",
       "time                                                                    \n",
       "2019-01-12 17:32:00        0.109670           0.493544       0.431702   \n",
       "2019-01-12 17:34:00        0.098938           0.527483       0.476772   \n",
       "2019-01-12 17:36:00        0.106045           0.501063       0.471773   \n",
       "2019-01-12 17:38:00        0.098968           0.499865       0.450949   \n",
       "2019-01-12 17:40:00        0.102669           0.535319       0.429488   \n",
       "\n",
       "                     mcs_down_var_lag-10  mcs_down_lag-10  rnti_count_lag-10  \\\n",
       "time                                                                           \n",
       "2019-01-12 17:32:00             0.459601         0.160269           0.210509   \n",
       "2019-01-12 17:34:00             0.392094         0.131085           0.200300   \n",
       "2019-01-12 17:36:00             0.453949         0.152725           0.221468   \n",
       "2019-01-12 17:38:00             0.382516         0.129036           0.183768   \n",
       "2019-01-12 17:40:00             0.412994         0.139577           0.191964   \n",
       "\n",
       "                     up_lag-10  ...  rb_down_var_lag-1  rb_down_lag-1  \\\n",
       "time                            ...                                     \n",
       "2019-01-12 17:32:00   0.017705  ...       1.974816e-08       0.114193   \n",
       "2019-01-12 17:34:00   0.018282  ...       1.791601e-08       0.105544   \n",
       "2019-01-12 17:36:00   0.019501  ...       1.766410e-08       0.101075   \n",
       "2019-01-12 17:38:00   0.012594  ...       1.963081e-08       0.114777   \n",
       "2019-01-12 17:40:00   0.013056  ...       1.895823e-08       0.107131   \n",
       "\n",
       "                     mcs_up_var_lag-1  mcs_up_lag-1  mcs_down_var_lag-1  \\\n",
       "time                                                                      \n",
       "2019-01-12 17:32:00          0.572030      0.400005            0.460896   \n",
       "2019-01-12 17:34:00          0.576015      0.445876            0.427184   \n",
       "2019-01-12 17:36:00          0.640010      0.448971            0.387407   \n",
       "2019-01-12 17:38:00          0.534812      0.414812            0.476978   \n",
       "2019-01-12 17:40:00          0.547422      0.456544            0.448805   \n",
       "\n",
       "                     mcs_down_lag-1  rnti_count_lag-1  up_lag-1  down_lag-1  \\\n",
       "time                                                                          \n",
       "2019-01-12 17:32:00        0.164719          0.212757  0.018888    0.131219   \n",
       "2019-01-12 17:34:00        0.145817          0.192432  0.014424    0.113276   \n",
       "2019-01-12 17:36:00        0.130386          0.173137  0.011194    0.101414   \n",
       "2019-01-12 17:38:00        0.171880          0.205545  0.023585    0.141225   \n",
       "2019-01-12 17:40:00        0.157300          0.181567  0.022127    0.125069   \n",
       "\n",
       "                     District  \n",
       "time                           \n",
       "2019-01-12 17:32:00  LesCorts  \n",
       "2019-01-12 17:34:00  LesCorts  \n",
       "2019-01-12 17:36:00  LesCorts  \n",
       "2019-01-12 17:38:00  LesCorts  \n",
       "2019-01-12 17:40:00  LesCorts  \n",
       "\n",
       "[5 rows x 111 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec22537f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rnti_count</th>\n",
       "      <th>rb_down</th>\n",
       "      <th>rb_up</th>\n",
       "      <th>down</th>\n",
       "      <th>up</th>\n",
       "      <th>District</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-12 17:32:00</th>\n",
       "      <td>0.192432</td>\n",
       "      <td>0.105544</td>\n",
       "      <td>0.033785</td>\n",
       "      <td>0.113276</td>\n",
       "      <td>0.014424</td>\n",
       "      <td>LesCorts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 17:34:00</th>\n",
       "      <td>0.173137</td>\n",
       "      <td>0.101075</td>\n",
       "      <td>0.025216</td>\n",
       "      <td>0.101414</td>\n",
       "      <td>0.011194</td>\n",
       "      <td>LesCorts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 17:36:00</th>\n",
       "      <td>0.205545</td>\n",
       "      <td>0.114777</td>\n",
       "      <td>0.060088</td>\n",
       "      <td>0.141225</td>\n",
       "      <td>0.023585</td>\n",
       "      <td>LesCorts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 17:38:00</th>\n",
       "      <td>0.181567</td>\n",
       "      <td>0.107131</td>\n",
       "      <td>0.042592</td>\n",
       "      <td>0.125069</td>\n",
       "      <td>0.022127</td>\n",
       "      <td>LesCorts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 17:40:00</th>\n",
       "      <td>0.175900</td>\n",
       "      <td>0.101726</td>\n",
       "      <td>0.023463</td>\n",
       "      <td>0.107920</td>\n",
       "      <td>0.011213</td>\n",
       "      <td>LesCorts</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     rnti_count   rb_down     rb_up      down        up  \\\n",
       "time                                                                      \n",
       "2019-01-12 17:32:00    0.192432  0.105544  0.033785  0.113276  0.014424   \n",
       "2019-01-12 17:34:00    0.173137  0.101075  0.025216  0.101414  0.011194   \n",
       "2019-01-12 17:36:00    0.205545  0.114777  0.060088  0.141225  0.023585   \n",
       "2019-01-12 17:38:00    0.181567  0.107131  0.042592  0.125069  0.022127   \n",
       "2019-01-12 17:40:00    0.175900  0.101726  0.023463  0.107920  0.011213   \n",
       "\n",
       "                     District  \n",
       "time                           \n",
       "2019-01-12 17:32:00  LesCorts  \n",
       "2019-01-12 17:34:00  LesCorts  \n",
       "2019-01-12 17:36:00  LesCorts  \n",
       "2019-01-12 17:38:00  LesCorts  \n",
       "2019-01-12 17:40:00  LesCorts  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "367c2b33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(MinMaxScaler(), MinMaxScaler())"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaler, y_scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488a9c30",
   "metadata": {},
   "source": [
    "### Postprocessing Stage\n",
    "\n",
    "In this stage we transform data in a way that can be fed into ML algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c417082c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_postprocessing(X_train, X_val, y_train, y_val, exogenous_data_train, exogenous_data_val, x_scaler, y_scaler):\n",
    "    \"\"\"Make data ready to be fed into ml algorithms\"\"\"\n",
    "    # if there are more than one specified areas, get the data per area\n",
    "    if X_train[args.identifier].nunique() != 1:\n",
    "        area_X_train, area_X_val, area_y_train, area_y_val = get_data_by_area(X_train, X_val,\n",
    "                                                                              y_train, y_val, \n",
    "                                                                              identifier=args.identifier)\n",
    "    else:\n",
    "        area_X_train, area_X_val, area_y_train, area_y_val = None, None, None, None\n",
    "\n",
    "    # Get the exogenous data per area.\n",
    "    if exogenous_data_train is not None:\n",
    "        exogenous_data_train, exogenous_data_val = get_exogenous_data_by_area(exogenous_data_train,\n",
    "                                                                              exogenous_data_val)\n",
    "    # transform to np\n",
    "    if area_X_train is not None:\n",
    "        for area in area_X_train:\n",
    "            tmp_X_train, tmp_y_train, tmp_X_val, tmp_y_val = remove_identifiers(\n",
    "                area_X_train[area], area_y_train[area], area_X_val[area], area_y_val[area])\n",
    "            tmp_X_train, tmp_y_train = tmp_X_train.to_numpy(), tmp_y_train.to_numpy()\n",
    "            tmp_X_val, tmp_y_val = tmp_X_val.to_numpy(), tmp_y_val.to_numpy()\n",
    "            area_X_train[area] = tmp_X_train\n",
    "            area_X_val[area] = tmp_X_val\n",
    "            area_y_train[area] = tmp_y_train\n",
    "            area_y_val[area] = tmp_y_val\n",
    "    \n",
    "    if exogenous_data_train is not None:\n",
    "        for area in exogenous_data_train:\n",
    "            exogenous_data_train[area] = exogenous_data_train[area].to_numpy()\n",
    "            exogenous_data_val[area] = exogenous_data_val[area].to_numpy()\n",
    "    \n",
    "    # remove identifiers from features, targets\n",
    "    X_train, y_train, X_val, y_val = remove_identifiers(X_train, y_train, X_val, y_val)\n",
    "    assert len(X_train.columns) == len(X_val.columns)\n",
    "    \n",
    "    num_features = len(X_train.columns) // args.num_lags\n",
    "    \n",
    "    # to timeseries representation\n",
    "    X_train = to_timeseries_rep(X_train.to_numpy(), num_lags=args.num_lags,\n",
    "                                            num_features=num_features)\n",
    "    X_val = to_timeseries_rep(X_val.to_numpy(), num_lags=args.num_lags,\n",
    "                                          num_features=num_features)\n",
    "    \n",
    "    if area_X_train is not None:\n",
    "        area_X_train = to_timeseries_rep(area_X_train, num_lags=args.num_lags,\n",
    "                                                     num_features=num_features)\n",
    "        area_X_val = to_timeseries_rep(area_X_val, num_lags=args.num_lags,\n",
    "                                                   num_features=num_features)\n",
    "    \n",
    "    # transform targets to numpy\n",
    "    y_train, y_val = y_train.to_numpy(), y_val.to_numpy()\n",
    "    \n",
    "    # centralized (all) learning specific\n",
    "    if not args.filter_bs and exogenous_data_train is not None:\n",
    "        exogenous_data_train_combined, exogenous_data_val_combined = [], []\n",
    "        for area in exogenous_data_train:\n",
    "            exogenous_data_train_combined.extend(exogenous_data_train[area])\n",
    "            exogenous_data_val_combined.extend(exogenous_data_val[area])\n",
    "        exogenous_data_train_combined = np.stack(exogenous_data_train_combined)\n",
    "        exogenous_data_val_combined = np.stack(exogenous_data_val_combined)\n",
    "        exogenous_data_train[\"all\"] = exogenous_data_train_combined\n",
    "        exogenous_data_val[\"all\"] = exogenous_data_val_combined\n",
    "    return X_train, X_val, y_train, y_val, area_X_train, area_X_val, area_y_train, area_y_val, exogenous_data_train, exogenous_data_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00c59dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val, area_X_train, area_X_val, area_y_train, area_y_val, exogenous_data_train, exogenous_data_val = make_postprocessing(X_train, X_val, y_train, y_val, exogenous_data_train, exogenous_data_val, x_scaler, y_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9171667b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[1.08368177e-08],\n",
       "         [3.60536501e-02],\n",
       "         [1.96102530e-08],\n",
       "         [1.09670356e-01],\n",
       "         [4.93544310e-01],\n",
       "         [4.31701750e-01],\n",
       "         [4.59600508e-01],\n",
       "         [1.60269260e-01],\n",
       "         [2.10509062e-01],\n",
       "         [1.77047513e-02],\n",
       "         [1.29703119e-01]],\n",
       "\n",
       "        [[1.06149072e-08],\n",
       "         [3.04260999e-02],\n",
       "         [1.64421117e-08],\n",
       "         [9.89375487e-02],\n",
       "         [5.27482629e-01],\n",
       "         [4.76772249e-01],\n",
       "         [3.92093986e-01],\n",
       "         [1.31084532e-01],\n",
       "         [2.00299725e-01],\n",
       "         [1.82819255e-02],\n",
       "         [9.46676582e-02]],\n",
       "\n",
       "        [[9.10970233e-09],\n",
       "         [3.89005616e-02],\n",
       "         [1.76130861e-08],\n",
       "         [1.06044851e-01],\n",
       "         [5.01062572e-01],\n",
       "         [4.71772611e-01],\n",
       "         [4.53948647e-01],\n",
       "         [1.52724519e-01],\n",
       "         [2.21467718e-01],\n",
       "         [1.95014086e-02],\n",
       "         [1.11105040e-01]],\n",
       "\n",
       "        [[5.81380233e-09],\n",
       "         [2.60500330e-02],\n",
       "         [1.66847958e-08],\n",
       "         [9.89684388e-02],\n",
       "         [4.99864906e-01],\n",
       "         [4.50949341e-01],\n",
       "         [3.82516414e-01],\n",
       "         [1.29036337e-01],\n",
       "         [1.83768094e-01],\n",
       "         [1.25944829e-02],\n",
       "         [9.68663320e-02]],\n",
       "\n",
       "        [[7.24652205e-09],\n",
       "         [2.89812144e-02],\n",
       "         [1.72170704e-08],\n",
       "         [1.02669150e-01],\n",
       "         [5.35318553e-01],\n",
       "         [4.29487944e-01],\n",
       "         [4.12994266e-01],\n",
       "         [1.39576867e-01],\n",
       "         [1.91963658e-01],\n",
       "         [1.30563863e-02],\n",
       "         [1.04084052e-01]],\n",
       "\n",
       "        [[6.06154105e-09],\n",
       "         [4.28037606e-02],\n",
       "         [2.31000232e-08],\n",
       "         [1.32117063e-01],\n",
       "         [5.78713536e-01],\n",
       "         [4.60009694e-01],\n",
       "         [5.32073081e-01],\n",
       "         [1.96870178e-01],\n",
       "         [2.43010357e-01],\n",
       "         [1.87698063e-02],\n",
       "         [1.75482631e-01]],\n",
       "\n",
       "        [[6.05376371e-09],\n",
       "         [3.68960202e-02],\n",
       "         [2.13197904e-08],\n",
       "         [1.28368676e-01],\n",
       "         [6.27172947e-01],\n",
       "         [4.36633140e-01],\n",
       "         [5.26264966e-01],\n",
       "         [1.94025993e-01],\n",
       "         [2.43150860e-01],\n",
       "         [1.70038119e-02],\n",
       "         [1.67414486e-01]],\n",
       "\n",
       "        [[1.08545919e-08],\n",
       "         [4.05925028e-02],\n",
       "         [1.91197547e-08],\n",
       "         [1.09047189e-01],\n",
       "         [5.68330526e-01],\n",
       "         [3.95149767e-01],\n",
       "         [4.69213665e-01],\n",
       "         [1.62352383e-01],\n",
       "         [2.05591723e-01],\n",
       "         [1.58412419e-02],\n",
       "         [1.19397290e-01]],\n",
       "\n",
       "        [[1.06866844e-08],\n",
       "         [1.01436771e-01],\n",
       "         [1.97316883e-08],\n",
       "         [1.14744321e-01],\n",
       "         [4.81375158e-01],\n",
       "         [3.61676991e-01],\n",
       "         [4.93533880e-01],\n",
       "         [1.76247209e-01],\n",
       "         [2.22966328e-01],\n",
       "         [2.71484163e-02],\n",
       "         [1.40494898e-01]],\n",
       "\n",
       "        [[1.21875967e-08],\n",
       "         [4.45951000e-02],\n",
       "         [1.97481569e-08],\n",
       "         [1.14193007e-01],\n",
       "         [5.72030365e-01],\n",
       "         [4.00004745e-01],\n",
       "         [4.60895538e-01],\n",
       "         [1.64718658e-01],\n",
       "         [2.12756991e-01],\n",
       "         [1.88877694e-02],\n",
       "         [1.31219164e-01]]],\n",
       "\n",
       "\n",
       "       [[[1.06149072e-08],\n",
       "         [3.04260999e-02],\n",
       "         [1.64421117e-08],\n",
       "         [9.89375487e-02],\n",
       "         [5.27482629e-01],\n",
       "         [4.76772249e-01],\n",
       "         [3.92093986e-01],\n",
       "         [1.31084532e-01],\n",
       "         [2.00299725e-01],\n",
       "         [1.82819255e-02],\n",
       "         [9.46676582e-02]],\n",
       "\n",
       "        [[9.10970233e-09],\n",
       "         [3.89005616e-02],\n",
       "         [1.76130861e-08],\n",
       "         [1.06044851e-01],\n",
       "         [5.01062572e-01],\n",
       "         [4.71772611e-01],\n",
       "         [4.53948647e-01],\n",
       "         [1.52724519e-01],\n",
       "         [2.21467718e-01],\n",
       "         [1.95014086e-02],\n",
       "         [1.11105040e-01]],\n",
       "\n",
       "        [[5.81380233e-09],\n",
       "         [2.60500330e-02],\n",
       "         [1.66847958e-08],\n",
       "         [9.89684388e-02],\n",
       "         [4.99864906e-01],\n",
       "         [4.50949341e-01],\n",
       "         [3.82516414e-01],\n",
       "         [1.29036337e-01],\n",
       "         [1.83768094e-01],\n",
       "         [1.25944829e-02],\n",
       "         [9.68663320e-02]],\n",
       "\n",
       "        [[7.24652205e-09],\n",
       "         [2.89812144e-02],\n",
       "         [1.72170704e-08],\n",
       "         [1.02669150e-01],\n",
       "         [5.35318553e-01],\n",
       "         [4.29487944e-01],\n",
       "         [4.12994266e-01],\n",
       "         [1.39576867e-01],\n",
       "         [1.91963658e-01],\n",
       "         [1.30563863e-02],\n",
       "         [1.04084052e-01]],\n",
       "\n",
       "        [[6.06154105e-09],\n",
       "         [4.28037606e-02],\n",
       "         [2.31000232e-08],\n",
       "         [1.32117063e-01],\n",
       "         [5.78713536e-01],\n",
       "         [4.60009694e-01],\n",
       "         [5.32073081e-01],\n",
       "         [1.96870178e-01],\n",
       "         [2.43010357e-01],\n",
       "         [1.87698063e-02],\n",
       "         [1.75482631e-01]],\n",
       "\n",
       "        [[6.05376371e-09],\n",
       "         [3.68960202e-02],\n",
       "         [2.13197904e-08],\n",
       "         [1.28368676e-01],\n",
       "         [6.27172947e-01],\n",
       "         [4.36633140e-01],\n",
       "         [5.26264966e-01],\n",
       "         [1.94025993e-01],\n",
       "         [2.43150860e-01],\n",
       "         [1.70038119e-02],\n",
       "         [1.67414486e-01]],\n",
       "\n",
       "        [[1.08545919e-08],\n",
       "         [4.05925028e-02],\n",
       "         [1.91197547e-08],\n",
       "         [1.09047189e-01],\n",
       "         [5.68330526e-01],\n",
       "         [3.95149767e-01],\n",
       "         [4.69213665e-01],\n",
       "         [1.62352383e-01],\n",
       "         [2.05591723e-01],\n",
       "         [1.58412419e-02],\n",
       "         [1.19397290e-01]],\n",
       "\n",
       "        [[1.06866844e-08],\n",
       "         [1.01436771e-01],\n",
       "         [1.97316883e-08],\n",
       "         [1.14744321e-01],\n",
       "         [4.81375158e-01],\n",
       "         [3.61676991e-01],\n",
       "         [4.93533880e-01],\n",
       "         [1.76247209e-01],\n",
       "         [2.22966328e-01],\n",
       "         [2.71484163e-02],\n",
       "         [1.40494898e-01]],\n",
       "\n",
       "        [[1.21875967e-08],\n",
       "         [4.45951000e-02],\n",
       "         [1.97481569e-08],\n",
       "         [1.14193007e-01],\n",
       "         [5.72030365e-01],\n",
       "         [4.00004745e-01],\n",
       "         [4.60895538e-01],\n",
       "         [1.64718658e-01],\n",
       "         [2.12756991e-01],\n",
       "         [1.88877694e-02],\n",
       "         [1.31219164e-01]],\n",
       "\n",
       "        [[9.16101239e-09],\n",
       "         [3.37851271e-02],\n",
       "         [1.79160082e-08],\n",
       "         [1.05543904e-01],\n",
       "         [5.76015115e-01],\n",
       "         [4.45875734e-01],\n",
       "         [4.27184075e-01],\n",
       "         [1.45816624e-01],\n",
       "         [1.92431986e-01],\n",
       "         [1.44237261e-02],\n",
       "         [1.13276407e-01]]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2949d425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.19243199, 0.1055439 , 0.03378513, 0.11327641, 0.01442373],\n",
       "       [0.17313728, 0.10107498, 0.02521631, 0.10141373, 0.01119418]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf7bb251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5504, 1368)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a83357",
   "metadata": {},
   "source": [
    "### Define the input dimensions for the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1486173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_dims(X_train, exogenous_data_train):\n",
    "    if args.model_name == \"mlp\":\n",
    "        input_dim = X_train.shape[1] * X_train.shape[2]\n",
    "    else:\n",
    "        input_dim = X_train.shape[2]\n",
    "    \n",
    "    if exogenous_data_train is not None:\n",
    "        if len(exogenous_data_train) == 1:\n",
    "            cid = next(iter(exogenous_data_train.keys()))\n",
    "            exogenous_dim = exogenous_data_train[cid].shape[1]\n",
    "        else:\n",
    "            exogenous_dim = exogenous_data_train[\"all\"].shape[1]\n",
    "    else:\n",
    "        exogenous_dim = 0\n",
    "    \n",
    "    return input_dim, exogenous_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6783ff",
   "metadata": {},
   "source": [
    "### Initialize the model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e7044d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model: str,\n",
    "              input_dim: int,\n",
    "              out_dim: int,\n",
    "              lags: int = 10,\n",
    "              exogenous_dim: int = 0,\n",
    "              seed=0):\n",
    "    if model == \"mlp\":\n",
    "        model = MLP(input_dim=input_dim, layer_units=[256, 128, 64], num_outputs=out_dim)\n",
    "    elif model == \"rnn\":\n",
    "        model = RNN(input_dim=input_dim, rnn_hidden_size=128, num_rnn_layers=1, rnn_dropout=0.0,\n",
    "                    layer_units=[128], num_outputs=out_dim, matrix_rep=True, exogenous_dim=exogenous_dim)\n",
    "    elif model == \"lstm\":\n",
    "        model = LSTM(input_dim=input_dim, lstm_hidden_size=128, num_lstm_layers=1, lstm_dropout=0.0,\n",
    "                     layer_units=[128], num_outputs=out_dim, matrix_rep=True, exogenous_dim=exogenous_dim)\n",
    "    elif model == \"gru\":\n",
    "        model = GRU(input_dim=input_dim, gru_hidden_size=128, num_gru_layers=1, gru_dropout=0.0,\n",
    "                    layer_units=[128], num_outputs=out_dim, matrix_rep=True, exogenous_dim=exogenous_dim)\n",
    "    elif model == \"cnn\":\n",
    "        model = CNN(num_features=input_dim, lags=lags, exogenous_dim=exogenous_dim, out_dim=out_dim)\n",
    "    elif model == \"da_encoder_decoder\":\n",
    "        model = DualAttentionAutoEncoder(input_dim=input_dim, architecture=\"lstm\", matrix_rep=True)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Specified model is not implemented. Plese define your own model or choose one from ['mlp', 'rnn', 'lstm', 'gru', 'cnn', 'da_encoder_decoder']\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44ed1dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 0\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "args.model_name = \"lstm\"\n",
    "\n",
    "input_dim, exogenous_dim = get_input_dims(X_train, exogenous_data_train)\n",
    "\n",
    "print(input_dim, exogenous_dim)\n",
    "\n",
    "model = get_model(model=args.model_name,\n",
    "                  input_dim=input_dim,\n",
    "                  out_dim=y_train.shape[1],\n",
    "                  lags=args.num_lags,\n",
    "                  exogenous_dim=exogenous_dim,\n",
    "                  seed=args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f285c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (lstm): LSTM(11, 128, batch_first=True)\n",
       "  (MLP_layers): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2473d72",
   "metadata": {},
   "source": [
    "### The fit function used to train the model specified above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b11fc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, X_train, y_train, X_val, y_val, \n",
    "        exogenous_data_train=None, exogenous_data_val=None, \n",
    "        idxs=[8, 3, 1, 10, 9], # the indices of our targets in X\n",
    "        log_per=1):\n",
    "    \n",
    "    # get exogenous data (if any)\n",
    "    if exogenous_data_train is not None and len(exogenous_data_train) > 1:\n",
    "        exogenous_data_train = exogenous_data_train[\"all\"]\n",
    "        exogenous_data_val = exogenous_data_val[\"all\"]\n",
    "    elif exogenous_data_train is not None and len(exogenous_data_train) == 1:\n",
    "        cid = next(iter(exogenous_data_train.keys()))\n",
    "        exogenous_data_train = exogenous_data_train[cid]\n",
    "        exogenous_data_val = exogenous_data_val[cid]\n",
    "    else:\n",
    "        exogenous_data_train = None\n",
    "        exogenous_data_val = None\n",
    "    num_features = len(X_train[0][0])\n",
    "    \n",
    "    # to torch loader\n",
    "    train_loader = to_torch_dataset(X_train, y_train,\n",
    "                                    num_lags=args.num_lags,\n",
    "                                    num_features=num_features,\n",
    "                                    exogenous_data=exogenous_data_train,\n",
    "                                    indices=idxs,\n",
    "                                    batch_size=args.batch_size, \n",
    "                                    shuffle=False)\n",
    "    val_loader = to_torch_dataset(X_val, y_val, \n",
    "                                  num_lags=args.num_lags,\n",
    "                                  num_features=num_features,\n",
    "                                  exogenous_data=exogenous_data_val,\n",
    "                                  indices=idxs,\n",
    "                                  batch_size=args.batch_size,\n",
    "                                  shuffle=False)\n",
    "    \n",
    "    # train the model\n",
    "    model = train(model, \n",
    "                  train_loader, val_loader,\n",
    "                  epochs=args.epochs,\n",
    "                  optimizer=args.optimizer, lr=args.lr,\n",
    "                  criterion=args.criterion,\n",
    "                  early_stopping=args.early_stopping,\n",
    "                  patience=args.patience,\n",
    "                  plot_history=args.plot_history, \n",
    "                  device=device, log_per=log_per)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "681cc512",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO logger 2024-04-27 20:23:17,434 | train_utils.py:97 | Epoch 1 [Train]: loss 0.009817008235015322, mse: 0.00668484577909112, rmse: 0.08176090617826541, mae 0.05011456087231636, r2: 0.301586198727843, nrmse: 1.6128631057356522\n",
      "INFO logger 2024-04-27 20:23:17,435 | train_utils.py:99 | Epoch 1 [Test]: loss 6.25659001253175e-05, mse: 0.007847568020224571, rmse: 0.0885865002143361, mae 0.05763064697384834, r2: -203.5568397813572, nrmse: 11.073634002155384\n",
      "INFO logger 2024-04-27 20:23:17,435 | helpers.py:148 | Validation loss decreased (inf --> 0.000063). Caching model ...\n",
      "INFO logger 2024-04-27 20:23:17,832 | train_utils.py:97 | Epoch 2 [Train]: loss 0.005097735776067819, mse: 0.0034416825510561466, rmse: 0.058665855069675296, mae 0.03914262354373932, r2: 0.4742180039654764, nrmse: 1.5601407919985195\n",
      "INFO logger 2024-04-27 20:23:17,833 | train_utils.py:99 | Epoch 2 [Test]: loss 3.1557813870900296e-05, mse: 0.0039516836404800415, rmse: 0.06286241834737223, mae 0.04491778090596199, r2: -493.2370775312673, nrmse: 10.969223417599089\n",
      "INFO logger 2024-04-27 20:23:17,833 | helpers.py:148 | Validation loss decreased (0.000063 --> 0.000032). Caching model ...\n",
      "INFO logger 2024-04-27 20:23:18,235 | train_utils.py:97 | Epoch 3 [Train]: loss 0.002882774609281833, mse: 0.0024190195836126804, rmse: 0.04918352959693601, mae 0.03209643065929413, r2: 0.6228880964727443, nrmse: 1.259301301986587\n",
      "INFO logger 2024-04-27 20:23:18,235 | train_utils.py:99 | Epoch 3 [Test]: loss 1.9019662538047728e-05, mse: 0.002370677189901471, rmse: 0.048689600428648734, mae 0.03474591672420502, r2: -260.81081807561515, nrmse: 8.268013170449336\n",
      "INFO logger 2024-04-27 20:23:18,236 | helpers.py:148 | Validation loss decreased (0.000032 --> 0.000019). Caching model ...\n",
      "INFO logger 2024-04-27 20:23:18,636 | train_utils.py:97 | Epoch 4 [Train]: loss 0.002493984137694243, mse: 0.0021236129105091095, rmse: 0.04608267473258371, mae 0.028472760692238808, r2: 0.6664073471408722, nrmse: 1.1941816990140455\n",
      "INFO logger 2024-04-27 20:23:18,636 | train_utils.py:99 | Epoch 4 [Test]: loss 1.666556636337191e-05, mse: 0.0020874198526144028, rmse: 0.04568829010385925, mae 0.031431253999471664, r2: -165.56367492420028, nrmse: 6.598777700526315\n",
      "INFO logger 2024-04-27 20:23:18,637 | helpers.py:148 | Validation loss decreased (0.000019 --> 0.000017). Caching model ...\n",
      "INFO logger 2024-04-27 20:23:19,092 | train_utils.py:97 | Epoch 5 [Train]: loss 0.002213419756180672, mse: 0.0020111470948904753, rmse: 0.04484581468643953, mae 0.026701927185058594, r2: 0.6933988815740639, nrmse: 1.1240114994434245\n",
      "INFO logger 2024-04-27 20:23:19,093 | train_utils.py:99 | Epoch 5 [Test]: loss 1.530537136002547e-05, mse: 0.001921034068800509, rmse: 0.043829602653919976, mae 0.029217755421996117, r2: -95.74541600408605, nrmse: 5.323812490334112\n",
      "INFO logger 2024-04-27 20:23:19,093 | helpers.py:148 | Validation loss decreased (0.000017 --> 0.000015). Caching model ...\n",
      "INFO logger 2024-04-27 20:23:19,478 | train_utils.py:97 | Epoch 6 [Train]: loss 0.0020861028941337365, mse: 0.0019475901499390602, rmse: 0.044131509717423675, mae 0.025793025270104408, r2: 0.7047534165223326, nrmse: 1.091920379601159\n",
      "INFO logger 2024-04-27 20:23:19,479 | train_utils.py:99 | Epoch 6 [Test]: loss 1.4485726565291441e-05, mse: 0.001821695826947689, rmse: 0.0426813287861061, mae 0.02794567681849003, r2: -70.61415991300242, nrmse: 4.532979806604149\n",
      "INFO logger 2024-04-27 20:23:19,479 | helpers.py:148 | Validation loss decreased (0.000015 --> 0.000014). Caching model ...\n",
      "INFO logger 2024-04-27 20:23:19,867 | train_utils.py:97 | Epoch 7 [Train]: loss 0.0020100446147194436, mse: 0.0019043354550376534, rmse: 0.043638692178359945, mae 0.025386523455381393, r2: 0.7131674001055527, nrmse: 1.0671630405153363\n",
      "INFO logger 2024-04-27 20:23:19,868 | train_utils.py:99 | Epoch 7 [Test]: loss 1.3906622716710406e-05, mse: 0.0017526292940601707, rmse: 0.04186441560633769, mae 0.026991605758666992, r2: -55.30570391724643, nrmse: 3.8863904519834716\n",
      "INFO logger 2024-04-27 20:23:19,868 | helpers.py:148 | Validation loss decreased (0.000014 --> 0.000014). Caching model ...\n",
      "INFO logger 2024-04-27 20:23:20,251 | train_utils.py:97 | Epoch 8 [Train]: loss 0.001959316291187099, mse: 0.0018611194100230932, rmse: 0.04314069320285771, mae 0.02499617077410221, r2: 0.7199796503196203, nrmse: 1.0550303204581362\n",
      "INFO logger 2024-04-27 20:23:20,251 | train_utils.py:99 | Epoch 8 [Test]: loss 1.3468194445213901e-05, mse: 0.0017003401881083846, rmse: 0.04123518143658864, mae 0.026125501841306686, r2: -40.8359717669573, nrmse: 3.291782313615678\n",
      "INFO logger 2024-04-27 20:23:20,252 | helpers.py:148 | Validation loss decreased (0.000014 --> 0.000013). Caching model ...\n",
      "INFO logger 2024-04-27 20:23:20,640 | train_utils.py:97 | Epoch 9 [Train]: loss 0.0019211232950531898, mse: 0.0018236491596326232, rmse: 0.04270420540921729, mae 0.024619851261377335, r2: 0.7246989310308394, nrmse: 1.0521941696771833\n",
      "INFO logger 2024-04-27 20:23:20,641 | train_utils.py:99 | Epoch 9 [Test]: loss 1.3161461474578174e-05, mse: 0.0016633784398436546, rmse: 0.04078453677367998, mae 0.025405239313840866, r2: -29.40706164591751, nrmse: 2.92764256166451\n",
      "INFO logger 2024-04-27 20:23:20,641 | helpers.py:148 | Validation loss decreased (0.000013 --> 0.000013). Caching model ...\n",
      "INFO logger 2024-04-27 20:23:21,033 | train_utils.py:97 | Epoch 10 [Train]: loss 0.0018926307952084286, mse: 0.0017932361224666238, rmse: 0.0423466187843448, mae 0.024304036051034927, r2: 0.7281051057422419, nrmse: 1.052494978678383\n",
      "INFO logger 2024-04-27 20:23:21,034 | train_utils.py:99 | Epoch 10 [Test]: loss 1.2968176745568615e-05, mse: 0.0016396439168602228, rmse: 0.04049251680076484, mae 0.024919116869568825, r2: -22.080441490560453, nrmse: 2.820363912533296\n",
      "INFO logger 2024-04-27 20:23:21,034 | helpers.py:148 | Validation loss decreased (0.000013 --> 0.000013). Caching model ...\n",
      "INFO logger 2024-04-27 20:23:21,434 | train_utils.py:97 | Epoch 11 [Train]: loss 0.0018690586125452429, mse: 0.001768552465364337, rmse: 0.042054161094525914, mae 0.02404385432600975, r2: 0.7308757285946929, nrmse: 1.0523594395072946\n",
      "INFO logger 2024-04-27 20:23:21,435 | train_utils.py:99 | Epoch 11 [Test]: loss 1.2869070246114467e-05, mse: 0.0016270078485831618, rmse: 0.040336185349920756, mae 0.02466597780585289, r2: -18.02201322245214, nrmse: 2.833478300390914\n",
      "INFO logger 2024-04-27 20:23:21,435 | helpers.py:148 | Validation loss decreased (0.000013 --> 0.000013). Caching model ...\n",
      "INFO logger 2024-04-27 20:23:21,886 | train_utils.py:97 | Epoch 12 [Train]: loss 0.0018439901627865512, mse: 0.0017486161086708307, rmse: 0.04181645739025283, mae 0.02383195236325264, r2: 0.7333820820038726, nrmse: 1.0506268104640837\n",
      "INFO logger 2024-04-27 20:23:21,887 | train_utils.py:99 | Epoch 12 [Test]: loss 1.2823795025442504e-05, mse: 0.001620815135538578, rmse: 0.04025934842416824, mae 0.024556364864110947, r2: -15.91024478662332, nrmse: 2.8545686027130195\n",
      "INFO logger 2024-04-27 20:23:21,887 | helpers.py:148 | Validation loss decreased (0.000013 --> 0.000013). Caching model ...\n",
      "INFO logger 2024-04-27 20:23:22,281 | train_utils.py:97 | Epoch 13 [Train]: loss 0.0018183393886554472, mse: 0.001731489785015583, rmse: 0.04161117380002135, mae 0.023656003177165985, r2: 0.735656725003108, nrmse: 1.0479670085820474\n",
      "INFO logger 2024-04-27 20:23:22,282 | train_utils.py:99 | Epoch 13 [Test]: loss 1.2778150752031555e-05, mse: 0.0016146966954693198, rmse: 0.04018328875875293, mae 0.0244829673320055, r2: -14.849911487345622, nrmse: 2.8562226891947704\n",
      "INFO logger 2024-04-27 20:23:22,282 | helpers.py:148 | Validation loss decreased (0.000013 --> 0.000013). Caching model ...\n",
      "INFO logger 2024-04-27 20:23:22,678 | train_utils.py:97 | Epoch 14 [Train]: loss 0.0017957670949494229, mse: 0.0017158116679638624, rmse: 0.04142235710294457, mae 0.023500632494688034, r2: 0.7376678832916933, nrmse: 1.04520654942725\n",
      "INFO logger 2024-04-27 20:23:22,678 | train_utils.py:99 | Epoch 14 [Test]: loss 1.2712825715340325e-05, mse: 0.0016063643852248788, rmse: 0.04007947586015664, mae 0.02440469339489937, r2: -14.439777084655896, nrmse: 2.859989106419361\n",
      "INFO logger 2024-04-27 20:23:22,679 | helpers.py:148 | Validation loss decreased (0.000013 --> 0.000013). Caching model ...\n",
      "INFO logger 2024-04-27 20:23:23,067 | train_utils.py:97 | Epoch 15 [Train]: loss 0.001777089568603578, mse: 0.0017015396151691675, rmse: 0.041249722607178435, mae 0.023363113403320312, r2: 0.7394017472522423, nrmse: 1.042776199951948\n",
      "INFO logger 2024-04-27 20:23:23,067 | train_utils.py:99 | Epoch 15 [Test]: loss 1.2638444217617695e-05, mse: 0.00159704708494246, rmse: 0.039963071515368535, mae 0.024325814098119736, r2: -14.403685132385519, nrmse: 2.8765228726184584\n",
      "INFO logger 2024-04-27 20:23:23,068 | helpers.py:148 | Validation loss decreased (0.000013 --> 0.000013). Caching model ...\n",
      "INFO logger 2024-04-27 20:23:23,461 | train_utils.py:97 | Epoch 16 [Train]: loss 0.001761500880135383, mse: 0.0016887749079614878, rmse: 0.04109470656862618, mae 0.02324194274842739, r2: 0.7408872959035172, nrmse: 1.0407473580529958\n",
      "INFO logger 2024-04-27 20:23:23,461 | train_utils.py:99 | Epoch 16 [Test]: loss 1.256620555323663e-05, mse: 0.0015880456194281578, rmse: 0.039850290079598644, mae 0.02425360307097435, r2: -14.519071939777962, nrmse: 2.8996304624481533\n",
      "INFO logger 2024-04-27 20:23:23,462 | helpers.py:148 | Validation loss decreased (0.000013 --> 0.000013). Caching model ...\n",
      "INFO logger 2024-04-27 20:23:23,847 | train_utils.py:97 | Epoch 17 [Train]: loss 0.0017479803725511248, mse: 0.0016773432726040483, rmse: 0.04095538148527063, mae 0.023135175928473473, r2: 0.7421773170065227, nrmse: 1.039052186718761\n",
      "INFO logger 2024-04-27 20:23:23,848 | train_utils.py:99 | Epoch 17 [Test]: loss 1.2499134244168536e-05, mse: 0.001579697011038661, rmse: 0.039745402388687186, mae 0.024185840040445328, r2: -14.64954139371866, nrmse: 2.920565570634025\n",
      "INFO logger 2024-04-27 20:23:23,848 | helpers.py:148 | Validation loss decreased (0.000013 --> 0.000012). Caching model ...\n",
      "INFO logger 2024-04-27 20:23:24,261 | train_utils.py:97 | Epoch 18 [Train]: loss 0.0017358027371512943, mse: 0.001666924566961825, rmse: 0.04082798754484263, mae 0.02303887903690338, r2: 0.7433210281514002, nrmse: 1.0376142974308724\n",
      "INFO logger 2024-04-27 20:23:24,261 | train_utils.py:99 | Epoch 18 [Test]: loss 1.2436177616779643e-05, mse: 0.0015718715731054544, rmse: 0.03964683560015168, mae 0.024120766669511795, r2: -14.744982429685427, nrmse: 2.936034552449817\n",
      "INFO logger 2024-04-27 20:23:24,262 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2024-04-27 20:23:24,721 | train_utils.py:97 | Epoch 19 [Train]: loss 0.0017246298996534003, mse: 0.001657302724197507, rmse: 0.04070998310239771, mae 0.022950056940317154, r2: 0.7443512718718643, nrmse: 1.0363867791864965\n",
      "INFO logger 2024-04-27 20:23:24,722 | train_utils.py:99 | Epoch 19 [Test]: loss 1.2376280933787382e-05, mse: 0.001564438221976161, rmse: 0.039552979938004176, mae 0.024054741486907005, r2: -14.806678717207229, nrmse: 2.9473101712454612\n",
      "INFO logger 2024-04-27 20:23:24,723 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2024-04-27 20:23:25,111 | train_utils.py:97 | Epoch 20 [Train]: loss 0.0017143526320069213, mse: 0.001648384379222989, rmse: 0.040600300235626205, mae 0.02286776714026928, r2: 0.7452889706705126, nrmse: 1.0353362080632982\n",
      "INFO logger 2024-04-27 20:23:25,112 | train_utils.py:99 | Epoch 20 [Test]: loss 1.231936039403081e-05, mse: 0.0015573850832879543, rmse: 0.03946371856893309, mae 0.02399158477783203, r2: -14.850310643696337, nrmse: 2.9564867494892058\n",
      "INFO logger 2024-04-27 20:23:25,112 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2024-04-27 20:23:25,512 | train_utils.py:97 | Epoch 21 [Train]: loss 0.001704915246549252, mse: 0.0016400909516960382, rmse: 0.04049803639308995, mae 0.022792620584368706, r2: 0.7461504927116672, nrmse: 1.0344268695150327\n",
      "INFO logger 2024-04-27 20:23:25,513 | train_utils.py:99 | Epoch 21 [Test]: loss 1.2265703653132445e-05, mse: 0.0015507451025769114, rmse: 0.03937950104530162, mae 0.023932037875056267, r2: -14.884524743957831, nrmse: 2.9639759461461206\n",
      "INFO logger 2024-04-27 20:23:25,513 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2024-04-27 20:23:25,916 | train_utils.py:97 | Epoch 22 [Train]: loss 0.0016962396841632274, mse: 0.001632362836971879, rmse: 0.04040251028057389, mae 0.022732365876436234, r2: 0.7469502641343903, nrmse: 1.033618654260873\n",
      "INFO logger 2024-04-27 20:23:25,916 | train_utils.py:99 | Epoch 22 [Test]: loss 1.2215488716757077e-05, mse: 0.001544536673463881, rmse: 0.03930059380548697, mae 0.023875685408711433, r2: -14.906602598920482, nrmse: 2.968202890048755\n",
      "INFO logger 2024-04-27 20:23:25,917 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2024-04-27 20:23:26,294 | train_utils.py:97 | Epoch 23 [Train]: loss 0.0016882348737390415, mse: 0.001625166041776538, rmse: 0.04031334818365423, mae 0.022679850459098816, r2: 0.7476990989576631, nrmse: 1.03287435520804\n",
      "INFO logger 2024-04-27 20:23:26,295 | train_utils.py:99 | Epoch 23 [Test]: loss 1.2168807768309216e-05, mse: 0.0015387691091746092, rmse: 0.039227147604364625, mae 0.023822635412216187, r2: -14.907405698163197, nrmse: 2.9665987122958395\n",
      "INFO logger 2024-04-27 20:23:26,295 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2024-04-27 20:23:26,676 | train_utils.py:97 | Epoch 24 [Train]: loss 0.0016808257361083444, mse: 0.0016184592386707664, rmse: 0.04023007878032016, mae 0.022634711116552353, r2: 0.7484019076968785, nrmse: 1.0321661322688802\n",
      "INFO logger 2024-04-27 20:23:26,677 | train_utils.py:99 | Epoch 24 [Test]: loss 1.212588366058421e-05, mse: 0.0015334688359871507, rmse: 0.039159530589463795, mae 0.023775020614266396, r2: -14.87824742860943, nrmse: 2.9568617210074684\n",
      "INFO logger 2024-04-27 20:23:26,677 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2024-04-27 20:23:27,135 | train_utils.py:97 | Epoch 25 [Train]: loss 0.0016739735335440222, mse: 0.0016122525557875633, rmse: 0.040152864851559014, mae 0.022597087547183037, r2: 0.7490574904611422, nrmse: 1.031479467287383\n",
      "INFO logger 2024-04-27 20:23:27,135 | train_utils.py:99 | Epoch 25 [Test]: loss 1.2087178459440015e-05, mse: 0.0015286883572116494, rmse: 0.039098444434678595, mae 0.023734387010335922, r2: -14.816679894798948, nrmse: 2.9380534760109573\n",
      "INFO logger 2024-04-27 20:23:27,136 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2024-04-27 20:23:27,521 | train_utils.py:97 | Epoch 26 [Train]: loss 0.0016676723016961512, mse: 0.0016065610107034445, rmse: 0.040081928729833405, mae 0.022568373009562492, r2: 0.7496605907423802, nrmse: 1.0308128041172653\n",
      "INFO logger 2024-04-27 20:23:27,522 | train_utils.py:99 | Epoch 26 [Test]: loss 1.2053366318516629e-05, mse: 0.0015245063696056604, rmse: 0.03904492757844046, mae 0.02370130643248558, r2: -14.727026401656243, nrmse: 2.9110103177277398\n",
      "INFO logger 2024-04-27 20:23:27,522 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2024-04-27 20:23:27,910 | train_utils.py:97 | Epoch 27 [Train]: loss 0.0016619334206383386, mse: 0.0016014078864827752, rmse: 0.04001759471136134, mae 0.02254904806613922, r2: 0.7502050456131928, nrmse: 1.0301740010208766\n",
      "INFO logger 2024-04-27 20:23:27,911 | train_utils.py:99 | Epoch 27 [Test]: loss 1.2025261464275485e-05, mse: 0.001521019614301622, rmse: 0.03900025146459471, mae 0.02367686852812767, r2: -14.6153368607982, nrmse: 2.8779081295385103\n",
      "INFO logger 2024-04-27 20:23:27,911 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2024-04-27 20:23:28,303 | train_utils.py:97 | Epoch 28 [Train]: loss 0.0016567643763157195, mse: 0.0015968645457178354, rmse: 0.03996078760132032, mae 0.02253926359117031, r2: 0.75068635891115, nrmse: 1.0295739708287304\n",
      "INFO logger 2024-04-27 20:23:28,304 | train_utils.py:99 | Epoch 28 [Test]: loss 1.2003725544550887e-05, mse: 0.0015183279756456614, rmse: 0.03896572821911149, mae 0.02366466633975506, r2: -14.482300645899874, nrmse: 2.8413274224757314\n",
      "INFO logger 2024-04-27 20:23:28,305 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2024-04-27 20:23:28,675 | train_utils.py:97 | Epoch 29 [Train]: loss 0.001652147696996656, mse: 0.0015929534565657377, rmse: 0.03991182101289965, mae 0.022538181394338608, r2: 0.751103489130504, nrmse: 1.0290222267934612\n",
      "INFO logger 2024-04-27 20:23:28,676 | train_utils.py:99 | Epoch 29 [Test]: loss 1.1989526172871129e-05, mse: 0.001516525400802493, rmse: 0.03894259108999417, mae 0.02366577461361885, r2: -14.317575138046227, nrmse: 2.8031708418824435\n",
      "INFO logger 2024-04-27 20:23:28,677 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2024-04-27 20:23:29,072 | train_utils.py:97 | Epoch 30 [Train]: loss 0.00164802002813684, mse: 0.0015896735712885857, rmse: 0.039870710694551025, mae 0.02254452556371689, r2: 0.751460150474495, nrmse: 1.0285217040851862\n",
      "INFO logger 2024-04-27 20:23:29,073 | train_utils.py:99 | Epoch 30 [Test]: loss 1.1983087078030957e-05, mse: 0.0015156600857153535, rmse: 0.038931479367156775, mae 0.023676637560129166, r2: -14.096874490299504, nrmse: 2.7636695441079304\n",
      "INFO logger 2024-04-27 20:23:29,073 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2024-04-27 20:23:29,560 | train_utils.py:97 | Epoch 31 [Train]: loss 0.001644262522700575, mse: 0.0015869897324591875, rmse: 0.03983703970501808, mae 0.02255523018538952, r2: 0.7517663961908768, nrmse: 1.028066177248499\n",
      "INFO logger 2024-04-27 20:23:29,560 | train_utils.py:99 | Epoch 31 [Test]: loss 1.198413134814643e-05, mse: 0.0015157051384449005, rmse: 0.03893205797854643, mae 0.023693043738603592, r2: -13.78441244062364, nrmse: 2.7207931276693684\n",
      "INFO logger 2024-04-27 20:23:29,560 | helpers.py:136 | EarlyStopping counter: 1 out of 50\n",
      "INFO logger 2024-04-27 20:23:29,947 | train_utils.py:97 | Epoch 32 [Train]: loss 0.0016407111641900311, mse: 0.00158478575758636, rmse: 0.03980936771146158, mae 0.022566094994544983, r2: 0.752038703914905, nrmse: 1.027643296958345\n",
      "INFO logger 2024-04-27 20:23:29,948 | train_utils.py:99 | Epoch 32 [Test]: loss 1.1991304506265901e-05, mse: 0.0015165016520768404, rmse: 0.0389422861691098, mae 0.023710347712039948, r2: -13.341809489984616, nrmse: 2.670429464859931\n",
      "INFO logger 2024-04-27 20:23:29,949 | helpers.py:136 | EarlyStopping counter: 2 out of 50\n",
      "INFO logger 2024-04-27 20:23:30,341 | train_utils.py:97 | Epoch 33 [Train]: loss 0.0016371979425055668, mse: 0.0015828643226996064, rmse: 0.03978522744310514, mae 0.022573664784431458, r2: 0.7522976135874553, nrmse: 1.0272434913661783\n",
      "INFO logger 2024-04-27 20:23:30,341 | train_utils.py:99 | Epoch 33 [Test]: loss 1.2002051563706107e-05, mse: 0.0015177521854639053, rmse: 0.03895833910042759, mae 0.023720677942037582, r2: -12.744113865965044, nrmse: 2.6077749875890146\n",
      "INFO logger 2024-04-27 20:23:30,342 | helpers.py:136 | EarlyStopping counter: 3 out of 50\n",
      "INFO logger 2024-04-27 20:23:30,727 | train_utils.py:97 | Epoch 34 [Train]: loss 0.001633609412851382, mse: 0.0015810137847438455, rmse: 0.039761964045351754, mae 0.022574525326490402, r2: 0.7525617078864355, nrmse: 1.0268713926288486\n",
      "INFO logger 2024-04-27 20:23:30,727 | train_utils.py:99 | Epoch 34 [Test]: loss 1.201310874288357e-05, mse: 0.001519066747277975, rmse: 0.038975206827905035, mae 0.023715876042842865, r2: -11.997364646395956, nrmse: 2.529605035220562\n",
      "INFO logger 2024-04-27 20:23:30,728 | helpers.py:136 | EarlyStopping counter: 4 out of 50\n",
      "INFO logger 2024-04-27 20:23:31,139 | train_utils.py:97 | Epoch 35 [Train]: loss 0.0016299403341585022, mse: 0.0015790362376719713, rmse: 0.03973708894310165, mae 0.02256912924349308, r2: 0.7528406327239907, nrmse: 1.0265482054689758\n",
      "INFO logger 2024-04-27 20:23:31,139 | train_utils.py:99 | Epoch 35 [Test]: loss 1.2021590275615073e-05, mse: 0.0015201012138277292, rmse: 0.038988475397580366, mae 0.02369351126253605, r2: -11.14694251460713, nrmse: 2.4363307347336933\n",
      "INFO logger 2024-04-27 20:23:31,140 | helpers.py:136 | EarlyStopping counter: 5 out of 50\n",
      "INFO logger 2024-04-27 20:23:31,536 | train_utils.py:97 | Epoch 36 [Train]: loss 0.0016263061734841972, mse: 0.001576888607814908, rmse: 0.03971005675914992, mae 0.02255949005484581, r2: 0.7531319351370582, nrmse: 1.0262998842204627\n",
      "INFO logger 2024-04-27 20:23:31,537 | train_utils.py:99 | Epoch 36 [Test]: loss 1.2026141238179634e-05, mse: 0.0015206888783723116, rmse: 0.03899601105718778, mae 0.023656480014324188, r2: -10.26722206509237, nrmse: 2.332259522455008\n",
      "INFO logger 2024-04-27 20:23:31,537 | helpers.py:136 | EarlyStopping counter: 6 out of 50\n",
      "INFO logger 2024-04-27 20:23:31,938 | train_utils.py:97 | Epoch 37 [Train]: loss 0.0016228986890509563, mse: 0.0015746352728456259, rmse: 0.03968167426968809, mae 0.022550104185938835, r2: 0.7534247764474298, nrmse: 1.0261367560835195\n",
      "INFO logger 2024-04-27 20:23:31,939 | train_utils.py:99 | Epoch 37 [Test]: loss 1.2027328665952378e-05, mse: 0.0015208888798952103, rmse: 0.038998575357251326, mae 0.023612525314092636, r2: -9.437913373423529, nrmse: 2.223875984806692\n",
      "INFO logger 2024-04-27 20:23:31,939 | helpers.py:136 | EarlyStopping counter: 7 out of 50\n",
      "INFO logger 2024-04-27 20:23:32,411 | train_utils.py:97 | Epoch 38 [Train]: loss 0.0016198974668589282, mse: 0.0015724286204203963, rmse: 0.03965386009483057, mae 0.022546280175447464, r2: 0.7537081314612142, nrmse: 1.026040027635565\n",
      "INFO logger 2024-04-27 20:23:32,412 | train_utils.py:99 | Epoch 38 [Test]: loss 1.2027024479864753e-05, mse: 0.0015209140256047249, rmse: 0.03899889774858675, mae 0.02357081137597561, r2: -8.724546973868502, nrmse: 2.1176111413560625\n",
      "INFO logger 2024-04-27 20:23:32,412 | helpers.py:136 | EarlyStopping counter: 8 out of 50\n",
      "INFO logger 2024-04-27 20:23:32,807 | train_utils.py:97 | Epoch 39 [Train]: loss 0.0016173853353520402, mse: 0.001570422202348709, rmse: 0.03962855286720308, mae 0.022550804540514946, r2: 0.7539771805265417, nrmse: 1.0259660272220352\n",
      "INFO logger 2024-04-27 20:23:32,807 | train_utils.py:99 | Epoch 39 [Test]: loss 1.2027325772589116e-05, mse: 0.0015210089040920138, rmse: 0.03900011415485875, mae 0.023540973663330078, r2: -8.172865998677661, nrmse: 2.018710070828592\n",
      "INFO logger 2024-04-27 20:23:32,808 | helpers.py:136 | EarlyStopping counter: 9 out of 50\n",
      "INFO logger 2024-04-27 20:23:33,201 | train_utils.py:97 | Epoch 40 [Train]: loss 0.001615308301499554, mse: 0.0015686772530898452, rmse: 0.039606530434889715, mae 0.022564660757780075, r2: 0.7542351578133248, nrmse: 1.0258614106007085\n",
      "INFO logger 2024-04-27 20:23:33,202 | train_utils.py:99 | Epoch 40 [Test]: loss 1.2029701202552309e-05, mse: 0.0015213405713438988, rmse: 0.039004366054890556, mae 0.023529337719082832, r2: -7.812204120299716, nrmse: 1.9313908875177621\n",
      "INFO logger 2024-04-27 20:23:33,202 | helpers.py:136 | EarlyStopping counter: 10 out of 50\n",
      "INFO logger 2024-04-27 20:23:33,606 | train_utils.py:97 | Epoch 41 [Train]: loss 0.0016134909099830195, mse: 0.0015671920264139771, rmse: 0.0395877762246628, mae 0.022585714235901833, r2: 0.7544914140074372, nrmse: 1.025680326562798\n",
      "INFO logger 2024-04-27 20:23:33,607 | train_utils.py:99 | Epoch 41 [Test]: loss 1.2034577328088143e-05, mse: 0.0015219568740576506, rmse: 0.03901226568731494, mae 0.023535553365945816, r2: -7.653732608106198, nrmse: 1.8588049345351514\n",
      "INFO logger 2024-04-27 20:23:33,607 | helpers.py:136 | EarlyStopping counter: 11 out of 50\n",
      "INFO logger 2024-04-27 20:23:33,996 | train_utils.py:97 | Epoch 42 [Train]: loss 0.0016116979506245286, mse: 0.001565861515700817, rmse: 0.039570968091529136, mae 0.022610409185290337, r2: 0.7547588293562442, nrmse: 1.0253920138953914\n",
      "INFO logger 2024-04-27 20:23:33,997 | train_utils.py:99 | Epoch 42 [Test]: loss 1.2041310375870876e-05, mse: 0.001522787963040173, rmse: 0.03902291587055192, mae 0.02355559542775154, r2: -7.681682310709496, nrmse: 1.8022497216915572\n",
      "INFO logger 2024-04-27 20:23:33,997 | helpers.py:136 | EarlyStopping counter: 12 out of 50\n",
      "INFO logger 2024-04-27 20:23:34,380 | train_utils.py:97 | Epoch 43 [Train]: loss 0.0016097198026410445, mse: 0.0015645645326003432, rmse: 0.039554576632803734, mae 0.02263329178094864, r2: 0.7550504445929243, nrmse: 1.024983484238813\n",
      "INFO logger 2024-04-27 20:23:34,381 | train_utils.py:99 | Epoch 43 [Test]: loss 1.2048429857846894e-05, mse: 0.0015236589824780822, rmse: 0.03903407463330061, mae 0.023583577945828438, r2: -7.846720132337074, nrmse: 1.7603135966516807\n",
      "INFO logger 2024-04-27 20:23:34,381 | helpers.py:136 | EarlyStopping counter: 13 out of 50\n",
      "INFO logger 2024-04-27 20:23:34,884 | train_utils.py:97 | Epoch 44 [Train]: loss 0.0016074493171398115, mse: 0.0015631724381819367, rmse: 0.03953697558212991, mae 0.022650063037872314, r2: 0.7553753802399299, nrmse: 1.024458264918401\n",
      "INFO logger 2024-04-27 20:23:34,884 | train_utils.py:99 | Epoch 44 [Test]: loss 1.2054220860591158e-05, mse: 0.0015243703965097666, rmse: 0.03904318630068205, mae 0.023611413314938545, r2: -8.076007374960787, nrmse: 1.729503464235226\n",
      "INFO logger 2024-04-27 20:23:34,885 | helpers.py:136 | EarlyStopping counter: 14 out of 50\n",
      "INFO logger 2024-04-27 20:23:35,328 | train_utils.py:97 | Epoch 45 [Train]: loss 0.0016049131528382342, mse: 0.0015616158489137888, rmse: 0.039517285444648, mae 0.022658102214336395, r2: 0.7557345181684125, nrmse: 1.0238379876080752\n",
      "INFO logger 2024-04-27 20:23:35,328 | train_utils.py:99 | Epoch 45 [Test]: loss 1.2057433919218636e-05, mse: 0.0015247783157974482, rmse: 0.03904840990101195, mae 0.023633383214473724, r2: -8.298119766006334, nrmse: 1.7061609724832203\n",
      "INFO logger 2024-04-27 20:23:35,329 | helpers.py:136 | EarlyStopping counter: 15 out of 50\n",
      "INFO logger 2024-04-27 20:23:35,730 | train_utils.py:97 | Epoch 46 [Train]: loss 0.0016022369214138158, mse: 0.001559890923090279, rmse: 0.03949545446111842, mae 0.022657252848148346, r2: 0.7561195189026272, nrmse: 1.0231586437227533\n",
      "INFO logger 2024-04-27 20:23:35,731 | train_utils.py:99 | Epoch 46 [Test]: loss 1.2057714001037322e-05, mse: 0.0015248373383656144, rmse: 0.03904916565517904, mae 0.0236455500125885, r2: -8.466055865990509, nrmse: 1.6880302912254979\n",
      "INFO logger 2024-04-27 20:23:35,731 | helpers.py:136 | EarlyStopping counter: 16 out of 50\n",
      "INFO logger 2024-04-27 20:23:36,111 | train_utils.py:97 | Epoch 47 [Train]: loss 0.0015995741508494003, mse: 0.001558074145577848, rmse: 0.039472447929889624, mae 0.022649671882390976, r2: 0.756515895320025, nrmse: 1.0224624367627486\n",
      "INFO logger 2024-04-27 20:23:36,112 | train_utils.py:99 | Epoch 47 [Test]: loss 1.2055537319600538e-05, mse: 0.0015246059047058225, rmse: 0.03904620218031227, mae 0.023648541420698166, r2: -8.562583448083206, nrmse: 1.6743182690722143\n",
      "INFO logger 2024-04-27 20:23:36,113 | helpers.py:136 | EarlyStopping counter: 17 out of 50\n",
      "INFO logger 2024-04-27 20:23:36,501 | train_utils.py:97 | Epoch 48 [Train]: loss 0.00159704490488366, mse: 0.0015562110347673297, rmse: 0.03944884072779997, mae 0.022637855261564255, r2: 0.756908527193594, nrmse: 1.021785742539645\n",
      "INFO logger 2024-04-27 20:23:36,502 | train_utils.py:99 | Epoch 48 [Test]: loss 1.205179405215534e-05, mse: 0.0015241848304867744, rmse: 0.039040809808286184, mae 0.02364395745098591, r2: -8.59001605505992, nrmse: 1.6647761769773388\n",
      "INFO logger 2024-04-27 20:23:36,502 | helpers.py:136 | EarlyStopping counter: 18 out of 50\n",
      "INFO logger 2024-04-27 20:23:36,888 | train_utils.py:97 | Epoch 49 [Train]: loss 0.001594714722497968, mse: 0.0015543916961178184, rmse: 0.03942577451512929, mae 0.022624218836426735, r2: 0.7572857607768395, nrmse: 1.0211517265584313\n",
      "INFO logger 2024-04-27 20:23:36,889 | train_utils.py:99 | Epoch 49 [Test]: loss 1.2047414925583905e-05, mse: 0.001523686689324677, rmse: 0.039034429537584854, mae 0.02363394759595394, r2: -8.556294209952545, nrmse: 1.6588413549981043\n",
      "INFO logger 2024-04-27 20:23:36,890 | helpers.py:136 | EarlyStopping counter: 19 out of 50\n",
      "INFO logger 2024-04-27 20:23:37,263 | train_utils.py:97 | Epoch 50 [Train]: loss 0.0015926070989333437, mse: 0.001552668632939458, rmse: 0.03940391646701452, mae 0.02261003479361534, r2: 0.7576409026717755, nrmse: 1.0205686156078029\n",
      "INFO logger 2024-04-27 20:23:37,264 | train_utils.py:99 | Epoch 50 [Test]: loss 1.204318825372669e-05, mse: 0.0015232048463076353, rmse: 0.03902825702369548, mae 0.023620476946234703, r2: -8.466961986562536, nrmse: 1.6553591208461707\n",
      "INFO logger 2024-04-27 20:23:37,264 | helpers.py:136 | EarlyStopping counter: 20 out of 50\n",
      "INFO logger 2024-04-27 20:23:37,705 | train_utils.py:97 | Epoch 51 [Train]: loss 0.001590722124155734, mse: 0.0015510946977883577, rmse: 0.03938393959202606, mae 0.022596154361963272, r2: 0.7579717643716526, nrmse: 1.0200333659703726\n",
      "INFO logger 2024-04-27 20:23:37,706 | train_utils.py:99 | Epoch 51 [Test]: loss 1.203975812903039e-05, mse: 0.0015228129923343658, rmse: 0.03902323656918229, mae 0.023605167865753174, r2: -8.323013065520652, nrmse: 1.6528322574399383\n",
      "INFO logger 2024-04-27 20:23:37,706 | helpers.py:136 | EarlyStopping counter: 21 out of 50\n",
      "INFO logger 2024-04-27 20:23:38,086 | train_utils.py:97 | Epoch 52 [Train]: loss 0.001589047961112746, mse: 0.001549677923321724, rmse: 0.039365948779646144, mae 0.0225826408714056, r2: 0.7582799559546852, nrmse: 1.0195356074759223\n",
      "INFO logger 2024-04-27 20:23:38,086 | train_utils.py:99 | Epoch 52 [Test]: loss 1.2037641952925346e-05, mse: 0.0015225736424326897, rmse: 0.03902016968738975, mae 0.023588474839925766, r2: -8.122637660845818, nrmse: 1.649859174138037\n",
      "INFO logger 2024-04-27 20:23:38,087 | helpers.py:136 | EarlyStopping counter: 22 out of 50\n",
      "INFO logger 2024-04-27 20:23:38,476 | train_utils.py:97 | Epoch 53 [Train]: loss 0.0015875564543971132, mse: 0.001548428786918521, rmse: 0.039350079884525276, mae 0.02256922982633114, r2: 0.7585702666867286, nrmse: 1.0190642036028452\n",
      "INFO logger 2024-04-27 20:23:38,477 | train_utils.py:99 | Epoch 53 [Test]: loss 1.2037187141750269e-05, mse: 0.0015225278912112117, rmse: 0.039019583432056415, mae 0.023570673540234566, r2: -7.863424350370947, nrmse: 1.6455338010046745\n",
      "INFO logger 2024-04-27 20:23:38,477 | helpers.py:136 | EarlyStopping counter: 23 out of 50\n",
      "INFO logger 2024-04-27 20:23:38,860 | train_utils.py:97 | Epoch 54 [Train]: loss 0.0015861899940551496, mse: 0.0015472990926355124, rmse: 0.03933572285640004, mae 0.02255462110042572, r2: 0.7588501171037655, nrmse: 1.018613966203126\n",
      "INFO logger 2024-04-27 20:23:38,861 | train_utils.py:99 | Epoch 54 [Test]: loss 1.2038471624840661e-05, mse: 0.001522687030956149, rmse: 0.039021622607935576, mae 0.02355087921023369, r2: -7.545437432631291, nrmse: 1.6397757690458175\n",
      "INFO logger 2024-04-27 20:23:38,861 | helpers.py:136 | EarlyStopping counter: 24 out of 50\n",
      "INFO logger 2024-04-27 20:23:39,244 | train_utils.py:97 | Epoch 55 [Train]: loss 0.0015848477382437417, mse: 0.0015461958246305585, rmse: 0.039321696614344585, mae 0.02253715507686138, r2: 0.7591286786753958, nrmse: 1.0181918204540372\n",
      "INFO logger 2024-04-27 20:23:39,245 | train_utils.py:99 | Epoch 55 [Test]: loss 1.2041101840968728e-05, mse: 0.0015230055432766676, rmse: 0.039025703623082414, mae 0.023527774959802628, r2: -7.174473262228126, nrmse: 1.6334470639140237\n",
      "INFO logger 2024-04-27 20:23:39,245 | helpers.py:136 | EarlyStopping counter: 25 out of 50\n",
      "INFO logger 2024-04-27 20:23:39,695 | train_utils.py:97 | Epoch 56 [Train]: loss 0.0015833825194466382, mse: 0.0015449745114892721, rmse: 0.03930616378494946, mae 0.02251502126455307, r2: 0.7594145093999536, nrmse: 1.017824693130307\n",
      "INFO logger 2024-04-27 20:23:39,695 | train_utils.py:99 | Epoch 56 [Test]: loss 1.2044022286961629e-05, mse: 0.001523361774161458, rmse: 0.03903026741083717, mae 0.023498907685279846, r2: -6.766543046614219, nrmse: 1.628200211120917\n",
      "INFO logger 2024-04-27 20:23:39,696 | helpers.py:136 | EarlyStopping counter: 26 out of 50\n",
      "INFO logger 2024-04-27 20:23:40,098 | train_utils.py:97 | Epoch 57 [Train]: loss 0.0015816294389455728, mse: 0.001543485326692462, rmse: 0.03928721581752087, mae 0.022487618029117584, r2: 0.7597112921746794, nrmse: 1.0175617519350522\n",
      "INFO logger 2024-04-27 20:23:40,100 | train_utils.py:99 | Epoch 57 [Test]: loss 1.2045501540203944e-05, mse: 0.0015235525788739324, rmse: 0.03903271165156134, mae 0.02346094883978367, r2: -6.351821613928837, nrmse: 1.6261511226858631\n",
      "INFO logger 2024-04-27 20:23:40,100 | helpers.py:136 | EarlyStopping counter: 27 out of 50\n",
      "INFO logger 2024-04-27 20:23:40,562 | train_utils.py:97 | Epoch 58 [Train]: loss 0.0015794750318512574, mse: 0.0015416350215673447, rmse: 0.03926366031800072, mae 0.022457722574472427, r2: 0.760011617746934, nrmse: 1.0174718750501364\n",
      "INFO logger 2024-04-27 20:23:40,563 | train_utils.py:99 | Epoch 58 [Test]: loss 1.2043514288961887e-05, mse: 0.0015233411686494946, rmse: 0.03903000344157677, mae 0.02341136708855629, r2: -5.974529717463069, nrmse: 1.6291730603598995\n",
      "INFO logger 2024-04-27 20:23:40,563 | helpers.py:136 | EarlyStopping counter: 28 out of 50\n",
      "INFO logger 2024-04-27 20:23:40,945 | train_utils.py:97 | Epoch 59 [Train]: loss 0.0015769591849316067, mse: 0.0015395086957141757, rmse: 0.03923657344511847, mae 0.022432740777730942, r2: 0.7602913462135217, nrmse: 1.017633099067198\n",
      "INFO logger 2024-04-27 20:23:40,945 | train_utils.py:99 | Epoch 59 [Test]: loss 1.2036649763348584e-05, mse: 0.0015225597890093923, rmse: 0.039019992170801265, mae 0.023353321477770805, r2: -5.683104479618711, nrmse: 1.6379292693361127\n",
      "INFO logger 2024-04-27 20:23:40,946 | helpers.py:136 | EarlyStopping counter: 29 out of 50\n",
      "INFO logger 2024-04-27 20:23:41,332 | train_utils.py:97 | Epoch 60 [Train]: loss 0.0015743579948363287, mse: 0.0015373954083770514, rmse: 0.03920963412704907, mae 0.02242337539792061, r2: 0.7605100533723113, nrmse: 1.0181093639805041\n",
      "INFO logger 2024-04-27 20:23:41,332 | train_utils.py:99 | Epoch 60 [Test]: loss 1.2025109349956943e-05, mse: 0.0015212257858365774, rmse: 0.03900289458279446, mae 0.02329331263899803, r2: -5.510403936239048, nrmse: 1.6506612385286403\n",
      "INFO logger 2024-04-27 20:23:41,333 | helpers.py:136 | EarlyStopping counter: 30 out of 50\n",
      "INFO logger 2024-04-27 20:23:41,715 | train_utils.py:97 | Epoch 61 [Train]: loss 0.001572183116856417, mse: 0.001535781193524599, rmse: 0.039189044304813035, mae 0.022442413493990898, r2: 0.7606208967694369, nrmse: 1.0189179259665642\n",
      "INFO logger 2024-04-27 20:23:41,716 | train_utils.py:99 | Epoch 61 [Test]: loss 1.2011139213491492e-05, mse: 0.0015196010936051607, rmse: 0.03898206117697166, mae 0.0232407059520483, r2: -5.4575739240029515, nrmse: 1.6626674496547338\n",
      "INFO logger 2024-04-27 20:23:41,716 | helpers.py:136 | EarlyStopping counter: 31 out of 50\n",
      "INFO logger 2024-04-27 20:23:42,104 | train_utils.py:97 | Epoch 62 [Train]: loss 0.0015710739707237523, mse: 0.0015351495239883661, rmse: 0.03918098421413589, mae 0.02250080369412899, r2: 0.7605905373336609, nrmse: 1.0199895672023074\n",
      "INFO logger 2024-04-27 20:23:42,104 | train_utils.py:99 | Epoch 62 [Test]: loss 1.1998589994959065e-05, mse: 0.001518133212812245, rmse: 0.038963228983392086, mae 0.023207783699035645, r2: -5.50712750343866, nrmse: 1.66807506228163\n",
      "INFO logger 2024-04-27 20:23:42,105 | helpers.py:136 | EarlyStopping counter: 32 out of 50\n",
      "INFO logger 2024-04-27 20:23:42,493 | train_utils.py:97 | Epoch 63 [Train]: loss 0.0015715630214017014, mse: 0.0015356738585978746, rmse: 0.0391876748302049, mae 0.022599905729293823, r2: 0.7604238125298151, nrmse: 1.02114769889587\n",
      "INFO logger 2024-04-27 20:23:42,494 | train_utils.py:99 | Epoch 63 [Test]: loss 1.1992240296533788e-05, mse: 0.0015173709252849221, rmse: 0.038953445615053386, mae 0.02321438677608967, r2: -5.690244338523111, nrmse: 1.6655570969218862\n",
      "INFO logger 2024-04-27 20:23:42,494 | helpers.py:136 | EarlyStopping counter: 33 out of 50\n",
      "INFO logger 2024-04-27 20:23:42,940 | train_utils.py:97 | Epoch 64 [Train]: loss 0.0015736105009806166, mse: 0.0015369197353720665, rmse: 0.039203567890844664, mae 0.022731538861989975, r2: 0.760180832877712, nrmse: 1.0221524348850406\n",
      "INFO logger 2024-04-27 20:23:42,940 | train_utils.py:99 | Epoch 64 [Test]: loss 1.1997185841513177e-05, mse: 0.0015179075999185443, rmse: 0.03896033367309043, mae 0.023289408534765244, r2: -6.187492107667514, nrmse: 1.6649104752508954\n",
      "INFO logger 2024-04-27 20:23:42,941 | helpers.py:136 | EarlyStopping counter: 34 out of 50\n",
      "INFO logger 2024-04-27 20:23:43,348 | train_utils.py:97 | Epoch 65 [Train]: loss 0.0015758657192490382, mse: 0.0015378119423985481, rmse: 0.0392149453958379, mae 0.02286289446055889, r2: 0.7599919414683323, nrmse: 1.0227598467631211\n",
      "INFO logger 2024-04-27 20:23:43,349 | train_utils.py:99 | Epoch 65 [Test]: loss 1.2015261809370203e-05, mse: 0.001519972924143076, rmse: 0.03898683013715114, mae 0.02343648485839367, r2: -7.240037270569798, nrmse: 1.6790408978081843\n",
      "INFO logger 2024-04-27 20:23:43,349 | helpers.py:136 | EarlyStopping counter: 35 out of 50\n",
      "INFO logger 2024-04-27 20:23:43,730 | train_utils.py:97 | Epoch 66 [Train]: loss 0.0015756135325064797, mse: 0.0015369607135653496, rmse: 0.03920409052082894, mae 0.02292773686349392, r2: 0.760085986158839, nrmse: 1.022619965672157\n",
      "INFO logger 2024-04-27 20:23:43,731 | train_utils.py:99 | Epoch 66 [Test]: loss 1.2034957486236119e-05, mse: 0.0015222544316202402, rmse: 0.03901607914206962, mae 0.023592552170157433, r2: -8.603689483754943, nrmse: 1.6995596478881014\n",
      "INFO logger 2024-04-27 20:23:43,731 | helpers.py:136 | EarlyStopping counter: 36 out of 50\n",
      "INFO logger 2024-04-27 20:23:44,114 | train_utils.py:97 | Epoch 67 [Train]: loss 0.0015710682708252766, mse: 0.0015335837379097939, rmse: 0.039160997662339936, mae 0.022874388843774796, r2: 0.7606820427100655, nrmse: 1.0212948890677374\n",
      "INFO logger 2024-04-27 20:23:44,114 | train_utils.py:99 | Epoch 67 [Test]: loss 1.203360449851556e-05, mse: 0.0015220913337543607, rmse: 0.03901398894953399, mae 0.023647543042898178, r2: -9.400598848101556, nrmse: 1.7060318286566285\n",
      "INFO logger 2024-04-27 20:23:44,114 | helpers.py:136 | EarlyStopping counter: 37 out of 50\n",
      "INFO logger 2024-04-27 20:23:44,502 | train_utils.py:97 | Epoch 68 [Train]: loss 0.001563962881359763, mse: 0.001528787543065846, rmse: 0.039099712825874385, mae 0.022730467841029167, r2: 0.761668822219351, nrmse: 1.0188804610799997\n",
      "INFO logger 2024-04-27 20:23:44,502 | train_utils.py:99 | Epoch 68 [Test]: loss 1.2007054146236266e-05, mse: 0.001518959878012538, rmse: 0.03897383581343435, mae 0.023582201451063156, r2: -9.238955787316247, nrmse: 1.702507444422366\n",
      "INFO logger 2024-04-27 20:23:44,503 | helpers.py:136 | EarlyStopping counter: 38 out of 50\n",
      "INFO logger 2024-04-27 20:23:44,890 | train_utils.py:97 | Epoch 69 [Train]: loss 0.0015578257287965835, mse: 0.0015247989213094115, rmse: 0.03904867374584458, mae 0.022578544914722443, r2: 0.7626383593128059, nrmse: 1.016209022549527\n",
      "INFO logger 2024-04-27 20:23:44,891 | train_utils.py:99 | Epoch 69 [Test]: loss 1.1975106118418472e-05, mse: 0.0015151825500652194, rmse: 0.03892534585671936, mae 0.023468125611543655, r2: -8.590922056917762, nrmse: 1.7039028359482409\n",
      "INFO logger 2024-04-27 20:23:44,892 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2024-04-27 20:23:45,290 | train_utils.py:97 | Epoch 70 [Train]: loss 0.0015544953796348257, mse: 0.0015226966934278607, rmse: 0.039021746416938605, mae 0.022468803450465202, r2: 0.7633422497417159, nrmse: 1.013934634552694\n",
      "INFO logger 2024-04-27 20:23:45,290 | train_utils.py:99 | Epoch 70 [Test]: loss 1.1953911041141057e-05, mse: 0.0015126672806218266, rmse: 0.038893023546927113, mae 0.02336963638663292, r2: -7.898457670132157, nrmse: 1.7058332115245125\n",
      "INFO logger 2024-04-27 20:23:45,291 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2024-04-27 20:23:45,754 | train_utils.py:97 | Epoch 71 [Train]: loss 0.0015536171654064166, mse: 0.0015218660701066256, rmse: 0.03901110188275417, mae 0.02239547111093998, r2: 0.7638498304461819, nrmse: 1.012075316014898\n",
      "INFO logger 2024-04-27 20:23:45,755 | train_utils.py:99 | Epoch 71 [Test]: loss 1.1946478288767762e-05, mse: 0.0015117768198251724, rmse: 0.038881574297154844, mae 0.023300854489207268, r2: -7.248202974899618, nrmse: 1.6962051329065517\n",
      "INFO logger 2024-04-27 20:23:45,756 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2024-04-27 20:23:46,146 | train_utils.py:97 | Epoch 72 [Train]: loss 0.001553751958475237, mse: 0.001520665013231337, rmse: 0.03899570506134409, mae 0.022329291328787804, r2: 0.7643392864393788, nrmse: 1.0105981753202526\n",
      "INFO logger 2024-04-27 20:23:46,146 | train_utils.py:99 | Epoch 72 [Test]: loss 1.1948986111374909e-05, mse: 0.0015120696043595672, rmse: 0.038885339195634736, mae 0.023249104619026184, r2: -6.643774800952974, nrmse: 1.6762043032203366\n",
      "INFO logger 2024-04-27 20:23:46,147 | helpers.py:136 | EarlyStopping counter: 1 out of 50\n",
      "INFO logger 2024-04-27 20:23:46,533 | train_utils.py:97 | Epoch 73 [Train]: loss 0.0015528882796483822, mse: 0.0015174468280747533, rmse: 0.0389544198785549, mae 0.022251632064580917, r2: 0.7649131209233861, nrmse: 1.0097791114410488\n",
      "INFO logger 2024-04-27 20:23:46,533 | train_utils.py:99 | Epoch 73 [Test]: loss 1.1952657746804649e-05, mse: 0.0015125216450542212, rmse: 0.03889115124362123, mae 0.02319701761007309, r2: -6.169341212571247, nrmse: 1.6610364821443282\n",
      "INFO logger 2024-04-27 20:23:46,534 | helpers.py:136 | EarlyStopping counter: 2 out of 50\n",
      "INFO logger 2024-04-27 20:23:46,915 | train_utils.py:97 | Epoch 74 [Train]: loss 0.0015492808464270722, mse: 0.0015121509786695242, rmse: 0.03888638551819292, mae 0.022175287827849388, r2: 0.765477638179408, nrmse: 1.0100100416122562\n",
      "INFO logger 2024-04-27 20:23:46,916 | train_utils.py:99 | Epoch 74 [Test]: loss 1.1945314943989226e-05, mse: 0.0015116940485313535, rmse: 0.03888050988003312, mae 0.02312718704342842, r2: -5.938477577477985, nrmse: 1.6663392259726317\n",
      "INFO logger 2024-04-27 20:23:46,916 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2024-04-27 20:23:47,327 | train_utils.py:97 | Epoch 75 [Train]: loss 0.001543356532396882, mse: 0.0015069690998643637, rmse: 0.038819699894053324, mae 0.02214699424803257, r2: 0.7657723866933791, nrmse: 1.0114559416955466\n",
      "INFO logger 2024-04-27 20:23:47,328 | train_utils.py:99 | Epoch 75 [Test]: loss 1.1921902614365174e-05, mse: 0.0015089684166014194, rmse: 0.03884544267480317, mae 0.023040590807795525, r2: -5.985870751673815, nrmse: 1.6920037818878695\n",
      "INFO logger 2024-04-27 20:23:47,328 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2024-04-27 20:23:47,711 | train_utils.py:97 | Epoch 76 [Train]: loss 0.0015382304451905353, mse: 0.0015048588393256068, rmse: 0.03879251009313018, mae 0.022222161293029785, r2: 0.765516626447663, nrmse: 1.0141165583246867\n",
      "INFO logger 2024-04-27 20:23:47,712 | train_utils.py:99 | Epoch 76 [Test]: loss 1.189221913261851e-05, mse: 0.001505480264313519, rmse: 0.03880051886655021, mae 0.02296157367527485, r2: -6.210195542770724, nrmse: 1.7192449646179515\n",
      "INFO logger 2024-04-27 20:23:47,712 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2024-04-27 20:23:48,096 | train_utils.py:97 | Epoch 77 [Train]: loss 0.0015372559763564759, mse: 0.0015077028656378388, rmse: 0.03882914968986365, mae 0.022425411269068718, r2: 0.7645262089994362, nrmse: 1.0179458913041166\n",
      "INFO logger 2024-04-27 20:23:48,097 | train_utils.py:99 | Epoch 77 [Test]: loss 1.1870201063690958e-05, mse: 0.0015028637135401368, rmse: 0.0387667862162978, mae 0.022927816957235336, r2: -6.471251910522429, nrmse: 1.7249243396480156\n",
      "INFO logger 2024-04-27 20:23:48,097 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2024-04-27 20:23:48,553 | train_utils.py:97 | Epoch 78 [Train]: loss 0.001541939217849028, mse: 0.0015147464582696557, rmse: 0.038919743810431943, mae 0.022743869572877884, r2: 0.7629162262663994, nrmse: 1.0223275471823425\n",
      "INFO logger 2024-04-27 20:23:48,554 | train_utils.py:99 | Epoch 78 [Test]: loss 1.1867534701796847e-05, mse: 0.0015024759341031313, rmse: 0.03876178445457757, mae 0.02300041913986206, r2: -7.0216907629004455, nrmse: 1.7149863269270471\n",
      "INFO logger 2024-04-27 20:23:48,554 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2024-04-27 20:23:48,935 | train_utils.py:97 | Epoch 79 [Train]: loss 0.0015505281205235428, mse: 0.0015217848122119904, rmse: 0.03901006039744095, mae 0.02309836633503437, r2: 0.7612656348363366, nrmse: 1.0260776050403342\n",
      "INFO logger 2024-04-27 20:23:48,936 | train_utils.py:99 | Epoch 79 [Test]: loss 1.1900054190133573e-05, mse: 0.0015062106540426612, rmse: 0.0388099298381569, mae 0.023275483399629593, r2: -9.005474571066184, nrmse: 1.7407011397563168\n",
      "INFO logger 2024-04-27 20:23:48,936 | helpers.py:136 | EarlyStopping counter: 1 out of 50\n",
      "INFO logger 2024-04-27 20:23:49,328 | train_utils.py:97 | Epoch 80 [Train]: loss 0.0015557443853986327, mse: 0.0015233894810080528, rmse: 0.039030622349740376, mae 0.02329891361296177, r2: 0.7604988352475702, nrmse: 1.0281251068327621\n",
      "INFO logger 2024-04-27 20:23:49,328 | train_utils.py:99 | Epoch 80 [Test]: loss 1.1956345827600576e-05, mse: 0.0015128585509955883, rmse: 0.03889548239828873, mae 0.023630458861589432, r2: -12.52051544695687, nrmse: 1.8196853513915145\n",
      "INFO logger 2024-04-27 20:23:49,328 | helpers.py:136 | EarlyStopping counter: 2 out of 50\n",
      "INFO logger 2024-04-27 20:23:49,706 | train_utils.py:97 | Epoch 81 [Train]: loss 0.0015504577358548654, mse: 0.0015173808205872774, rmse: 0.03895357262931447, mae 0.023153284564614296, r2: 0.761642710518351, nrmse: 1.0260250124324461\n",
      "INFO logger 2024-04-27 20:23:49,706 | train_utils.py:99 | Epoch 81 [Test]: loss 1.1963686481668281e-05, mse: 0.0015137969749048352, rmse: 0.03890754393308366, mae 0.02371811307966709, r2: -13.721389088496304, nrmse: 1.8608774528554297\n",
      "INFO logger 2024-04-27 20:23:49,707 | helpers.py:136 | EarlyStopping counter: 3 out of 50\n",
      "INFO logger 2024-04-27 20:23:50,074 | train_utils.py:97 | Epoch 82 [Train]: loss 0.0015403785451717552, mse: 0.0015099046286195517, rmse: 0.03885749128056972, mae 0.022799549624323845, r2: 0.7640467294171265, nrmse: 1.0193609461263966\n",
      "INFO logger 2024-04-27 20:23:50,075 | train_utils.py:99 | Epoch 82 [Test]: loss 1.1929040371334148e-05, mse: 0.0015097135910764337, rmse: 0.038855033021172866, mae 0.02353556454181671, r2: -11.611505093836081, nrmse: 1.8287989584768218\n",
      "INFO logger 2024-04-27 20:23:50,075 | helpers.py:136 | EarlyStopping counter: 4 out of 50\n",
      "INFO logger 2024-04-27 20:23:50,463 | train_utils.py:97 | Epoch 83 [Train]: loss 0.0015353010497662137, mse: 0.0015065375482663512, rmse: 0.03881414108628904, mae 0.022543255239725113, r2: 0.7657987712819777, nrmse: 1.013369638427713\n",
      "INFO logger 2024-04-27 20:23:50,463 | train_utils.py:99 | Epoch 83 [Test]: loss 1.1917704833591118e-05, mse: 0.0015083550242707133, rmse: 0.03883754657893201, mae 0.02337581105530262, r2: -9.370065502778266, nrmse: 1.7724787183080877\n",
      "INFO logger 2024-04-27 20:23:50,464 | helpers.py:136 | EarlyStopping counter: 5 out of 50\n",
      "INFO logger 2024-04-27 20:23:50,922 | train_utils.py:97 | Epoch 84 [Train]: loss 0.001535190002535929, mse: 0.0015041319420561194, rmse: 0.03878313992002349, mae 0.022424785420298576, r2: 0.7666830662603077, nrmse: 1.0109199362719554\n",
      "INFO logger 2024-04-27 20:23:50,923 | train_utils.py:99 | Epoch 84 [Test]: loss 1.1933583738820149e-05, mse: 0.0015102582983672619, rmse: 0.03886204187079292, mae 0.02331387624144554, r2: -8.225405148591367, nrmse: 1.7449527539057135\n",
      "INFO logger 2024-04-27 20:23:50,923 | helpers.py:136 | EarlyStopping counter: 6 out of 50\n",
      "INFO logger 2024-04-27 20:23:51,305 | train_utils.py:97 | Epoch 85 [Train]: loss 0.001533092076336944, mse: 0.0014979987172409892, rmse: 0.0387039883893248, mae 0.022350149229168892, r2: 0.7673567308617032, nrmse: 1.0107866021685024\n",
      "INFO logger 2024-04-27 20:23:51,306 | train_utils.py:99 | Epoch 85 [Test]: loss 1.1934742360608652e-05, mse: 0.0015104502672329545, rmse: 0.038864511668525496, mae 0.02326904982328415, r2: -7.869283637080295, nrmse: 1.7289901444638271\n",
      "INFO logger 2024-04-27 20:23:51,307 | helpers.py:136 | EarlyStopping counter: 7 out of 50\n",
      "INFO logger 2024-04-27 20:23:51,688 | train_utils.py:97 | Epoch 86 [Train]: loss 0.0015275019708771947, mse: 0.0014931330224499106, rmse: 0.0386410794679692, mae 0.022357920184731483, r2: 0.7672913887591022, nrmse: 1.0133501912423786\n",
      "INFO logger 2024-04-27 20:23:51,689 | train_utils.py:99 | Epoch 86 [Test]: loss 1.1915798298674173e-05, mse: 0.001508243614807725, rmse: 0.03883611225145644, mae 0.023214546963572502, r2: -8.095648795279992, nrmse: 1.754736196957585\n",
      "INFO logger 2024-04-27 20:23:51,689 | helpers.py:136 | EarlyStopping counter: 8 out of 50\n",
      "INFO logger 2024-04-27 20:23:52,082 | train_utils.py:97 | Epoch 87 [Train]: loss 0.0015217254286907583, mse: 0.001490606926381588, rmse: 0.038608378965991154, mae 0.02246847003698349, r2: 0.7667299803199913, nrmse: 1.016319968595052\n",
      "INFO logger 2024-04-27 20:23:52,083 | train_utils.py:99 | Epoch 87 [Test]: loss 1.1877987104228409e-05, mse: 0.0015037416014820337, rmse: 0.03877810724470747, mae 0.02318279817700386, r2: -8.652597386856185, nrmse: 1.7639518188806074\n",
      "INFO logger 2024-04-27 20:23:52,083 | helpers.py:136 | EarlyStopping counter: 9 out of 50\n",
      "INFO logger 2024-04-27 20:23:52,493 | train_utils.py:97 | Epoch 88 [Train]: loss 0.0015223490107251394, mse: 0.001495931064710021, rmse: 0.038677268061614964, mae 0.0227352324873209, r2: 0.7649934560586725, nrmse: 1.0217942759134817\n",
      "INFO logger 2024-04-27 20:23:52,494 | train_utils.py:99 | Epoch 88 [Test]: loss 1.1878752675382638e-05, mse: 0.0015037801349535584, rmse: 0.03877860408722261, mae 0.023282384499907494, r2: -10.194689870389544, nrmse: 1.8667484723627876\n",
      "INFO logger 2024-04-27 20:23:52,495 | helpers.py:136 | EarlyStopping counter: 10 out of 50\n",
      "INFO logger 2024-04-27 20:23:52,874 | train_utils.py:97 | Epoch 89 [Train]: loss 0.0015245375791278623, mse: 0.0014986168825998902, rmse: 0.038711973375170246, mae 0.023025888949632645, r2: 0.7632828008334404, nrmse: 1.0257987099709691\n",
      "INFO logger 2024-04-27 20:23:52,875 | train_utils.py:99 | Epoch 89 [Test]: loss 1.1880222652842731e-05, mse: 0.001503929728642106, rmse: 0.03878053285660353, mae 0.02354465238749981, r2: -14.023532698652408, nrmse: 1.946585409242854\n",
      "INFO logger 2024-04-27 20:23:52,875 | helpers.py:136 | EarlyStopping counter: 11 out of 50\n",
      "INFO logger 2024-04-27 20:23:53,257 | train_utils.py:97 | Epoch 90 [Train]: loss 0.0015310376668842996, mse: 0.0015088224317878485, rmse: 0.03884356358250165, mae 0.023217247799038887, r2: 0.7616691296161826, nrmse: 1.0308226438774906\n",
      "INFO logger 2024-04-27 20:23:53,257 | train_utils.py:99 | Epoch 90 [Test]: loss 1.1940889225759386e-05, mse: 0.001511070877313614, rmse: 0.03887249512590636, mae 0.023701103404164314, r2: -15.014845402829092, nrmse: 2.1224520743842326\n",
      "INFO logger 2024-04-27 20:23:53,258 | helpers.py:136 | EarlyStopping counter: 12 out of 50\n",
      "INFO logger 2024-04-27 20:23:53,693 | train_utils.py:97 | Epoch 91 [Train]: loss 0.0015275730385288034, mse: 0.0014958380488678813, rmse: 0.03867606558154386, mae 0.02296142838895321, r2: 0.7628610366068207, nrmse: 1.0248632808487852\n",
      "INFO logger 2024-04-27 20:23:53,694 | train_utils.py:99 | Epoch 91 [Test]: loss 1.1906671439561644e-05, mse: 0.0015072801616042852, rmse: 0.03882370618068663, mae 0.023919934406876564, r2: -21.925707741927294, nrmse: 2.0258012279188873\n",
      "INFO logger 2024-04-27 20:23:53,694 | helpers.py:136 | EarlyStopping counter: 13 out of 50\n",
      "INFO logger 2024-04-27 20:23:54,069 | train_utils.py:97 | Epoch 92 [Train]: loss 0.0015423244165207886, mse: 0.001534243463538587, rmse: 0.039169420005133945, mae 0.023485053330659866, r2: 0.7571385683559321, nrmse: 1.0413707041579912\n",
      "INFO logger 2024-04-27 20:23:54,069 | train_utils.py:99 | Epoch 92 [Test]: loss 1.2093128490790027e-05, mse: 0.0015294349286705256, rmse: 0.03910799059873219, mae 0.023929977789521217, r2: -19.351590251022174, nrmse: 2.7201791409441562\n",
      "INFO logger 2024-04-27 20:23:54,070 | helpers.py:136 | EarlyStopping counter: 14 out of 50\n",
      "INFO logger 2024-04-27 20:23:54,465 | train_utils.py:97 | Epoch 93 [Train]: loss 0.001542732589959362, mse: 0.0015061877202242613, rmse: 0.03880963437375134, mae 0.02185225300490856, r2: 0.763978817766451, nrmse: 1.0095112139261053\n",
      "INFO logger 2024-04-27 20:23:54,466 | train_utils.py:99 | Epoch 93 [Test]: loss 1.1825246261725507e-05, mse: 0.001497287186793983, rmse: 0.038694795345032944, mae 0.02301461435854435, r2: -6.732743733366324, nrmse: 1.4278936882929765\n",
      "INFO logger 2024-04-27 20:23:54,466 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2024-04-27 20:23:54,856 | train_utils.py:97 | Epoch 94 [Train]: loss 0.0015775301462569989, mse: 0.0015115808928385377, rmse: 0.03887905468036148, mae 0.02301599830389023, r2: 0.7621480484595813, nrmse: 1.0340738956361493\n",
      "INFO logger 2024-04-27 20:23:54,856 | train_utils.py:99 | Epoch 94 [Test]: loss 1.197763153522722e-05, mse: 0.0015154677676036954, rmse: 0.03892900933242067, mae 0.023640314117074013, r2: -12.34521763887812, nrmse: 2.0515118062572677\n",
      "INFO logger 2024-04-27 20:23:54,857 | helpers.py:136 | EarlyStopping counter: 1 out of 50\n",
      "INFO logger 2024-04-27 20:23:55,228 | train_utils.py:97 | Epoch 95 [Train]: loss 0.0015189809559541213, mse: 0.0014743449864909053, rmse: 0.03839720024286804, mae 0.021723879501223564, r2: 0.7718299504575189, nrmse: 0.9993814754092302\n",
      "INFO logger 2024-04-27 20:23:55,228 | train_utils.py:99 | Epoch 95 [Test]: loss 1.1806255246462213e-05, mse: 0.001494756666943431, rmse: 0.03866208306523888, mae 0.023077337071299553, r2: -8.113567925556463, nrmse: 1.5355555408813473\n",
      "INFO logger 2024-04-27 20:23:55,229 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2024-04-27 20:23:55,595 | train_utils.py:97 | Epoch 96 [Train]: loss 0.0015236960322041807, mse: 0.0014796246541664004, rmse: 0.038465889488823735, mae 0.022313091903924942, r2: 0.7700827174347107, nrmse: 1.0085703015482193\n",
      "INFO logger 2024-04-27 20:23:55,596 | train_utils.py:99 | Epoch 96 [Test]: loss 1.1876147605985272e-05, mse: 0.0015032402006909251, rmse: 0.03877164170745063, mae 0.023482758551836014, r2: -11.853266114148155, nrmse: 1.7865911840502406\n",
      "INFO logger 2024-04-27 20:23:55,597 | helpers.py:136 | EarlyStopping counter: 1 out of 50\n",
      "INFO logger 2024-04-27 20:23:56,044 | train_utils.py:97 | Epoch 97 [Train]: loss 0.0015033936047392524, mse: 0.0014689152594655752, rmse: 0.03832643029901918, mae 0.021935544908046722, r2: 0.7724484950753094, nrmse: 1.0003659958155735\n",
      "INFO logger 2024-04-27 20:23:56,044 | train_utils.py:99 | Epoch 97 [Test]: loss 1.1821773225899068e-05, mse: 0.0014966941671445966, rmse: 0.03868713180302459, mae 0.023246997967362404, r2: -10.038926482068032, nrmse: 1.6116358727621447\n",
      "INFO logger 2024-04-27 20:23:56,045 | helpers.py:136 | EarlyStopping counter: 2 out of 50\n",
      "INFO logger 2024-04-27 20:23:56,441 | train_utils.py:97 | Epoch 98 [Train]: loss 0.0015035665816423378, mse: 0.0014715725556015968, rmse: 0.038361081262154186, mae 0.02210818976163864, r2: 0.7714995122132459, nrmse: 1.0044728524426443\n",
      "INFO logger 2024-04-27 20:23:56,441 | train_utils.py:99 | Epoch 98 [Test]: loss 1.1831474609091627e-05, mse: 0.0014979068655520678, rmse: 0.03870280177909692, mae 0.02328116074204445, r2: -10.183453292063808, nrmse: 1.6981955451339497\n",
      "INFO logger 2024-04-27 20:23:56,442 | helpers.py:136 | EarlyStopping counter: 3 out of 50\n",
      "INFO logger 2024-04-27 20:23:56,842 | train_utils.py:97 | Epoch 99 [Train]: loss 0.001498335052474431, mse: 0.0014677969738841057, rmse: 0.03831183856047769, mae 0.02196892723441124, r2: 0.7725009155353575, nrmse: 1.0007569532882068\n",
      "INFO logger 2024-04-27 20:23:56,843 | train_utils.py:99 | Epoch 99 [Test]: loss 1.1809479155201922e-05, mse: 0.001495257718488574, rmse: 0.03866856240524819, mae 0.023154277354478836, r2: -8.95544373183619, nrmse: 1.6247568891668738\n",
      "INFO logger 2024-04-27 20:23:56,843 | helpers.py:136 | EarlyStopping counter: 4 out of 50\n",
      "INFO logger 2024-04-27 20:23:57,241 | train_utils.py:97 | Epoch 100 [Train]: loss 0.0015005322117684595, mse: 0.0014710340183228254, rmse: 0.038354061301547004, mae 0.02215050719678402, r2: 0.7715299079780411, nrmse: 1.004622352259397\n",
      "INFO logger 2024-04-27 20:23:57,242 | train_utils.py:99 | Epoch 100 [Test]: loss 1.182452007009626e-05, mse: 0.0014970802003517747, rmse: 0.03869212064945232, mae 0.023227745667099953, r2: -9.671675711777553, nrmse: 1.715310022958275\n",
      "INFO logger 2024-04-27 20:23:57,242 | helpers.py:136 | EarlyStopping counter: 5 out of 50\n",
      "INFO logger 2024-04-27 20:23:57,630 | train_utils.py:97 | Epoch 101 [Train]: loss 0.0014992816366239546, mse: 0.0014690639218315482, rmse: 0.0383283696735401, mae 0.02211892232298851, r2: 0.772041779156062, nrmse: 1.002859418493634\n",
      "INFO logger 2024-04-27 20:23:57,630 | train_utils.py:99 | Epoch 101 [Test]: loss 1.1820771356320216e-05, mse: 0.0014966208254918456, rmse: 0.038686183909657536, mae 0.023205557838082314, r2: -9.43089200939806, nrmse: 1.692841558889033\n",
      "INFO logger 2024-04-27 20:23:57,630 | helpers.py:136 | EarlyStopping counter: 6 out of 50\n",
      "INFO logger 2024-04-27 20:23:58,003 | train_utils.py:97 | Epoch 102 [Train]: loss 0.0015011453521849966, mse: 0.0014714626595377922, rmse: 0.038359648845339975, mae 0.022271960973739624, r2: 0.7711961525876079, nrmse: 1.0064846751999421\n",
      "INFO logger 2024-04-27 20:23:58,003 | train_utils.py:99 | Epoch 102 [Test]: loss 1.1838335666923963e-05, mse: 0.0014987471513450146, rmse: 0.0387136558767706, mae 0.02329094521701336, r2: -10.343297323055634, nrmse: 1.784001425782637\n",
      "INFO logger 2024-04-27 20:23:58,004 | helpers.py:136 | EarlyStopping counter: 7 out of 50\n",
      "INFO logger 2024-04-27 20:23:58,413 | train_utils.py:97 | Epoch 103 [Train]: loss 0.00150013300008352, mse: 0.0014688672963529825, rmse: 0.03832580457541606, mae 0.022242438048124313, r2: 0.7717070137869324, nrmse: 1.0051458949404788\n",
      "INFO logger 2024-04-27 20:23:58,413 | train_utils.py:99 | Epoch 103 [Test]: loss 1.1837220849804253e-05, mse: 0.001498606288805604, rmse: 0.0387118365465345, mae 0.023291271179914474, r2: -10.338083255210421, nrmse: 1.7735863934801581\n",
      "INFO logger 2024-04-27 20:23:58,414 | helpers.py:136 | EarlyStopping counter: 8 out of 50\n",
      "INFO logger 2024-04-27 20:23:58,863 | train_utils.py:97 | Epoch 104 [Train]: loss 0.0015005699919985874, mse: 0.0014700156170874834, rmse: 0.03834078268746588, mae 0.022349195554852486, r2: 0.7710619488999335, nrmse: 1.0082948614359553\n",
      "INFO logger 2024-04-27 20:23:58,864 | train_utils.py:99 | Epoch 104 [Test]: loss 1.1850458624968796e-05, mse: 0.001500213285908103, rmse: 0.03873258687343389, mae 0.023357069119811058, r2: -11.135936534633036, nrmse: 1.8516889348957752\n",
      "INFO logger 2024-04-27 20:23:58,865 | helpers.py:136 | EarlyStopping counter: 9 out of 50\n",
      "INFO logger 2024-04-27 20:23:59,243 | train_utils.py:97 | Epoch 105 [Train]: loss 0.001498891302389893, mse: 0.001466991612687707, rmse: 0.03830132651342127, mae 0.022308947518467903, r2: 0.7716245867858362, nrmse: 1.0069485930496989\n",
      "INFO logger 2024-04-27 20:23:59,243 | train_utils.py:99 | Epoch 105 [Test]: loss 1.1845121348388383e-05, mse: 0.0014995549572631717, rmse: 0.038724087558820175, mae 0.023348985239863396, r2: -11.072332111612695, nrmse: 1.8373460746033912\n",
      "INFO logger 2024-04-27 20:23:59,244 | helpers.py:136 | EarlyStopping counter: 10 out of 50\n",
      "INFO logger 2024-04-27 20:23:59,632 | train_utils.py:97 | Epoch 106 [Train]: loss 0.0014985333518621321, mse: 0.0014678268926218152, rmse: 0.038312229021838645, mae 0.022391224279999733, r2: 0.7710789660219806, nrmse: 1.0097026676284162\n",
      "INFO logger 2024-04-27 20:23:59,633 | train_utils.py:99 | Epoch 106 [Test]: loss 1.1853209702837231e-05, mse: 0.0015005317982286215, rmse: 0.03873669833928314, mae 0.023394253104925156, r2: -11.691294402190346, nrmse: 1.9001107151777612\n",
      "INFO logger 2024-04-27 20:23:59,633 | helpers.py:136 | EarlyStopping counter: 11 out of 50\n",
      "INFO logger 2024-04-27 20:24:00,008 | train_utils.py:97 | Epoch 107 [Train]: loss 0.0014967533916734643, mse: 0.0014649111544713378, rmse: 0.038274157789183784, mae 0.022351257503032684, r2: 0.7716615645496119, nrmse: 1.0082016429888623\n",
      "INFO logger 2024-04-27 20:24:00,008 | train_utils.py:99 | Epoch 107 [Test]: loss 1.1844332247382923e-05, mse: 0.001499429577961564, rmse: 0.038722468644981356, mae 0.02338261529803276, r2: -11.59488267103978, nrmse: 1.8808509768922996\n",
      "INFO logger 2024-04-27 20:24:00,009 | helpers.py:136 | EarlyStopping counter: 12 out of 50\n",
      "INFO logger 2024-04-27 20:24:00,397 | train_utils.py:97 | Epoch 108 [Train]: loss 0.0014963365185690766, mse: 0.0014659948647022247, rmse: 0.038288312377306795, mae 0.02241714484989643, r2: 0.7711607004663354, nrmse: 1.0105911777145666\n",
      "INFO logger 2024-04-27 20:24:00,397 | train_utils.py:99 | Epoch 108 [Test]: loss 1.1850533724765222e-05, mse: 0.0015001704450696707, rmse: 0.03873203383595639, mae 0.023414302617311478, r2: -12.061599290522441, nrmse: 1.9333060605283252\n",
      "INFO logger 2024-04-27 20:24:00,398 | helpers.py:136 | EarlyStopping counter: 13 out of 50\n",
      "INFO logger 2024-04-27 20:24:00,786 | train_utils.py:97 | Epoch 109 [Train]: loss 0.0014944504366614512, mse: 0.001462653512135148, rmse: 0.038244653379722875, mae 0.02235623262822628, r2: 0.7719112653714639, nrmse: 1.0084134339942132\n",
      "INFO logger 2024-04-27 20:24:00,786 | train_utils.py:99 | Epoch 109 [Test]: loss 1.1837938233695336e-05, mse: 0.0014986138558015227, rmse: 0.03871193428132367, mae 0.0233897902071476, r2: -11.819649424633202, nrmse: 1.9001031312868621\n",
      "INFO logger 2024-04-27 20:24:00,787 | helpers.py:136 | EarlyStopping counter: 14 out of 50\n",
      "INFO logger 2024-04-27 20:24:01,226 | train_utils.py:97 | Epoch 110 [Train]: loss 0.0014938907002615028, mse: 0.0014642000896856189, rmse: 0.03826486756393675, mae 0.022382918745279312, r2: 0.7715222347837418, nrmse: 1.0101432250099056\n",
      "INFO logger 2024-04-27 20:24:01,227 | train_utils.py:99 | Epoch 110 [Test]: loss 1.1844131924526505e-05, mse: 0.0014993611257523298, rmse: 0.03872158475259412, mae 0.02338993176817894, r2: -11.874410021182669, nrmse: 1.938378477763609\n",
      "INFO logger 2024-04-27 20:24:01,227 | helpers.py:136 | EarlyStopping counter: 15 out of 50\n",
      "INFO logger 2024-04-27 20:24:01,613 | train_utils.py:97 | Epoch 111 [Train]: loss 0.0014915768413756754, mse: 0.0014597555855289102, rmse: 0.038206747905689516, mae 0.022282799705863, r2: 0.7725979222271331, nrmse: 1.0067844080318962\n",
      "INFO logger 2024-04-27 20:24:01,614 | train_utils.py:99 | Epoch 111 [Test]: loss 1.1825216860048832e-05, mse: 0.0014970401534810662, rmse: 0.03869160313919632, mae 0.02333405241370201, r2: -11.229984465817703, nrmse: 1.8724483527542095\n",
      "INFO logger 2024-04-27 20:24:01,614 | helpers.py:136 | EarlyStopping counter: 16 out of 50\n",
      "INFO logger 2024-04-27 20:24:02,010 | train_utils.py:97 | Epoch 112 [Train]: loss 0.0014914198428985317, mse: 0.001463482971303165, rmse: 0.03825549596205969, mae 0.022288551554083824, r2: 0.7721317677936984, nrmse: 1.0082084831854192\n",
      "INFO logger 2024-04-27 20:24:02,011 | train_utils.py:99 | Epoch 112 [Test]: loss 1.183960517260453e-05, mse: 0.001498787198215723, rmse: 0.03871417309223746, mae 0.023301441222429276, r2: -10.89094162086289, nrmse: 1.922465974246491\n",
      "INFO logger 2024-04-27 20:24:02,011 | helpers.py:136 | EarlyStopping counter: 17 out of 50\n",
      "INFO logger 2024-04-27 20:24:02,411 | train_utils.py:97 | Epoch 113 [Train]: loss 0.0014893036599206361, mse: 0.0014570477651432157, rmse: 0.03817129504147345, mae 0.022166062146425247, r2: 0.7734863998855317, nrmse: 1.0040070333957836\n",
      "INFO logger 2024-04-27 20:24:02,412 | train_utils.py:99 | Epoch 113 [Test]: loss 1.1810706558192083e-05, mse: 0.0014952535275369883, rmse: 0.038668508214527594, mae 0.023229192942380905, r2: -9.909704338993315, nrmse: 1.8045941336920255\n",
      "INFO logger 2024-04-27 20:24:02,412 | helpers.py:136 | EarlyStopping counter: 18 out of 50\n",
      "INFO logger 2024-04-27 20:24:02,799 | train_utils.py:97 | Epoch 114 [Train]: loss 0.0014907942820786427, mse: 0.0014657587744295597, rmse: 0.0382852291939014, mae 0.022247314453125, r2: 0.7722735293460241, nrmse: 1.0073551713396378\n",
      "INFO logger 2024-04-27 20:24:02,800 | train_utils.py:99 | Epoch 114 [Test]: loss 1.1853674236563856e-05, mse: 0.0015004824381321669, rmse: 0.0387360612108687, mae 0.02322438172996044, r2: -10.273435344103234, nrmse: 1.9710761484480819\n",
      "INFO logger 2024-04-27 20:24:02,800 | helpers.py:136 | EarlyStopping counter: 19 out of 50\n",
      "INFO logger 2024-04-27 20:24:03,192 | train_utils.py:97 | Epoch 115 [Train]: loss 0.0014892559773342813, mse: 0.0014542739372700453, rmse: 0.038134943782180214, mae 0.02206582948565483, r2: 0.7740124688691694, nrmse: 1.0026317925298796\n",
      "INFO logger 2024-04-27 20:24:03,192 | train_utils.py:99 | Epoch 115 [Test]: loss 1.1794049189322524e-05, mse: 0.0014932049671187997, rmse: 0.03864201039178474, mae 0.02312256023287773, r2: -8.64258556100422, nrmse: 1.7289101651146805\n",
      "INFO logger 2024-04-27 20:24:03,193 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2024-04-27 20:24:03,567 | train_utils.py:97 | Epoch 116 [Train]: loss 0.001493012436058475, mse: 0.001470018643885851, rmse: 0.038340822159753575, mae 0.022383173927664757, r2: 0.7703133738530422, nrmse: 1.0155599582974595\n",
      "INFO logger 2024-04-27 20:24:03,568 | train_utils.py:99 | Epoch 116 [Test]: loss 1.1885648091903708e-05, mse: 0.0015044150641188025, rmse: 0.03878678981455932, mae 0.023278091102838516, r2: -12.078410291176985, nrmse: 2.1840127597694834\n",
      "INFO logger 2024-04-27 20:24:03,568 | helpers.py:136 | EarlyStopping counter: 1 out of 50\n",
      "INFO logger 2024-04-27 20:24:04,008 | train_utils.py:97 | Epoch 117 [Train]: loss 0.0014918117077011334, mse: 0.0014537486713379622, rmse: 0.03812805622291755, mae 0.021947113797068596, r2: 0.7737824141186412, nrmse: 1.0029881914735386\n",
      "INFO logger 2024-04-27 20:24:04,008 | train_utils.py:99 | Epoch 117 [Test]: loss 1.1770375584740987e-05, mse: 0.0014902513939887285, rmse: 0.03860377434900283, mae 0.023017626255750656, r2: -7.839439935025068, nrmse: 1.6467391105295466\n",
      "INFO logger 2024-04-27 20:24:04,009 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2024-04-27 20:24:04,400 | train_utils.py:97 | Epoch 118 [Train]: loss 0.0015016460918474855, mse: 0.0014751633862033486, rmse: 0.03840785578762955, mae 0.022688675671815872, r2: 0.7662221085659847, nrmse: 1.0346163106829396\n",
      "INFO logger 2024-04-27 20:24:04,401 | train_utils.py:99 | Epoch 118 [Test]: loss 1.1874206180511348e-05, mse: 0.0015030141221359372, rmse: 0.03876872608348045, mae 0.023387305438518524, r2: -13.747428648261527, nrmse: 2.2981260998182926\n",
      "INFO logger 2024-04-27 20:24:04,401 | helpers.py:136 | EarlyStopping counter: 1 out of 50\n",
      "INFO logger 2024-04-27 20:24:04,786 | train_utils.py:97 | Epoch 119 [Train]: loss 0.0014987160539021716, mse: 0.0014532237546518445, rmse: 0.03812117199997719, mae 0.02165527269244194, r2: 0.7747833503426971, nrmse: 0.9981692870258145\n",
      "INFO logger 2024-04-27 20:24:04,787 | train_utils.py:99 | Epoch 119 [Test]: loss 1.1769056360016724e-05, mse: 0.0014899014495313168, rmse: 0.0385992415667888, mae 0.02292093075811863, r2: -6.695598032521613, nrmse: 1.567566739806373\n",
      "INFO logger 2024-04-27 20:24:04,788 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2024-04-27 20:24:05,181 | train_utils.py:97 | Epoch 120 [Train]: loss 0.0015059638136272254, mse: 0.0014669125666841865, rmse: 0.03830029460309916, mae 0.022556843236088753, r2: 0.7702685837721874, nrmse: 1.0214594263008228\n",
      "INFO logger 2024-04-27 20:24:05,182 | train_utils.py:99 | Epoch 120 [Test]: loss 1.1845778886464353e-05, mse: 0.0014993272488936782, rmse: 0.038721147308591955, mae 0.023468706756830215, r2: -12.56090891260024, nrmse: 2.1383142160512363\n",
      "INFO logger 2024-04-27 20:24:05,183 | helpers.py:136 | EarlyStopping counter: 1 out of 50\n",
      "INFO logger 2024-04-27 20:24:05,568 | train_utils.py:97 | Epoch 121 [Train]: loss 0.0014859545645290488, mse: 0.0014467376749962568, rmse: 0.03803600498207267, mae 0.021632611751556396, r2: 0.7766686780305864, nrmse: 0.9938022719500285\n",
      "INFO logger 2024-04-27 20:24:05,569 | train_utils.py:99 | Epoch 121 [Test]: loss 1.1774281582594931e-05, mse: 0.0014905615244060755, rmse: 0.038607790980656684, mae 0.0229999590665102, r2: -7.683786830214143, nrmse: 1.6920947674862648\n",
      "INFO logger 2024-04-27 20:24:05,569 | helpers.py:136 | EarlyStopping counter: 2 out of 50\n",
      "INFO logger 2024-04-27 20:24:05,962 | train_utils.py:97 | Epoch 122 [Train]: loss 0.0014874685183776607, mse: 0.0014535471564158797, rmse: 0.03812541352452298, mae 0.02202504500746727, r2: 0.7747243217438925, nrmse: 1.0034098216065084\n",
      "INFO logger 2024-04-27 20:24:05,963 | train_utils.py:99 | Epoch 122 [Test]: loss 1.1790566941538052e-05, mse: 0.0014926946023479104, rmse: 0.03863540607199451, mae 0.023144837468862534, r2: -9.186634466066643, nrmse: 1.800680864170919\n",
      "INFO logger 2024-04-27 20:24:05,963 | helpers.py:136 | EarlyStopping counter: 3 out of 50\n",
      "INFO logger 2024-04-27 20:24:06,415 | train_utils.py:97 | Epoch 123 [Train]: loss 0.0014727532538398559, mse: 0.0014425312401726842, rmse: 0.03798066929600746, mae 0.021524474024772644, r2: 0.7774709066646448, nrmse: 0.9922643523065006\n",
      "INFO logger 2024-04-27 20:24:06,416 | train_utils.py:99 | Epoch 123 [Test]: loss 1.1737616223825022e-05, mse: 0.001486244727857411, rmse: 0.03855184467515674, mae 0.022817984223365784, r2: -6.278273217176815, nrmse: 1.6205936491240471\n",
      "INFO logger 2024-04-27 20:24:06,416 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2024-04-27 20:24:06,802 | train_utils.py:97 | Epoch 124 [Train]: loss 0.0014784301968596314, mse: 0.001450996147468686, rmse: 0.03809194334066833, mae 0.021928224712610245, r2: 0.7752080081525651, nrmse: 1.0014522450381103\n",
      "INFO logger 2024-04-27 20:24:06,802 | train_utils.py:99 | Epoch 124 [Test]: loss 1.1774282008089528e-05, mse: 0.0014906866708770394, rmse: 0.038609411687787205, mae 0.023025400936603546, r2: -7.9560498344079065, nrmse: 1.792523462208697\n",
      "INFO logger 2024-04-27 20:24:06,803 | helpers.py:136 | EarlyStopping counter: 1 out of 50\n",
      "INFO logger 2024-04-27 20:24:07,190 | train_utils.py:97 | Epoch 125 [Train]: loss 0.0014733585868841972, mse: 0.0014452653704211116, rmse: 0.03801664596490742, mae 0.021708402782678604, r2: 0.7766956303290892, nrmse: 0.9954320471427676\n",
      "INFO logger 2024-04-27 20:24:07,191 | train_utils.py:99 | Epoch 125 [Test]: loss 1.1759748474227765e-05, mse: 0.0014888776931911707, rmse: 0.03858597793488161, mae 0.02290811762213707, r2: -6.6266253282753835, nrmse: 1.7238527457691826\n",
      "INFO logger 2024-04-27 20:24:07,191 | helpers.py:136 | EarlyStopping counter: 2 out of 50\n",
      "INFO logger 2024-04-27 20:24:07,570 | train_utils.py:97 | Epoch 126 [Train]: loss 0.0014784074366066624, mse: 0.0014526360901072621, rmse: 0.03811346337066814, mae 0.02204519882798195, r2: 0.7744123703879306, nrmse: 1.0049814725362576\n",
      "INFO logger 2024-04-27 20:24:07,571 | train_utils.py:99 | Epoch 126 [Test]: loss 1.1802093717827849e-05, mse: 0.0014939827378839254, rmse: 0.03865207287952259, mae 0.023119483143091202, r2: -8.357114574606252, nrmse: 1.9341587926410693\n",
      "INFO logger 2024-04-27 20:24:07,572 | helpers.py:136 | EarlyStopping counter: 3 out of 50\n",
      "INFO logger 2024-04-27 20:24:07,946 | train_utils.py:97 | Epoch 127 [Train]: loss 0.0014760212126380201, mse: 0.001447776798158884, rmse: 0.03804966226077288, mae 0.021895524114370346, r2: 0.7754950591210182, nrmse: 1.0015459539903546\n",
      "INFO logger 2024-04-27 20:24:07,947 | train_utils.py:99 | Epoch 127 [Test]: loss 1.179413241606576e-05, mse: 0.0014929783064872026, rmse: 0.038639077453883425, mae 0.023056000471115112, r2: -7.721942529997824, nrmse: 1.9589375037841001\n",
      "INFO logger 2024-04-27 20:24:07,947 | helpers.py:136 | EarlyStopping counter: 4 out of 50\n",
      "INFO logger 2024-04-27 20:24:08,350 | train_utils.py:97 | Epoch 128 [Train]: loss 0.0014781542535443847, mse: 0.001452420256100595, rmse: 0.03811063179875919, mae 0.022102318704128265, r2: 0.7736142673019397, nrmse: 1.0097701560030528\n",
      "INFO logger 2024-04-27 20:24:08,350 | train_utils.py:99 | Epoch 128 [Test]: loss 1.1819526103831696e-05, mse: 0.001496045500971377, rmse: 0.03867874740695951, mae 0.023189324885606766, r2: -9.005482812495828, nrmse: 2.1230069490533507\n",
      "INFO logger 2024-04-27 20:24:08,351 | helpers.py:136 | EarlyStopping counter: 5 out of 50\n",
      "INFO logger 2024-04-27 20:24:08,738 | train_utils.py:97 | Epoch 129 [Train]: loss 0.0014759592756469597, mse: 0.0014485724968835711, rmse: 0.03806011687953114, mae 0.021945636719465256, r2: 0.7746753823671053, nrmse: 1.0064463532140278\n",
      "INFO logger 2024-04-27 20:24:08,738 | train_utils.py:99 | Epoch 129 [Test]: loss 1.1808797129911877e-05, mse: 0.0014946997398510575, rmse: 0.03866134684476289, mae 0.02309797890484333, r2: -8.261649669890982, nrmse: 2.1342542363470547\n",
      "INFO logger 2024-04-27 20:24:08,739 | helpers.py:136 | EarlyStopping counter: 6 out of 50\n",
      "INFO logger 2024-04-27 20:24:09,190 | train_utils.py:97 | Epoch 130 [Train]: loss 0.00147691958558912, mse: 0.0014528912724927068, rmse: 0.03811681089090097, mae 0.022112136706709862, r2: 0.7729530194129912, nrmse: 1.0131024709159062\n",
      "INFO logger 2024-04-27 20:24:09,191 | train_utils.py:99 | Epoch 130 [Test]: loss 1.1825966943199718e-05, mse: 0.0014967601746320724, rmse: 0.03868798488719815, mae 0.023194851353764534, r2: -9.25132663389665, nrmse: 2.243636603094067\n",
      "INFO logger 2024-04-27 20:24:09,191 | helpers.py:136 | EarlyStopping counter: 7 out of 50\n",
      "INFO logger 2024-04-27 20:24:09,584 | train_utils.py:97 | Epoch 131 [Train]: loss 0.0014756235320821235, mse: 0.0014501630794256926, rmse: 0.03808100680688068, mae 0.021971940994262695, r2: 0.7738907520908909, nrmse: 1.0098271556064577\n",
      "INFO logger 2024-04-27 20:24:09,585 | train_utils.py:99 | Epoch 131 [Test]: loss 1.181828861661958e-05, mse: 0.0014957849634811282, rmse: 0.03867537929330659, mae 0.0231029000133276, r2: -8.532040099554887, nrmse: 2.2417909202909327\n",
      "INFO logger 2024-04-27 20:24:09,585 | helpers.py:136 | EarlyStopping counter: 8 out of 50\n",
      "INFO logger 2024-04-27 20:24:09,967 | train_utils.py:97 | Epoch 132 [Train]: loss 0.001476375229357959, mse: 0.0014557309914380312, rmse: 0.03815404292388988, mae 0.022150373086333275, r2: 0.7719544106758306, nrmse: 1.0158639507636045\n",
      "INFO logger 2024-04-27 20:24:09,967 | train_utils.py:99 | Epoch 132 [Test]: loss 1.183627480511657e-05, mse: 0.001497933641076088, rmse: 0.03870314768951084, mae 0.023211929947137833, r2: -9.796002223917679, nrmse: 2.344136539306549\n",
      "INFO logger 2024-04-27 20:24:09,968 | helpers.py:136 | EarlyStopping counter: 9 out of 50\n",
      "INFO logger 2024-04-27 20:24:10,356 | train_utils.py:97 | Epoch 133 [Train]: loss 0.0014763353398721361, mse: 0.001453055185265839, rmse: 0.03811896096781547, mae 0.022026777267456055, r2: 0.7726973311731401, nrmse: 1.0136289566457697\n",
      "INFO logger 2024-04-27 20:24:10,356 | train_utils.py:99 | Epoch 133 [Test]: loss 1.1830954122825454e-05, mse: 0.0014972648350521922, rmse: 0.03869450652291863, mae 0.02314218506217003, r2: -9.389593390970756, nrmse: 2.3652823866872414\n",
      "INFO logger 2024-04-27 20:24:10,357 | helpers.py:136 | EarlyStopping counter: 10 out of 50\n",
      "INFO logger 2024-04-27 20:24:10,779 | train_utils.py:97 | Epoch 134 [Train]: loss 0.001476568323702194, mse: 0.0014601789880543947, rmse: 0.03821228844304401, mae 0.022246990352869034, r2: 0.7704371087848056, nrmse: 1.0195706002692455\n",
      "INFO logger 2024-04-27 20:24:10,780 | train_utils.py:99 | Epoch 134 [Test]: loss 1.1853949042249537e-05, mse: 0.0015000004786998034, rmse: 0.038729839642061566, mae 0.02329936996102333, r2: -11.505967029300058, nrmse: 2.4903397340124225\n",
      "INFO logger 2024-04-27 20:24:10,780 | helpers.py:136 | EarlyStopping counter: 11 out of 50\n",
      "INFO logger 2024-04-27 20:24:11,165 | train_utils.py:97 | Epoch 135 [Train]: loss 0.0014783835938011857, mse: 0.0014567722100764513, rmse: 0.038167685416808435, mae 0.022151459008455276, r2: 0.7707101253839574, nrmse: 1.0210191545227099\n",
      "INFO logger 2024-04-27 20:24:11,166 | train_utils.py:99 | Epoch 135 [Test]: loss 1.1851861884876029e-05, mse: 0.0014997480902820826, rmse: 0.0387265811850476, mae 0.02327841892838478, r2: -11.735271941825127, nrmse: 2.591591402497957\n",
      "INFO logger 2024-04-27 20:24:11,166 | helpers.py:136 | EarlyStopping counter: 12 out of 50\n",
      "INFO logger 2024-04-27 20:24:11,550 | train_utils.py:97 | Epoch 136 [Train]: loss 0.0014768057978662276, mse: 0.0014649450313299894, rmse: 0.038274600341871495, mae 0.022404653951525688, r2: 0.7684063903764146, nrmse: 1.0258673573431267\n",
      "INFO logger 2024-04-27 20:24:11,551 | train_utils.py:99 | Epoch 136 [Test]: loss 1.1879054053205921e-05, mse: 0.0015029622009024024, rmse: 0.038768056449897025, mae 0.023492764681577682, r2: -15.259341396709157, nrmse: 2.763520092226158\n",
      "INFO logger 2024-04-27 20:24:11,551 | helpers.py:136 | EarlyStopping counter: 13 out of 50\n",
      "INFO logger 2024-04-27 20:24:12,001 | train_utils.py:97 | Epoch 137 [Train]: loss 0.001480763710745567, mse: 0.0014606670010834932, rmse: 0.03821867346054142, mae 0.02233283780515194, r2: 0.768054479831316, nrmse: 1.0322906727886376\n",
      "INFO logger 2024-04-27 20:24:12,001 | train_utils.py:99 | Epoch 137 [Test]: loss 1.1879007184976026e-05, mse: 0.001502973260357976, rmse: 0.038768199085822595, mae 0.023500392213463783, r2: -15.911903818402152, nrmse: 2.93995163185025\n",
      "INFO logger 2024-04-27 20:24:12,002 | helpers.py:136 | EarlyStopping counter: 14 out of 50\n",
      "INFO logger 2024-04-27 20:24:12,396 | train_utils.py:97 | Epoch 138 [Train]: loss 0.0014762729002953609, mse: 0.0014694004785269499, rmse: 0.03833275986055465, mae 0.0225572157651186, r2: 0.7671150358705549, nrmse: 1.0294148935300236\n",
      "INFO logger 2024-04-27 20:24:12,397 | train_utils.py:99 | Epoch 138 [Test]: loss 1.1906524814123407e-05, mse: 0.0015062176389619708, rmse: 0.038810019826869074, mae 0.023727018386125565, r2: -20.126673291677836, nrmse: 3.0747883343737468\n",
      "INFO logger 2024-04-27 20:24:12,397 | helpers.py:136 | EarlyStopping counter: 15 out of 50\n",
      "INFO logger 2024-04-27 20:24:12,781 | train_utils.py:97 | Epoch 139 [Train]: loss 0.0014820389300502727, mse: 0.0014621244044974446, rmse: 0.03823773534739531, mae 0.02244284562766552, r2: 0.7674298855194359, nrmse: 1.034610694716567\n",
      "INFO logger 2024-04-27 20:24:12,781 | train_utils.py:99 | Epoch 139 [Test]: loss 1.1901610053678135e-05, mse: 0.0015056655975058675, rmse: 0.038802907075448194, mae 0.023678643628954887, r2: -19.37598664786529, nrmse: 3.1645547631688795\n",
      "INFO logger 2024-04-27 20:24:12,782 | helpers.py:136 | EarlyStopping counter: 16 out of 50\n",
      "INFO logger 2024-04-27 20:24:13,162 | train_utils.py:97 | Epoch 140 [Train]: loss 0.001473276212463197, mse: 0.001468047732487321, rmse: 0.03831511102016176, mae 0.022525371983647346, r2: 0.7688759856243897, nrmse: 1.0216088480113779\n",
      "INFO logger 2024-04-27 20:24:13,162 | train_utils.py:99 | Epoch 140 [Test]: loss 1.1922280006798267e-05, mse: 0.0015081235906109214, rmse: 0.0388345669553675, mae 0.023830819875001907, r2: -21.942944663105607, nrmse: 3.1335089475238678\n",
      "INFO logger 2024-04-27 20:24:13,163 | helpers.py:136 | EarlyStopping counter: 17 out of 50\n",
      "INFO logger 2024-04-27 20:24:13,565 | train_utils.py:97 | Epoch 141 [Train]: loss 0.0014779600496906422, mse: 0.0014574561500921845, rmse: 0.03817664403915284, mae 0.022307977080345154, r2: 0.7715033679177228, nrmse: 1.0160558003150106\n",
      "INFO logger 2024-04-27 20:24:13,566 | train_utils.py:99 | Epoch 141 [Test]: loss 1.1900850056503132e-05, mse: 0.001505653839558363, rmse: 0.03880275556656206, mae 0.02365371771156788, r2: -17.953843986197384, nrmse: 2.933961832955397\n",
      "INFO logger 2024-04-27 20:24:13,567 | helpers.py:136 | EarlyStopping counter: 18 out of 50\n",
      "INFO logger 2024-04-27 20:24:13,944 | train_utils.py:97 | Epoch 142 [Train]: loss 0.0014658511761089634, mse: 0.0014578897971659899, rmse: 0.0381823230980776, mae 0.02220161445438862, r2: 0.7739637034756012, nrmse: 1.0025562218707909\n",
      "INFO logger 2024-04-27 20:24:13,945 | train_utils.py:99 | Epoch 142 [Test]: loss 1.1905294326297416e-05, mse: 0.0015062050661072135, rmse: 0.03880985784703692, mae 0.023663554340600967, r2: -17.53149049170696, nrmse: 2.7533741718980536\n",
      "INFO logger 2024-04-27 20:24:13,945 | helpers.py:136 | EarlyStopping counter: 19 out of 50\n",
      "INFO logger 2024-04-27 20:24:14,381 | train_utils.py:97 | Epoch 143 [Train]: loss 0.001469771008984491, mse: 0.0014489480527117848, rmse: 0.03806505027859263, mae 0.02202395722270012, r2: 0.776329728791897, nrmse: 0.9973645675840798\n",
      "INFO logger 2024-04-27 20:24:14,382 | train_utils.py:99 | Epoch 143 [Test]: loss 1.189148851584545e-05, mse: 0.0015046466141939163, rmse: 0.03878977460870218, mae 0.02346944436430931, r2: -13.314360722832216, nrmse: 2.5325531567250597\n",
      "INFO logger 2024-04-27 20:24:14,383 | helpers.py:136 | EarlyStopping counter: 20 out of 50\n",
      "INFO logger 2024-04-27 20:24:14,769 | train_utils.py:97 | Epoch 144 [Train]: loss 0.001457317891393803, mse: 0.0014475247589871287, rmse: 0.03804635014015311, mae 0.021800842136144638, r2: 0.7781703146797215, nrmse: 0.989103830390928\n",
      "INFO logger 2024-04-27 20:24:14,769 | train_utils.py:99 | Epoch 144 [Test]: loss 1.1877665068642428e-05, mse: 0.0015030386857688427, rmse: 0.03876904287919477, mae 0.023331986740231514, r2: -10.734059432302653, nrmse: 2.1800801023435974\n",
      "INFO logger 2024-04-27 20:24:14,770 | helpers.py:136 | EarlyStopping counter: 21 out of 50\n",
      "INFO logger 2024-04-27 20:24:15,165 | train_utils.py:97 | Epoch 145 [Train]: loss 0.0014633592007919295, mse: 0.0014425280969589949, rmse: 0.03798062791686039, mae 0.02185855433344841, r2: 0.77920244913854, nrmse: 0.9884626956315025\n",
      "INFO logger 2024-04-27 20:24:15,166 | train_utils.py:99 | Epoch 145 [Test]: loss 1.187458542384594e-05, mse: 0.0015027547487989068, rmse: 0.03876538080296525, mae 0.023281648755073547, r2: -9.820105075811957, nrmse: 2.3114250485680286\n",
      "INFO logger 2024-04-27 20:24:15,166 | helpers.py:136 | EarlyStopping counter: 22 out of 50\n",
      "INFO logger 2024-04-27 20:24:15,551 | train_utils.py:97 | Epoch 146 [Train]: loss 0.0014534793407683325, mse: 0.0014446519780904055, rmse: 0.03800857769096873, mae 0.021624695509672165, r2: 0.7789039925630192, nrmse: 0.9889530806176761\n",
      "INFO logger 2024-04-27 20:24:15,552 | train_utils.py:99 | Epoch 146 [Test]: loss 1.1871727993602902e-05, mse: 0.0015023748856037855, rmse: 0.038760480977456734, mae 0.02306772582232952, r2: -6.061933643410309, nrmse: 1.7095177360247986\n",
      "INFO logger 2024-04-27 20:24:15,552 | helpers.py:136 | EarlyStopping counter: 23 out of 50\n",
      "INFO logger 2024-04-27 20:24:15,942 | train_utils.py:97 | Epoch 147 [Train]: loss 0.0014639867855970976, mse: 0.0014484478160738945, rmse: 0.03805847889858309, mae 0.02193531207740307, r2: 0.7790532351704605, nrmse: 0.9867457294530405\n",
      "INFO logger 2024-04-27 20:24:15,943 | train_utils.py:99 | Epoch 147 [Test]: loss 1.189455356617725e-05, mse: 0.001505161402747035, rmse: 0.03879640966309943, mae 0.02322676032781601, r2: -7.405811917994512, nrmse: 1.9745578064099023\n",
      "INFO logger 2024-04-27 20:24:15,943 | helpers.py:136 | EarlyStopping counter: 24 out of 50\n",
      "INFO logger 2024-04-27 20:24:16,331 | train_utils.py:97 | Epoch 148 [Train]: loss 0.0014590583234039937, mse: 0.001450453419238329, rmse: 0.03808481875023602, mae 0.02165725640952587, r2: 0.7755958723547559, nrmse: 1.0023491023075357\n",
      "INFO logger 2024-04-27 20:24:16,332 | train_utils.py:99 | Epoch 148 [Test]: loss 1.1899932966722796e-05, mse: 0.0015057179844006896, rmse: 0.038803582107850426, mae 0.02298710122704506, r2: -4.289402445720792, nrmse: 1.4443764543676918\n",
      "INFO logger 2024-04-27 20:24:16,332 | helpers.py:136 | EarlyStopping counter: 25 out of 50\n",
      "INFO logger 2024-04-27 20:24:16,714 | train_utils.py:97 | Epoch 149 [Train]: loss 0.0014702204983244205, mse: 0.0014648694777861238, rmse: 0.038273613335901846, mae 0.02231554314494133, r2: 0.770971012842794, nrmse: 1.0183129042618866\n",
      "INFO logger 2024-04-27 20:24:16,715 | train_utils.py:99 | Epoch 149 [Test]: loss 1.1978563049274405e-05, mse: 0.0015151026891544461, rmse: 0.03892432002173508, mae 0.02341742441058159, r2: -9.11106688639666, nrmse: 2.276867739748912\n",
      "INFO logger 2024-04-27 20:24:16,716 | helpers.py:136 | EarlyStopping counter: 26 out of 50\n",
      "INFO logger 2024-04-27 20:24:17,168 | train_utils.py:97 | Epoch 150 [Train]: loss 0.0014693587551402405, mse: 0.0014630347723141313, rmse: 0.03824963754487265, mae 0.022175312042236328, r2: 0.7650064663283322, nrmse: 1.047500129105222\n",
      "INFO logger 2024-04-27 20:24:17,168 | train_utils.py:99 | Epoch 150 [Test]: loss 1.1940369292636189e-05, mse: 0.0015105842612683773, rmse: 0.03886623549134103, mae 0.02340180240571499, r2: -10.974767831044662, nrmse: 2.631582427260625\n",
      "INFO logger 2024-04-27 20:24:17,169 | helpers.py:136 | EarlyStopping counter: 27 out of 50\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO logger 2024-04-27 20:24:17,385 | train_utils.py:140 | Best Loss: 1.1737616223825022e-05, Best epoch: 150\n"
     ]
    }
   ],
   "source": [
    "trained_model = fit(model, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5196ee3c-1f4d-46c9-b236-a54ae12fa7d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "a39106e1a9d6d153b7400628e7589ff266b5caee5b0db427f0903be982155882"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
