{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aee8de2b",
   "metadata": {},
   "source": [
    "### In this notebook we perform federated learning\n",
    "\n",
    "In federated learning each base station has access only to it's private dataset, however they collaborate together to train a model that has satifactory results on data from any other base station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a3a97e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "parent = Path(os.path.abspath(\"\")).resolve().parents[0]\n",
    "if parent not in sys.path:\n",
    "    sys.path.insert(0, str(parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "eed1269d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import random\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c1c1ca7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.utils.data_utils import read_data, generate_time_lags, time_to_feature, handle_nans, to_Xy, \\\n",
    "    to_torch_dataset, to_timeseries_rep, assign_statistics, \\\n",
    "    to_train_val, scale_features, get_data_by_area, remove_identifiers, get_exogenous_data_by_area, handle_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f79e4b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.utils.train_utils import train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a7c779c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.models.mlp import MLP\n",
    "from ml.models.rnn import RNN\n",
    "from ml.models.lstm import LSTM\n",
    "from ml.models.gru import GRU\n",
    "from ml.models.cnn import CNN\n",
    "from ml.models.rnn_autoencoder import DualAttentionAutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "67d7baf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.fl.defaults import create_regression_client\n",
    "from ml.fl.client_proxy import SimpleClientProxy\n",
    "from ml.fl.server.server import Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c4606af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    data_path='../dataset/full_dataset.csv', # dataset\n",
    "    data_path_test=['../dataset/upstream_test.csv'], # test dataset\n",
    "    test_size=0.3, # validation size\n",
    "    targets=['temp','pH','DissolvedOxygen','Conductivity','Turbidity','AmmoniaNitrogen'], # the target columns\n",
    "    num_lags=10, # the number of past observations to feed as input\n",
    "\n",
    "    identifier='District', # the column name that identifies a bs\n",
    "\n",
    "    nan_constant=0, # the constant to transform nan values\n",
    "    x_scaler='minmax', # x_scaler\n",
    "    y_scaler='minmax', # y_scaler\n",
    "    outlier_detection=None, # whether to perform flooring and capping\n",
    "\n",
    "    \n",
    "    criterion='mse', # optimization criterion, mse or l1\n",
    "    fl_rounds=5, # the number of federated rounds\n",
    "    fraction=1., # the percentage of available client to consider for random selection\n",
    "    aggregation=\"avg\", # federated aggregation algorithm\n",
    "    epochs=3, # the number of maximum local epochs\n",
    "    lr=0.001, # learning rate\n",
    "    optimizer='adam', # the optimizer, it can be sgd or adam\n",
    "    batch_size=128, # the batch size to use\n",
    "    local_early_stopping=False, # whether to use early stopping\n",
    "    local_patience=50, # patience value for the early stopping parameter (if specified)\n",
    "    max_grad_norm=0.0, # whether to clip grad norm\n",
    "    reg1=0.0, # l1 regularization\n",
    "    reg2=0.0, # l2 regularization\n",
    "\n",
    "    cuda=True, # whether to use gpu\n",
    "    \n",
    "    seed=0, # reproducibility\n",
    "\n",
    "    assign_stats=None, # whether to use statistics as exogenous data, [\"mean\", \"median\", \"std\", \"variance\", \"kurtosis\", \"skew\"]\n",
    "    use_time_features=False # whether to use datetime features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "727c20ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script arguments: Namespace(aggregation='avg', assign_stats=None, batch_size=128, criterion='mse', cuda=True, data_path='../dataset/full_dataset.csv', data_path_test=['../dataset/upstream_test.csv'], epochs=3, fl_rounds=5, fraction=1.0, identifier='District', local_early_stopping=False, local_patience=50, lr=0.001, max_grad_norm=0.0, nan_constant=0, num_lags=10, optimizer='adam', outlier_detection=None, reg1=0.0, reg2=0.0, seed=0, targets=['temp', 'pH', 'DissolvedOxygen', 'Conductivity', 'Turbidity', 'AmmoniaNitrogen'], test_size=0.3, use_time_features=False, x_scaler='minmax', y_scaler='minmax')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Script arguments: {args}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d36c3d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if args.cuda and torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "28dbb608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection specification\n",
    "if args.outlier_detection is not None:\n",
    "    outlier_columns = ['Conductivity', 'Turbidity', 'pH', 'DissolvedOxygen']\n",
    "    outlier_kwargs = {\"upstream\": (10, 90), \"midstream\": (10, 90), \"downstream\": (5, 95)}\n",
    "    args.outlier_columns = outlier_columns\n",
    "    args.outlier_kwargs = outlier_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c7015c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_all():\n",
    "    # ensure reproducibility\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "581869c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3a510771",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "022d3623",
   "metadata": {},
   "source": [
    "### The pre-processing method is almost equivalent to centralized learning. The only difference is that the scaling operations are performed individually on each base station. In contrast, in centralized learning the scaling is performed by considering the combined data from all base stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1b304bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preprocessing():\n",
    "    \"\"\"Preprocess a given .csv\"\"\"\n",
    "    # read data\n",
    "    df = read_data(args.data_path)\n",
    "    # handle nans\n",
    "    df = handle_nans(train_data=df, constant=args.nan_constant,\n",
    "                     identifier=args.identifier)\n",
    "    # split to train/validation\n",
    "    train_data, val_data = to_train_val(df)\n",
    "    \n",
    "    # handle outliers (if specified)\n",
    "    if args.outlier_detection is not None:\n",
    "        train_data = handle_outliers(df=train_data, columns=args.outlier_columns,\n",
    "                                     identifier=args.identifier, kwargs=args.outlier_kwargs)\n",
    "    \n",
    "    # get X and y\n",
    "    X_train, X_val, y_train, y_val = to_Xy(train_data=train_data, val_data=val_data,\n",
    "                                          targets=args.targets)\n",
    "    \n",
    "    # scale X\n",
    "    X_train, X_val, x_scalers = scale_features(train_data=X_train, val_data=X_val,\n",
    "                                              scaler=args.x_scaler,\n",
    "                                              per_area=True, # the features are scaled locally\n",
    "                                              identifier=args.identifier)\n",
    "    # scale y\n",
    "    y_train, y_val, y_scalers = scale_features(train_data=y_train, val_data=y_val,\n",
    "                                              scaler=args.y_scaler, \n",
    "                                              per_area=True,\n",
    "                                              identifier=args.identifier)\n",
    "    \n",
    "    # generate time lags\n",
    "    X_train = generate_time_lags(X_train, args.num_lags)\n",
    "    X_val = generate_time_lags(X_val, args.num_lags)\n",
    "    y_train = generate_time_lags(y_train, args.num_lags, is_y=True)\n",
    "    y_val = generate_time_lags(y_val, args.num_lags, is_y=True)\n",
    "    \n",
    "    # get datetime features as exogenous data\n",
    "    date_time_df_train = time_to_feature(\n",
    "        X_train, args.use_time_features, identifier=args.identifier\n",
    "    )\n",
    "    date_time_df_val = time_to_feature(\n",
    "        X_val, args.use_time_features, identifier=args.identifier\n",
    "    )\n",
    "    \n",
    "    # get statistics as exogenous data\n",
    "    stats_df_train = assign_statistics(X_train, args.assign_stats, args.num_lags,\n",
    "                                       targets=args.targets, identifier=args.identifier)\n",
    "    stats_df_val = assign_statistics(X_val, args.assign_stats, args.num_lags, \n",
    "                                       targets=args.targets, identifier=args.identifier)\n",
    "    \n",
    "    # concat the exogenous features (if any) to a single dataframe\n",
    "    if date_time_df_train is not None or stats_df_train is not None:\n",
    "        exogenous_data_train = pd.concat([date_time_df_train, stats_df_train], axis=1)\n",
    "        # remove duplicate columns (if any)\n",
    "        exogenous_data_train = exogenous_data_train.loc[:, ~exogenous_data_train.columns.duplicated()].copy()\n",
    "        assert len(exogenous_data_train) == len(X_train) == len(y_train)\n",
    "    else:\n",
    "        exogenous_data_train = None\n",
    "    if date_time_df_val is not None or stats_df_val is not None:\n",
    "        exogenous_data_val = pd.concat([date_time_df_val, stats_df_val], axis=1)\n",
    "        exogenous_data_val = exogenous_data_val.loc[:, ~exogenous_data_val.columns.duplicated()].copy()\n",
    "        assert len(exogenous_data_val) == len(X_val) == len(y_val)\n",
    "    else:\n",
    "        exogenous_data_val = None\n",
    "        \n",
    "    return X_train, X_val, y_train, y_val, exogenous_data_train, exogenous_data_val, x_scalers, y_scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ad15f79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO logger 2024-06-03 08:14:19,873 | data_utils.py:383 | Observations info in upstream\n",
      "INFO logger 2024-06-03 08:14:19,874 | data_utils.py:384 | \tTotal number of samples:  4863\n",
      "INFO logger 2024-06-03 08:14:19,874 | data_utils.py:385 | \tNumber of samples for training: 3891\n",
      "INFO logger 2024-06-03 08:14:19,875 | data_utils.py:386 | \tNumber of samples for validation:  972\n",
      "INFO logger 2024-06-03 08:14:19,877 | data_utils.py:383 | Observations info in midstream\n",
      "INFO logger 2024-06-03 08:14:19,878 | data_utils.py:384 | \tTotal number of samples:  4930\n",
      "INFO logger 2024-06-03 08:14:19,878 | data_utils.py:385 | \tNumber of samples for training: 3944\n",
      "INFO logger 2024-06-03 08:14:19,878 | data_utils.py:386 | \tNumber of samples for validation:  986\n",
      "INFO logger 2024-06-03 08:14:19,880 | data_utils.py:383 | Observations info in downstream\n",
      "INFO logger 2024-06-03 08:14:19,880 | data_utils.py:384 | \tTotal number of samples:  4920\n",
      "INFO logger 2024-06-03 08:14:19,880 | data_utils.py:385 | \tNumber of samples for training: 3936\n",
      "INFO logger 2024-06-03 08:14:19,880 | data_utils.py:386 | \tNumber of samples for validation:  984\n",
      "INFO logger 2024-06-03 08:14:19,884 | data_utils.py:389 | Observations info using all data\n",
      "INFO logger 2024-06-03 08:14:19,884 | data_utils.py:390 | \tTotal number of samples:  14713\n",
      "INFO logger 2024-06-03 08:14:19,885 | data_utils.py:391 | \tNumber of samples for training: 11771\n",
      "INFO logger 2024-06-03 08:14:19,885 | data_utils.py:392 | \tNumber of samples for validation:  2942\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val, exogenous_data_train, exogenous_data_val, x_scalers, y_scalers = make_preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "bf0982ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                     TotalNitrogen_lag-10  TotalPhosphorus_lag-10  \\\ntime                                                                \n2020-11-10 16:00:00              0.053548                0.487918   \n2020-11-10 20:00:00              0.053548                0.487918   \n2020-11-11 00:00:00              0.052996                0.489781   \n2020-11-11 04:00:00              0.054376                0.489781   \n2020-11-11 08:00:00              0.055756                0.488850   \n\n                     AmmoniaNitrogen_lag-10  PermanganateIndex_lag-10  \\\ntime                                                                    \n2020-11-10 16:00:00                0.454747                  0.202132   \n2020-11-10 20:00:00                0.454747                  0.202132   \n2020-11-11 00:00:00                0.468944                  0.179582   \n2020-11-11 04:00:00                0.468944                  0.188602   \n2020-11-11 08:00:00                0.458740                  0.192292   \n\n                     Turbidity_lag-10  Conductivity_lag-10  \\\ntime                                                         \n2020-11-10 16:00:00          0.004535             0.185411   \n2020-11-10 20:00:00          0.005683             0.185266   \n2020-11-11 00:00:00          0.005683             0.185266   \n2020-11-11 04:00:00          0.004995             0.186714   \n2020-11-11 08:00:00          0.004995             0.186714   \n\n                     DissolvedOxygen_lag-10  pH_lag-10  temp_lag-10  \\\ntime                                                                  \n2020-11-10 16:00:00                0.314404   0.227700     0.671883   \n2020-11-10 20:00:00                0.278265   0.194836     0.663788   \n2020-11-11 00:00:00                0.277233   0.194836     0.658392   \n2020-11-11 04:00:00                0.323180   0.248826     0.677280   \n2020-11-11 08:00:00                0.347444   0.248826     0.685375   \n\n                     TotalNitrogen_lag-9  ...  TotalNitrogen_lag-1  \\\ntime                                      ...                        \n2020-11-10 16:00:00             0.053548  ...             0.054376   \n2020-11-10 20:00:00             0.052996  ...             0.052996   \n2020-11-11 00:00:00             0.054376  ...             0.057137   \n2020-11-11 04:00:00             0.055756  ...             0.052720   \n2020-11-11 08:00:00             0.055480  ...             0.052720   \n\n                     TotalPhosphorus_lag-1  AmmoniaNitrogen_lag-1  \\\ntime                                                                \n2020-11-10 16:00:00               0.490246               0.469831   \n2020-11-10 20:00:00               0.490246               0.463620   \n2020-11-11 00:00:00               0.492574               0.459184   \n2020-11-11 04:00:00               0.488850               0.458740   \n2020-11-11 08:00:00               0.488850               0.458740   \n\n                     PermanganateIndex_lag-1  Turbidity_lag-1  \\\ntime                                                            \n2020-11-10 16:00:00                 0.184912         0.007406   \n2020-11-10 20:00:00                 0.191472         0.005454   \n2020-11-11 00:00:00                 0.181222         0.004765   \n2020-11-11 04:00:00                 0.187782         0.005396   \n2020-11-11 08:00:00                 0.187782         0.005798   \n\n                     Conductivity_lag-1  DissolvedOxygen_lag-1  pH_lag-1  \\\ntime                                                                       \n2020-11-10 16:00:00            0.187004               0.316469  0.225352   \n2020-11-10 20:00:00            0.189756               0.352091  0.262911   \n2020-11-11 00:00:00            0.188742               0.325245  0.230047   \n2020-11-11 04:00:00            0.187438               0.304595  0.206573   \n2020-11-11 08:00:00            0.187004               0.293237  0.199530   \n\n                     temp_lag-1  District  \ntime                                       \n2020-11-10 16:00:00    0.661090  upstream  \n2020-11-10 20:00:00    0.679978  upstream  \n2020-11-11 00:00:00    0.666487  upstream  \n2020-11-11 04:00:00    0.658392  upstream  \n2020-11-11 08:00:00    0.655693  upstream  \n\n[5 rows x 91 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TotalNitrogen_lag-10</th>\n      <th>TotalPhosphorus_lag-10</th>\n      <th>AmmoniaNitrogen_lag-10</th>\n      <th>PermanganateIndex_lag-10</th>\n      <th>Turbidity_lag-10</th>\n      <th>Conductivity_lag-10</th>\n      <th>DissolvedOxygen_lag-10</th>\n      <th>pH_lag-10</th>\n      <th>temp_lag-10</th>\n      <th>TotalNitrogen_lag-9</th>\n      <th>...</th>\n      <th>TotalNitrogen_lag-1</th>\n      <th>TotalPhosphorus_lag-1</th>\n      <th>AmmoniaNitrogen_lag-1</th>\n      <th>PermanganateIndex_lag-1</th>\n      <th>Turbidity_lag-1</th>\n      <th>Conductivity_lag-1</th>\n      <th>DissolvedOxygen_lag-1</th>\n      <th>pH_lag-1</th>\n      <th>temp_lag-1</th>\n      <th>District</th>\n    </tr>\n    <tr>\n      <th>time</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2020-11-10 16:00:00</th>\n      <td>0.053548</td>\n      <td>0.487918</td>\n      <td>0.454747</td>\n      <td>0.202132</td>\n      <td>0.004535</td>\n      <td>0.185411</td>\n      <td>0.314404</td>\n      <td>0.227700</td>\n      <td>0.671883</td>\n      <td>0.053548</td>\n      <td>...</td>\n      <td>0.054376</td>\n      <td>0.490246</td>\n      <td>0.469831</td>\n      <td>0.184912</td>\n      <td>0.007406</td>\n      <td>0.187004</td>\n      <td>0.316469</td>\n      <td>0.225352</td>\n      <td>0.661090</td>\n      <td>upstream</td>\n    </tr>\n    <tr>\n      <th>2020-11-10 20:00:00</th>\n      <td>0.053548</td>\n      <td>0.487918</td>\n      <td>0.454747</td>\n      <td>0.202132</td>\n      <td>0.005683</td>\n      <td>0.185266</td>\n      <td>0.278265</td>\n      <td>0.194836</td>\n      <td>0.663788</td>\n      <td>0.052996</td>\n      <td>...</td>\n      <td>0.052996</td>\n      <td>0.490246</td>\n      <td>0.463620</td>\n      <td>0.191472</td>\n      <td>0.005454</td>\n      <td>0.189756</td>\n      <td>0.352091</td>\n      <td>0.262911</td>\n      <td>0.679978</td>\n      <td>upstream</td>\n    </tr>\n    <tr>\n      <th>2020-11-11 00:00:00</th>\n      <td>0.052996</td>\n      <td>0.489781</td>\n      <td>0.468944</td>\n      <td>0.179582</td>\n      <td>0.005683</td>\n      <td>0.185266</td>\n      <td>0.277233</td>\n      <td>0.194836</td>\n      <td>0.658392</td>\n      <td>0.054376</td>\n      <td>...</td>\n      <td>0.057137</td>\n      <td>0.492574</td>\n      <td>0.459184</td>\n      <td>0.181222</td>\n      <td>0.004765</td>\n      <td>0.188742</td>\n      <td>0.325245</td>\n      <td>0.230047</td>\n      <td>0.666487</td>\n      <td>upstream</td>\n    </tr>\n    <tr>\n      <th>2020-11-11 04:00:00</th>\n      <td>0.054376</td>\n      <td>0.489781</td>\n      <td>0.468944</td>\n      <td>0.188602</td>\n      <td>0.004995</td>\n      <td>0.186714</td>\n      <td>0.323180</td>\n      <td>0.248826</td>\n      <td>0.677280</td>\n      <td>0.055756</td>\n      <td>...</td>\n      <td>0.052720</td>\n      <td>0.488850</td>\n      <td>0.458740</td>\n      <td>0.187782</td>\n      <td>0.005396</td>\n      <td>0.187438</td>\n      <td>0.304595</td>\n      <td>0.206573</td>\n      <td>0.658392</td>\n      <td>upstream</td>\n    </tr>\n    <tr>\n      <th>2020-11-11 08:00:00</th>\n      <td>0.055756</td>\n      <td>0.488850</td>\n      <td>0.458740</td>\n      <td>0.192292</td>\n      <td>0.004995</td>\n      <td>0.186714</td>\n      <td>0.347444</td>\n      <td>0.248826</td>\n      <td>0.685375</td>\n      <td>0.055480</td>\n      <td>...</td>\n      <td>0.052720</td>\n      <td>0.488850</td>\n      <td>0.458740</td>\n      <td>0.187782</td>\n      <td>0.005798</td>\n      <td>0.187004</td>\n      <td>0.293237</td>\n      <td>0.199530</td>\n      <td>0.655693</td>\n      <td>upstream</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 91 columns</p>\n</div>"
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e5e37966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                         temp        pH  DissolvedOxygen  Conductivity  \\\ntime                                                                     \n2020-11-10 16:00:00  0.679978  0.262911         0.352091      0.189756   \n2020-11-10 20:00:00  0.666487  0.230047         0.325245      0.188742   \n2020-11-11 00:00:00  0.658392  0.206573         0.304595      0.187438   \n2020-11-11 04:00:00  0.655693  0.199530         0.293237      0.187004   \n2020-11-11 08:00:00  0.652995  0.197183         0.290139      0.187438   \n\n                     Turbidity  AmmoniaNitrogen  District  \ntime                                                       \n2020-11-10 16:00:00   0.005454         0.463620  upstream  \n2020-11-10 20:00:00   0.004765         0.459184  upstream  \n2020-11-11 00:00:00   0.005396         0.458740  upstream  \n2020-11-11 04:00:00   0.005798         0.458740  upstream  \n2020-11-11 08:00:00   0.006946         0.462733  upstream  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>temp</th>\n      <th>pH</th>\n      <th>DissolvedOxygen</th>\n      <th>Conductivity</th>\n      <th>Turbidity</th>\n      <th>AmmoniaNitrogen</th>\n      <th>District</th>\n    </tr>\n    <tr>\n      <th>time</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2020-11-10 16:00:00</th>\n      <td>0.679978</td>\n      <td>0.262911</td>\n      <td>0.352091</td>\n      <td>0.189756</td>\n      <td>0.005454</td>\n      <td>0.463620</td>\n      <td>upstream</td>\n    </tr>\n    <tr>\n      <th>2020-11-10 20:00:00</th>\n      <td>0.666487</td>\n      <td>0.230047</td>\n      <td>0.325245</td>\n      <td>0.188742</td>\n      <td>0.004765</td>\n      <td>0.459184</td>\n      <td>upstream</td>\n    </tr>\n    <tr>\n      <th>2020-11-11 00:00:00</th>\n      <td>0.658392</td>\n      <td>0.206573</td>\n      <td>0.304595</td>\n      <td>0.187438</td>\n      <td>0.005396</td>\n      <td>0.458740</td>\n      <td>upstream</td>\n    </tr>\n    <tr>\n      <th>2020-11-11 04:00:00</th>\n      <td>0.655693</td>\n      <td>0.199530</td>\n      <td>0.293237</td>\n      <td>0.187004</td>\n      <td>0.005798</td>\n      <td>0.458740</td>\n      <td>upstream</td>\n    </tr>\n    <tr>\n      <th>2020-11-11 08:00:00</th>\n      <td>0.652995</td>\n      <td>0.197183</td>\n      <td>0.290139</td>\n      <td>0.187438</td>\n      <td>0.006946</td>\n      <td>0.462733</td>\n      <td>upstream</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "02cb4089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "({'upstream': MinMaxScaler(),\n  'midstream': MinMaxScaler(),\n  'downstream': MinMaxScaler()},\n {'upstream': MinMaxScaler(),\n  'midstream': MinMaxScaler(),\n  'downstream': MinMaxScaler()})"
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scalers, y_scalers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892a26b1",
   "metadata": {},
   "source": [
    "### Postprocessing in a same manner with centalized learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ff4401b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_postprocessing(X_train, X_val, y_train, y_val, exogenous_data_train, exogenous_data_val, x_scalers, y_scalers):\n",
    "    \"\"\"Make data ready to be fed into ml algorithms\"\"\"\n",
    "    # if there are more than one specified areas, get the data per area\n",
    "    if X_train[args.identifier].nunique() != 1:\n",
    "        area_X_train, area_X_val, area_y_train, area_y_val = get_data_by_area(X_train, X_val,\n",
    "                                                                              y_train, y_val, \n",
    "                                                                              identifier=args.identifier)\n",
    "    else:\n",
    "        area_X_train, area_X_val, area_y_train, area_y_val = None, None, None, None\n",
    "\n",
    "    # Get the exogenous data per area.\n",
    "    if exogenous_data_train is not None:\n",
    "        exogenous_data_train, exogenous_data_val = get_exogenous_data_by_area(exogenous_data_train,\n",
    "                                                                              exogenous_data_val)\n",
    "    # transform to np\n",
    "    if area_X_train is not None:\n",
    "        for area in area_X_train:\n",
    "            tmp_X_train, tmp_y_train, tmp_X_val, tmp_y_val = remove_identifiers(\n",
    "                area_X_train[area], area_y_train[area], area_X_val[area], area_y_val[area])\n",
    "            tmp_X_train, tmp_y_train = tmp_X_train.to_numpy(), tmp_y_train.to_numpy()\n",
    "            tmp_X_val, tmp_y_val = tmp_X_val.to_numpy(), tmp_y_val.to_numpy()\n",
    "            area_X_train[area] = tmp_X_train\n",
    "            area_X_val[area] = tmp_X_val\n",
    "            area_y_train[area] = tmp_y_train\n",
    "            area_y_val[area] = tmp_y_val\n",
    "    \n",
    "    if exogenous_data_train is not None:\n",
    "        for area in exogenous_data_train:\n",
    "            exogenous_data_train[area] = exogenous_data_train[area].to_numpy()\n",
    "            exogenous_data_val[area] = exogenous_data_val[area].to_numpy()\n",
    "    \n",
    "    # remove identifiers from features, targets\n",
    "    X_train, y_train, X_val, y_val = remove_identifiers(X_train, y_train, X_val, y_val)\n",
    "    assert len(X_train.columns) == len(X_val.columns)\n",
    "    \n",
    "    num_features = len(X_train.columns) // args.num_lags\n",
    "    \n",
    "    # to timeseries representation\n",
    "    X_train = to_timeseries_rep(X_train.to_numpy(), num_lags=args.num_lags,\n",
    "                                            num_features=num_features)\n",
    "    X_val = to_timeseries_rep(X_val.to_numpy(), num_lags=args.num_lags,\n",
    "                                          num_features=num_features)\n",
    "    \n",
    "    if area_X_train is not None:\n",
    "        area_X_train = to_timeseries_rep(area_X_train, num_lags=args.num_lags,\n",
    "                                                     num_features=num_features)\n",
    "        area_X_val = to_timeseries_rep(area_X_val, num_lags=args.num_lags,\n",
    "                                                   num_features=num_features)\n",
    "    \n",
    "    # transform targets to numpy\n",
    "    y_train, y_val = y_train.to_numpy(), y_val.to_numpy()\n",
    "    \n",
    "    if exogenous_data_train is not None:\n",
    "        exogenous_data_train_combined, exogenous_data_val_combined = [], []\n",
    "        for area in exogenous_data_train:\n",
    "            exogenous_data_train_combined.extend(exogenous_data_train[area])\n",
    "            exogenous_data_val_combined.extend(exogenous_data_val[area])\n",
    "        exogenous_data_train_combined = np.stack(exogenous_data_train_combined)\n",
    "        exogenous_data_val_combined = np.stack(exogenous_data_val_combined)\n",
    "        exogenous_data_train[\"all\"] = exogenous_data_train_combined\n",
    "        exogenous_data_val[\"all\"] = exogenous_data_val_combined\n",
    "    return X_train, X_val, y_train, y_val, area_X_train, area_X_val, area_y_train, area_y_val, exogenous_data_train, exogenous_data_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "03064d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val, client_X_train, client_X_val, client_y_train, client_y_val, exogenous_data_train, exogenous_data_val = make_postprocessing(X_train, X_val, y_train, y_val, exogenous_data_train, exogenous_data_val, x_scalers, y_scalers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ee088254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['upstream', 'midstream', 'downstream'])"
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_X_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "aef6d703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['upstream', 'midstream', 'downstream'])"
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_X_val.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "dfe82601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Client: upstream\n",
      "X_train shape: (3881, 10, 9, 1), y_train shape: (3881, 6)\n",
      "X_val shape: (962, 10, 9, 1), y_val shape: (962, 6)\n",
      "\n",
      "Client: midstream\n",
      "X_train shape: (3934, 10, 9, 1), y_train shape: (3934, 6)\n",
      "X_val shape: (976, 10, 9, 1), y_val shape: (976, 6)\n",
      "\n",
      "Client: downstream\n",
      "X_train shape: (3926, 10, 9, 1), y_train shape: (3926, 6)\n",
      "X_val shape: (974, 10, 9, 1), y_val shape: (974, 6)\n"
     ]
    }
   ],
   "source": [
    "for client in client_X_train:\n",
    "    print(f\"\\nClient: {client}\")\n",
    "    print(f\"X_train shape: {client_X_train[client].shape}, y_train shape: {client_y_train[client].shape}\")\n",
    "    print(f\"X_val shape: {client_X_val[client].shape}, y_val shape: {client_y_val[client].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "922abedf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7c20f040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_dims(X_train, exogenous_data_train):\n",
    "    if args.model_name == \"mlp\":\n",
    "        input_dim = X_train.shape[1] * X_train.shape[2]\n",
    "    else:\n",
    "        input_dim = X_train.shape[2]\n",
    "    \n",
    "    if exogenous_data_train is not None:\n",
    "        if len(exogenous_data_train) == 1:\n",
    "            cid = next(iter(exogenous_data_train.keys()))\n",
    "            exogenous_dim = exogenous_data_train[cid].shape[1]\n",
    "        else:\n",
    "            exogenous_dim = exogenous_data_train[\"all\"].shape[1]\n",
    "    else:\n",
    "        exogenous_dim = 0\n",
    "    \n",
    "    return input_dim, exogenous_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "2e866f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model: str,\n",
    "              input_dim: int,\n",
    "              out_dim: int,\n",
    "              lags: int = 10,\n",
    "              exogenous_dim: int = 0,\n",
    "              seed=0):\n",
    "    if model == \"mlp\":\n",
    "        model = MLP(input_dim=input_dim, layer_units=[256, 128, 64], num_outputs=out_dim)\n",
    "    elif model == \"rnn\":\n",
    "        model = RNN(input_dim=input_dim, rnn_hidden_size=128, num_rnn_layers=1, rnn_dropout=0.0,\n",
    "                    layer_units=[128], num_outputs=out_dim, matrix_rep=True, exogenous_dim=exogenous_dim)\n",
    "    elif model == \"lstm\":\n",
    "        model = LSTM(input_dim=input_dim, lstm_hidden_size=128, num_lstm_layers=1, lstm_dropout=0.0,\n",
    "                     layer_units=[128], num_outputs=out_dim, matrix_rep=True, exogenous_dim=exogenous_dim)\n",
    "    elif model == \"gru\":\n",
    "        model = GRU(input_dim=input_dim, gru_hidden_size=128, num_gru_layers=1, gru_dropout=0.0,\n",
    "                    layer_units=[128], num_outputs=out_dim, matrix_rep=True, exogenous_dim=exogenous_dim)\n",
    "    elif model == \"cnn\":\n",
    "        model = CNN(num_features=input_dim, lags=lags, exogenous_dim=exogenous_dim, out_dim=out_dim)\n",
    "    elif model == \"da_encoder_decoder\":\n",
    "        model = DualAttentionAutoEncoder(input_dim=input_dim, architecture=\"lstm\", matrix_rep=True)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Specified model is not implemented. Plese define your own model or choose one from ['mlp', 'rnn', 'lstm', 'gru', 'cnn', 'da_encoder_decoder']\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1e163843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 0\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "args.model_name = \"cnn\"\n",
    "\n",
    "input_dim, exogenous_dim = get_input_dims(X_train, exogenous_data_train)\n",
    "\n",
    "print(input_dim, exogenous_dim)\n",
    "\n",
    "model = get_model(model=args.model_name,\n",
    "                  input_dim=input_dim,\n",
    "                  out_dim=y_train.shape[1],\n",
    "                  lags=args.num_lags,\n",
    "                  exogenous_dim=exogenous_dim,\n",
    "                  seed=args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "7e15bd45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "CNN(\n  (activation): ReLU()\n  (conv1): Conv2d(1, 16, kernel_size=(16, 3), stride=(1, 1), padding=same)\n  (conv2): Conv2d(16, 16, kernel_size=(3, 5), stride=(1, 1), padding=same)\n  (conv3): Conv2d(16, 32, kernel_size=(4, 3), stride=(1, 1), padding=same)\n  (conv4): Conv2d(32, 32, kernel_size=(4, 3), stride=(1, 1), padding=same)\n  (pool): AvgPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0)\n  (fc): Linear(in_features=1440, out_features=6, bias=True)\n)"
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2813c1",
   "metadata": {},
   "source": [
    "### Fit function initiates the training process of every base station local model and then performs parameters aggregation on a central server for N specified federated epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a913bbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, X_train, y_train, X_val, y_val, \n",
    "        exogenous_data_train=None, exogenous_data_val=None, \n",
    "        idxs=[0,1,3,7,8,2], # the indices of our targets in X\n",
    "        log_per=1,\n",
    "        client_creation_fn = None, # client specification\n",
    "        local_train_params=None, # local params\n",
    "        aggregation_params=None, # aggregation params\n",
    "        use_carbontracker=False\n",
    "       ):\n",
    "    # client creation definition\n",
    "    if client_creation_fn is None:\n",
    "        client_creation_fn = create_regression_client\n",
    "    # local params\n",
    "    if local_train_params is None:\n",
    "        local_train_params = {\n",
    "            \"epochs\": args.epochs, \"optimizer\": args.optimizer, \"lr\": args.lr,\n",
    "            \"criterion\": args.criterion, \"early_stopping\": args.local_early_stopping,\n",
    "            \"patience\": args.local_patience, \"device\": device\n",
    "        }\n",
    "    \n",
    "    train_loaders, val_loaders = [], []\n",
    "    \n",
    "    # get data per client\n",
    "    for client in X_train:\n",
    "        if client == \"all\":\n",
    "            continue\n",
    "        if exogenous_data_train is not None:\n",
    "            tmp_exogenous_data_train = exogenous_data_train[client]\n",
    "            tmp_exogenous_data_val = tmp_exogenous_data_val[client]\n",
    "        else:\n",
    "            tmp_exogenous_data_train = None\n",
    "            tmp_exogenous_data_val = None\n",
    "    \n",
    "        num_features = len(X_train[client][0][0])\n",
    "        \n",
    "        # to torch loader\n",
    "        train_loaders.append(\n",
    "            to_torch_dataset(\n",
    "                X_train[client], y_train[client],\n",
    "                num_lags=args.num_lags,\n",
    "                num_features=num_features,\n",
    "                exogenous_data=tmp_exogenous_data_train,\n",
    "                indices=idxs,\n",
    "                batch_size=args.batch_size,\n",
    "                shuffle=False\n",
    "            )\n",
    "        )\n",
    "        val_loaders.append(\n",
    "            to_torch_dataset(\n",
    "                X_val[client], y_val[client],\n",
    "                num_lags=args.num_lags,\n",
    "                exogenous_data=tmp_exogenous_data_val,\n",
    "                indices=idxs,\n",
    "                batch_size=args.batch_size,\n",
    "                shuffle=False\n",
    "            )\n",
    "            \n",
    "        )\n",
    "        \n",
    "    # create clients with their local data\n",
    "    cids = [k for k in X_train.keys() if k != \"all\"]\n",
    "    clients = [\n",
    "        client_creation_fn(\n",
    "            cid=cid, # client id\n",
    "            model=model, # the global model\n",
    "            train_loader=train_loader, # the local train loader\n",
    "            test_loader=val_loader, # the local val loader\n",
    "            local_params=local_train_params # local parameters\n",
    "        )\n",
    "        for cid, train_loader, val_loader in zip(cids, train_loaders, val_loaders)\n",
    "    ]\n",
    "    \n",
    "    # represent clients to server\n",
    "    client_proxies = [\n",
    "        SimpleClientProxy(cid, client) for cid, client in zip(cids, clients)\n",
    "    ]\n",
    "    \n",
    "    # represent the server\n",
    "    server = Server(\n",
    "        client_proxies=client_proxies, # the client representations\n",
    "        aggregation=args.aggregation, # the aggregation algorithm\n",
    "        aggregation_params=aggregation_params, # aggregation specific params\n",
    "        local_params_fn=None, # we can change the local params on demand\n",
    "    )\n",
    "    # Note that the client manager instance will be initialized automatically. You can define your own client manager.\n",
    "\n",
    "    # train with FL\n",
    "    model_params, history = server.fit(args.fl_rounds, args.fraction, use_carbontracker=use_carbontracker)\n",
    "    \n",
    "    params_dict = zip(model.state_dict().keys(), model_params)\n",
    "    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "    model = copy.deepcopy(model)\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "51240332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# federated local params\n",
    "local_train_params = {\"epochs\": args.epochs, \"optimizer\": args.optimizer, \"lr\": args.lr,\n",
    "                      \"criterion\": args.criterion, \"early_stopping\": args.local_early_stopping,\n",
    "                      \"patience\": args.local_patience, \"device\": device\n",
    "                      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b1054cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO logger 2024-06-03 08:14:20,552 | server.py:62 | Initializing client manager...\n",
      "INFO logger 2024-06-03 08:14:20,552 | server.py:69 | Registering clients...\n",
      "INFO logger 2024-06-03 08:14:20,553 | client_manager.py:66 | Registered client with id: upstream\n",
      "INFO logger 2024-06-03 08:14:20,553 | client_manager.py:66 | Registered client with id: midstream\n",
      "INFO logger 2024-06-03 08:14:20,553 | client_manager.py:66 | Registered client with id: downstream\n",
      "INFO logger 2024-06-03 08:14:20,553 | server.py:73 | Client manager initialized!\n",
      "INFO logger 2024-06-03 08:14:20,554 | server.py:55 | Aggregation algorithm: SimpleAvg()\n",
      "INFO logger 2024-06-03 08:14:20,554 | client_manager.py:88 | Parameter c=0.0. Sampled 1 client(s): ['midstream']\n",
      "INFO logger 2024-06-03 08:14:21,224 | server.py:86 | Starting FL rounds\n",
      "INFO logger 2024-06-03 08:14:21,227 | client_manager.py:88 | Parameter c=1.0. Sampled 3 client(s): ['midstream', 'upstream', 'downstream']\n",
      "D:\\Program\\anaconda\\envs\\fed\\lib\\site-packages\\torch\\autograd\\graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "INFO logger 2024-06-03 08:14:22,875 | train_utils.py:142 | Best Loss: 0.0001560016182159669\n",
      "D:\\Program\\anaconda\\envs\\fed\\lib\\site-packages\\torch\\autograd\\graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "INFO logger 2024-06-03 08:14:23,629 | train_utils.py:142 | Best Loss: 0.00012047470231654738\n",
      "D:\\Program\\anaconda\\envs\\fed\\lib\\site-packages\\torch\\autograd\\graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "INFO logger 2024-06-03 08:14:24,410 | train_utils.py:142 | Best Loss: 7.496257236101177e-05\n",
      "INFO logger 2024-06-03 08:14:24,500 | server.py:191 | [Global round 1] Aggregating local models...\n",
      "INFO logger 2024-06-03 08:14:24,776 | client_manager.py:88 | Parameter c=1.0. Sampled 3 client(s): ['downstream', 'midstream', 'upstream']\n",
      "INFO logger 2024-06-03 08:14:25,406 | train_utils.py:142 | Best Loss: 5.5007447222787005e-05\n",
      "INFO logger 2024-06-03 08:14:26,152 | train_utils.py:142 | Best Loss: 0.00010777601579082062\n",
      "INFO logger 2024-06-03 08:14:26,839 | train_utils.py:142 | Best Loss: 6.873955322805165e-05\n",
      "INFO logger 2024-06-03 08:14:26,926 | server.py:191 | [Global round 2] Aggregating local models...\n",
      "INFO logger 2024-06-03 08:14:27,195 | client_manager.py:88 | Parameter c=1.0. Sampled 3 client(s): ['midstream', 'downstream', 'upstream']\n",
      "INFO logger 2024-06-03 08:14:27,797 | train_utils.py:142 | Best Loss: 6.533208585603804e-05\n",
      "INFO logger 2024-06-03 08:14:28,554 | train_utils.py:142 | Best Loss: 5.1095616072416306e-05\n",
      "INFO logger 2024-06-03 08:14:29,261 | train_utils.py:142 | Best Loss: 6.0913455043290115e-05\n",
      "INFO logger 2024-06-03 08:14:29,350 | server.py:191 | [Global round 3] Aggregating local models...\n",
      "INFO logger 2024-06-03 08:14:29,614 | client_manager.py:88 | Parameter c=1.0. Sampled 3 client(s): ['downstream', 'upstream', 'midstream']\n",
      "INFO logger 2024-06-03 08:14:30,293 | train_utils.py:142 | Best Loss: 3.933274398464136e-05\n",
      "INFO logger 2024-06-03 08:14:30,976 | train_utils.py:142 | Best Loss: 4.6732939301511726e-05\n",
      "INFO logger 2024-06-03 08:14:31,663 | train_utils.py:142 | Best Loss: 5.993714411055944e-05\n",
      "INFO logger 2024-06-03 08:14:31,827 | server.py:191 | [Global round 4] Aggregating local models...\n",
      "INFO logger 2024-06-03 08:14:32,095 | client_manager.py:88 | Parameter c=1.0. Sampled 3 client(s): ['midstream', 'upstream', 'downstream']\n",
      "INFO logger 2024-06-03 08:14:32,686 | train_utils.py:142 | Best Loss: 6.337892946397855e-05\n",
      "INFO logger 2024-06-03 08:14:33,371 | train_utils.py:142 | Best Loss: 5.2376439709704284e-05\n",
      "INFO logger 2024-06-03 08:14:34,144 | train_utils.py:142 | Best Loss: 3.465448957495009e-05\n",
      "INFO logger 2024-06-03 08:14:34,233 | server.py:191 | [Global round 5] Aggregating local models...\n",
      "INFO logger 2024-06-03 08:14:34,505 | server.py:112 | Time passed: 13.277194261550903 seconds.\n",
      "INFO logger 2024-06-03 08:14:34,506 | server.py:113 | Best global model found on fl_round=5 with loss=0.00010451346307680789\n"
     ]
    }
   ],
   "source": [
    "global_model, history = fit(\n",
    "    model,\n",
    "    client_X_train,\n",
    "    client_y_train, \n",
    "    client_X_val, \n",
    "    client_y_val, \n",
    "    local_train_params=local_train_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c913897",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "dfb2ead6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "OrderedDict([('conv1.weight',\n              tensor([[[[-4.7581e-02, -5.2498e-02,  1.5617e-02],\n                        [-1.0877e-02,  1.5730e-02, -8.8933e-02],\n                        [-1.0140e-01, -1.3296e-02,  4.6949e-05],\n                        [-4.2465e-02, -9.1430e-02, -1.0499e-01],\n                        [ 4.7291e-02,  1.6294e-02,  3.8342e-02],\n                        [-2.2059e-02,  6.8426e-02, -2.3353e-02],\n                        [ 4.5175e-02, -4.0454e-02, -3.5840e-02],\n                        [-2.2462e-03, -5.1156e-02,  3.8547e-02],\n                        [ 3.5015e-02, -4.5142e-02,  2.1954e-02],\n                        [-1.4889e-02, -7.7429e-02,  2.0978e-02],\n                        [-1.2930e-03,  5.9037e-02, -4.8076e-03],\n                        [-3.0349e-02, -1.0640e-02,  2.5386e-02],\n                        [-3.0047e-02,  4.4646e-02, -4.2805e-02],\n                        [-5.4551e-02,  9.2332e-03,  1.2898e-01],\n                        [-3.4249e-02, -7.5406e-02, -2.0197e-02],\n                        [-7.1729e-02, -4.9213e-02, -8.8489e-02]]],\n              \n              \n                      [[[ 3.2765e-02, -8.5810e-02, -8.1106e-02],\n                        [-7.3243e-02, -6.2446e-02,  1.5747e-02],\n                        [ 5.9888e-02, -5.2793e-02,  4.6169e-02],\n                        [-7.6862e-02, -6.8492e-02,  1.9147e-02],\n                        [-2.7151e-02, -4.3251e-02, -5.4417e-03],\n                        [ 3.7072e-03,  6.2288e-02,  2.4555e-02],\n                        [-5.7896e-03, -7.9657e-02, -3.6283e-02],\n                        [-7.3718e-02, -2.7864e-02, -3.3662e-02],\n                        [-2.3822e-02, -7.4244e-02,  2.1722e-02],\n                        [ 2.8043e-02, -9.1230e-03, -4.0617e-02],\n                        [ 1.7227e-02,  5.3828e-02,  2.5886e-02],\n                        [ 8.2714e-03, -2.4393e-03,  1.7241e-02],\n                        [ 1.9525e-03, -2.5949e-02, -6.0043e-02],\n                        [ 5.3864e-02,  4.3332e-02, -7.9004e-02],\n                        [ 1.3181e-02, -6.1175e-02,  1.0750e-01],\n                        [-1.0121e-02, -5.0602e-02, -3.9439e-02]]],\n              \n              \n                      [[[ 4.8063e-02, -4.0426e-02, -1.2115e-02],\n                        [ 8.0515e-02, -4.6163e-02, -5.5870e-02],\n                        [ 2.4057e-02, -1.7166e-03, -6.4397e-02],\n                        [ 3.1213e-02, -7.3119e-02, -6.1145e-02],\n                        [-5.9483e-02, -4.0033e-02, -2.2566e-04],\n                        [-3.0621e-02, -6.5761e-02, -3.3969e-02],\n                        [ 2.3455e-02, -4.5391e-03, -8.1361e-02],\n                        [-6.5664e-02,  5.7843e-02,  9.5972e-03],\n                        [ 5.6115e-02, -5.2793e-03, -8.8943e-02],\n                        [-7.6662e-02,  4.3562e-02, -3.2978e-02],\n                        [-2.8425e-02, -8.6620e-02,  4.3225e-02],\n                        [ 6.7572e-03, -6.7957e-02,  1.4472e-02],\n                        [-5.3044e-02, -2.9775e-02, -5.2453e-02],\n                        [-2.3386e-02, -2.2233e-02, -6.4753e-02],\n                        [-5.9905e-02, -7.8555e-02,  5.4434e-02],\n                        [-5.2808e-02,  6.3956e-02,  5.6435e-02]]],\n              \n              \n                      [[[-5.2683e-02,  6.8303e-02,  1.2635e-02],\n                        [-7.5533e-03,  5.5543e-02,  6.7065e-02],\n                        [-1.3747e-03,  9.3289e-02, -4.4032e-02],\n                        [ 5.5794e-02,  5.7915e-02,  8.1150e-02],\n                        [-2.0939e-02,  9.0700e-02,  7.7249e-02],\n                        [-7.9118e-02,  9.0478e-02, -6.2286e-02],\n                        [ 6.9730e-02, -5.0999e-02,  1.4228e-02],\n                        [-6.8429e-02,  5.6906e-02, -4.0819e-02],\n                        [-4.9858e-02, -1.4008e-02, -6.9943e-02],\n                        [-5.6455e-02, -2.4762e-02, -8.8019e-02],\n                        [-9.0564e-02, -4.8440e-02,  4.3427e-02],\n                        [-5.3176e-03, -3.9906e-02, -6.0950e-02],\n                        [ 1.5139e-02, -4.6064e-02, -7.1401e-02],\n                        [-5.6243e-02, -4.2559e-02,  4.4511e-02],\n                        [ 9.3397e-03, -3.1923e-02,  7.6294e-02],\n                        [ 7.2986e-02,  2.3806e-02,  9.7314e-02]]],\n              \n              \n                      [[[-3.7898e-02, -4.0492e-02,  2.1999e-02],\n                        [ 9.0948e-02,  2.2910e-02, -7.2257e-02],\n                        [-1.3611e-02, -1.2714e-02, -5.5749e-02],\n                        [-4.0169e-02, -1.8967e-02,  8.4599e-03],\n                        [-2.2021e-02,  8.0755e-02, -4.3927e-02],\n                        [ 6.2909e-03,  6.7880e-02, -3.4731e-04],\n                        [-8.5586e-02, -2.4986e-03,  3.6372e-02],\n                        [-9.1212e-02,  4.9737e-02,  1.1212e-02],\n                        [-9.0384e-02, -2.2980e-03, -5.5367e-02],\n                        [ 4.1572e-02, -1.1659e-01, -8.7238e-02],\n                        [-3.5598e-02, -1.2033e-02, -3.9715e-02],\n                        [-5.5017e-02, -7.3857e-02,  4.5414e-02],\n                        [ 5.0106e-02, -6.5400e-02, -1.9480e-02],\n                        [-9.9484e-02,  6.1137e-02, -4.9270e-02],\n                        [-7.2144e-02, -6.3173e-02, -6.7243e-03],\n                        [ 3.6487e-02, -6.8204e-02, -1.7980e-02]]],\n              \n              \n                      [[[ 6.0369e-02,  2.1926e-02, -6.7253e-02],\n                        [ 5.9736e-02,  7.9423e-02, -5.6804e-03],\n                        [ 9.7040e-02,  7.0640e-02, -7.0971e-02],\n                        [-2.0369e-02,  9.0547e-02, -2.7947e-02],\n                        [ 9.1526e-02, -4.9601e-02,  7.2226e-02],\n                        [ 3.9576e-02, -5.5928e-02, -7.5980e-02],\n                        [-5.2384e-02,  9.1462e-02,  2.7656e-02],\n                        [ 3.8490e-02, -1.2849e-02, -8.4312e-02],\n                        [ 8.1175e-02, -2.9741e-02, -1.7164e-02],\n                        [-4.3236e-03,  8.5362e-02,  3.1061e-02],\n                        [-1.5598e-02,  8.6401e-02,  7.7218e-02],\n                        [-3.1939e-02,  7.9647e-02, -3.2144e-02],\n                        [-1.0482e-01,  6.9587e-02,  1.8735e-03],\n                        [-6.3242e-02, -7.7963e-02, -2.8053e-02],\n                        [-1.0122e-01, -2.3888e-02, -9.5599e-02],\n                        [-2.8678e-02, -3.9799e-03, -3.1313e-02]]],\n              \n              \n                      [[[-6.4036e-02, -6.6219e-02, -4.2487e-02],\n                        [ 3.2827e-02,  6.2266e-02,  3.4227e-02],\n                        [-5.8562e-02, -4.8424e-02,  8.0831e-03],\n                        [-1.0565e-01,  5.4514e-02,  6.0228e-02],\n                        [ 1.1242e-02,  8.1694e-02, -7.5817e-02],\n                        [-4.8854e-02,  2.0932e-02, -2.9744e-02],\n                        [-3.3789e-02,  7.5233e-02, -8.8045e-02],\n                        [ 9.2666e-04,  2.3898e-02,  1.9727e-02],\n                        [-1.0518e-01,  6.2916e-02,  5.9944e-02],\n                        [-9.2463e-02, -5.4053e-02,  1.3934e-02],\n                        [-1.1882e-03,  4.2655e-02, -7.9382e-03],\n                        [-5.1585e-02,  5.1475e-02, -2.7081e-02],\n                        [-7.5293e-02, -5.1758e-02, -9.8202e-02],\n                        [-1.1466e-02,  3.7146e-03, -3.3282e-02],\n                        [ 1.6732e-02,  2.9449e-02,  2.0838e-02],\n                        [-1.0193e-01,  2.9819e-02,  1.7757e-03]]],\n              \n              \n                      [[[-1.3092e-02,  8.1896e-02,  6.6000e-02],\n                        [-1.3017e-02, -6.6352e-02,  9.6231e-02],\n                        [-5.6067e-02,  9.6890e-02, -2.6814e-03],\n                        [-4.6464e-02,  2.2578e-02,  3.5785e-02],\n                        [-8.1116e-02,  7.2924e-02,  4.6544e-02],\n                        [ 4.2129e-02, -1.3902e-02,  1.7253e-02],\n                        [-9.0967e-02, -6.3321e-02, -1.8870e-02],\n                        [-2.8784e-03,  9.1465e-02,  2.5918e-02],\n                        [ 5.4424e-02, -7.0066e-02,  2.8255e-02],\n                        [-6.2487e-03, -7.7018e-02,  5.9714e-02],\n                        [-7.3599e-03, -5.0678e-02, -1.1547e-02],\n                        [ 2.8285e-02, -1.2115e-01,  4.8460e-02],\n                        [ 5.4168e-02, -6.6410e-02,  5.1096e-02],\n                        [-7.4005e-03, -1.0969e-01,  6.1351e-02],\n                        [-8.1477e-03,  9.8471e-03,  6.4029e-02],\n                        [ 3.4256e-02, -7.6776e-02, -3.7036e-02]]],\n              \n              \n                      [[[-2.6809e-02, -5.9590e-03, -7.7455e-02],\n                        [-1.6101e-02,  4.5105e-02,  1.1436e-02],\n                        [ 8.6115e-02, -2.0452e-02,  8.8582e-03],\n                        [-2.9926e-02, -3.3720e-02, -8.5965e-02],\n                        [-3.9080e-02,  3.5437e-02,  4.2498e-02],\n                        [-2.3117e-02,  1.8038e-02,  1.6007e-02],\n                        [-1.1308e-02, -2.8824e-02, -5.2256e-02],\n                        [-1.9409e-02, -3.5466e-02,  1.0936e-02],\n                        [-2.5070e-02, -8.5680e-02,  4.9218e-02],\n                        [ 7.4134e-02,  6.2493e-02,  4.3622e-03],\n                        [ 3.8248e-02, -9.9090e-02,  7.9186e-02],\n                        [-1.1261e-01, -9.7043e-02, -6.2190e-02],\n                        [ 1.1240e-02, -1.6588e-02, -1.1306e-02],\n                        [ 2.1946e-02,  7.7323e-03, -5.7721e-02],\n                        [-1.1990e-01,  6.2941e-02, -1.0188e-01],\n                        [-1.7882e-02, -8.6553e-02, -8.3942e-02]]],\n              \n              \n                      [[[-6.9649e-02,  8.1745e-02, -7.7433e-02],\n                        [ 7.9779e-02, -4.9343e-02, -5.6477e-02],\n                        [ 9.5552e-02, -9.4680e-03, -3.6853e-02],\n                        [-7.1154e-03, -6.7139e-02,  1.2219e-02],\n                        [-1.7667e-02,  5.2635e-02,  5.8595e-02],\n                        [-2.5107e-02,  8.2285e-02, -3.2755e-04],\n                        [ 2.7716e-03, -7.7999e-02,  6.3640e-03],\n                        [ 2.2226e-02, -5.9896e-02, -1.9068e-02],\n                        [-5.3052e-02, -7.8523e-02, -3.8382e-02],\n                        [-2.0716e-02, -7.5396e-02, -6.2880e-02],\n                        [-6.5288e-02, -4.7656e-02,  2.1913e-02],\n                        [-4.6959e-02,  4.4093e-02, -6.7393e-03],\n                        [-2.0432e-03,  9.5135e-02,  8.5414e-03],\n                        [ 9.5303e-02, -6.4093e-02, -5.2074e-02],\n                        [ 2.7592e-02,  1.0780e-02, -4.0908e-02],\n                        [-6.9216e-02, -7.5216e-03,  8.9772e-02]]],\n              \n              \n                      [[[ 1.5521e-02, -4.9918e-02,  5.8344e-02],\n                        [-5.5637e-02, -2.4832e-02, -4.6520e-03],\n                        [ 1.3054e-03, -6.2625e-03, -3.8694e-02],\n                        [-1.1672e-02,  4.7552e-02, -3.6669e-02],\n                        [ 6.2398e-03,  7.2825e-02, -6.4974e-02],\n                        [-4.6649e-02, -5.7607e-02,  2.1306e-02],\n                        [-2.5287e-03,  2.0688e-02, -7.2473e-02],\n                        [ 5.0380e-02, -5.7901e-02,  8.8859e-03],\n                        [ 2.4575e-03, -5.1132e-02,  4.5559e-02],\n                        [ 4.4302e-02,  8.6011e-02, -9.0284e-02],\n                        [ 7.7607e-02,  3.3339e-02, -5.1184e-02],\n                        [ 9.0987e-02,  8.7391e-02, -5.8419e-02],\n                        [-2.4133e-02,  9.4320e-03,  2.4086e-02],\n                        [ 8.9348e-02,  3.6504e-02,  6.5040e-02],\n                        [ 1.6956e-02,  8.6905e-02, -5.6786e-02],\n                        [ 1.9901e-02,  8.1672e-02,  3.2010e-02]]],\n              \n              \n                      [[[-7.1982e-02, -1.6011e-02,  3.5038e-02],\n                        [ 4.7465e-02, -8.0002e-02, -4.3810e-02],\n                        [-6.3286e-02, -7.4658e-02, -3.0043e-02],\n                        [-8.1692e-02, -9.4254e-04,  8.3492e-02],\n                        [-7.9621e-03, -7.9308e-03,  3.2130e-02],\n                        [-6.4160e-03,  2.0043e-02,  6.5308e-02],\n                        [ 6.3714e-02,  4.1236e-02, -1.9476e-02],\n                        [ 4.2517e-02,  7.0239e-02,  8.4842e-02],\n                        [-1.2251e-02,  1.8821e-02,  2.0045e-02],\n                        [-3.7411e-02, -2.2465e-02,  8.3131e-02],\n                        [ 5.3536e-02, -6.4521e-02,  9.4595e-02],\n                        [-5.7078e-02, -5.9093e-02,  9.4646e-02],\n                        [-9.1685e-03,  7.5687e-02, -6.9623e-03],\n                        [-7.8507e-02, -7.2675e-03, -1.1866e-02],\n                        [-3.0860e-02,  3.3871e-03,  1.1097e-01],\n                        [-1.0910e-01,  6.3237e-02, -4.6455e-02]]],\n              \n              \n                      [[[-1.2099e-02, -8.8606e-02, -3.6545e-02],\n                        [-3.0882e-03,  4.4279e-02, -2.5064e-02],\n                        [ 2.1438e-02,  5.6427e-02,  5.9704e-04],\n                        [ 3.4011e-02, -6.7101e-03, -7.4112e-02],\n                        [ 5.7447e-02, -5.1828e-02, -6.6971e-02],\n                        [ 7.2194e-02, -7.5867e-02,  5.0035e-02],\n                        [-3.7866e-03,  2.5040e-02, -9.3392e-02],\n                        [-5.0002e-02, -4.9617e-02, -6.0399e-02],\n                        [-6.4320e-02, -2.9612e-03, -6.7182e-02],\n                        [-2.6289e-02, -8.0519e-03, -2.6264e-02],\n                        [-8.3186e-03, -1.9075e-02, -9.7076e-02],\n                        [-5.0540e-02, -4.1098e-02, -5.1362e-02],\n                        [-9.4910e-02,  1.0309e-02, -7.5848e-02],\n                        [-6.8895e-02,  1.0838e-02,  1.5198e-02],\n                        [ 2.1576e-02,  6.4253e-02, -9.9158e-02],\n                        [-6.3661e-02,  7.7763e-02, -1.0397e-01]]],\n              \n              \n                      [[[ 2.8477e-02, -1.0674e-01,  3.8366e-02],\n                        [ 3.4447e-02, -2.9580e-02, -2.9016e-02],\n                        [-5.2338e-02,  5.2312e-02, -6.1963e-02],\n                        [ 4.2183e-02, -5.3253e-02, -4.8488e-02],\n                        [-5.7753e-02,  8.5164e-02, -7.6620e-02],\n                        [ 7.2706e-02,  2.8970e-02,  6.1289e-02],\n                        [-7.2355e-02, -3.2928e-02, -5.5398e-02],\n                        [ 5.5184e-02, -8.1424e-02,  4.8601e-03],\n                        [ 1.7905e-02,  7.5889e-02, -4.8246e-02],\n                        [ 7.8102e-02,  6.3724e-04, -6.3492e-03],\n                        [-4.8898e-02,  9.8833e-02,  1.6225e-02],\n                        [ 8.1530e-02, -1.2338e-02, -2.9830e-02],\n                        [ 7.5944e-02,  8.3084e-02,  6.4079e-03],\n                        [ 1.0376e-01, -6.0820e-02, -3.7929e-02],\n                        [ 4.0887e-02,  3.2573e-02,  1.4135e-02],\n                        [ 3.0300e-04,  9.4096e-02,  2.4751e-02]]],\n              \n              \n                      [[[ 3.1372e-03, -7.6952e-02, -3.2344e-02],\n                        [-3.2143e-02, -7.2239e-02,  2.5169e-02],\n                        [ 4.0350e-02, -1.4470e-02, -5.8720e-02],\n                        [ 2.0166e-02, -1.1617e-01,  6.1435e-02],\n                        [-5.2507e-02, -1.0991e-01, -1.2860e-02],\n                        [-4.7142e-02,  6.4081e-02,  6.0635e-02],\n                        [-6.9683e-02,  8.7341e-02,  3.6534e-02],\n                        [-8.3267e-02,  1.2800e-02, -1.4074e-02],\n                        [ 1.4034e-02,  7.0524e-04, -7.2155e-02],\n                        [ 6.3052e-02,  1.0174e-01, -9.9033e-02],\n                        [ 1.0765e-01, -6.3600e-02, -4.0649e-02],\n                        [ 4.3766e-02, -3.6685e-02,  3.4306e-02],\n                        [-1.3270e-02,  7.6709e-02, -9.4348e-02],\n                        [ 6.9527e-02, -5.3522e-02,  3.0214e-02],\n                        [ 1.1161e-01,  1.0064e-01, -9.3266e-02],\n                        [ 7.8870e-02,  1.8673e-02, -1.0978e-01]]],\n              \n              \n                      [[[ 4.2609e-02,  5.0343e-02,  9.4004e-02],\n                        [ 8.6841e-02, -6.2448e-02, -2.8529e-03],\n                        [-4.1708e-03,  1.7053e-02, -2.2843e-02],\n                        [-6.5662e-02, -6.5043e-02,  8.0640e-02],\n                        [-4.5512e-02,  3.9070e-02,  9.8361e-02],\n                        [-2.3456e-03, -8.7635e-02, -5.3635e-02],\n                        [ 2.5611e-02, -9.0534e-02,  6.9574e-02],\n                        [-1.9802e-02, -7.7369e-03, -4.2395e-03],\n                        [-4.6946e-02, -9.6013e-02,  7.5816e-02],\n                        [-1.3713e-02, -2.2078e-02, -6.3523e-02],\n                        [-1.6753e-02, -1.7661e-02, -4.4948e-02],\n                        [-1.1720e-02, -1.5394e-03, -1.3155e-01],\n                        [-8.7571e-02, -4.2476e-02, -1.3612e-01],\n                        [-1.7705e-02,  4.9440e-02, -3.0812e-02],\n                        [ 3.2759e-02, -5.4336e-02,  3.4264e-02],\n                        [ 4.4947e-02, -6.9764e-02, -5.1005e-02]]]])),\n             ('conv1.bias',\n              tensor([-0.0045, -0.0125, -0.0029, -0.0105,  0.0028,  0.0004, -0.0055, -0.0159,\n                      -0.0132,  0.0031, -0.0008, -0.0065,  0.0007, -0.0027, -0.0128, -0.0013])),\n             ('conv2.weight',\n              tensor([[[[ 4.2775e-02,  5.4125e-02, -3.1174e-02,  4.7914e-02,  5.0119e-02],\n                        [-1.8698e-02, -8.1055e-03,  1.1078e-02, -4.0229e-03, -5.5556e-02],\n                        [ 2.7458e-02, -5.4789e-02, -3.5982e-02,  3.9487e-02, -1.1241e-02]],\n              \n                       [[-1.8839e-02, -5.8686e-02, -1.1575e-01,  7.6510e-02, -3.4738e-02],\n                        [ 1.4519e-03,  6.1714e-02,  7.3957e-02,  1.0167e-01,  7.8245e-02],\n                        [-6.4279e-02, -1.2053e-01,  2.9758e-02, -5.3798e-02,  4.0974e-02]],\n              \n                       [[ 2.0475e-02, -9.8996e-02, -1.9230e-02, -2.6931e-02, -2.9757e-03],\n                        [ 2.2631e-02,  6.4846e-02, -8.0318e-02, -8.4056e-03, -6.6136e-02],\n                        [ 6.4664e-02,  8.2859e-02, -4.2264e-02,  9.0046e-02,  1.0626e-01]],\n              \n                       ...,\n              \n                       [[-3.9700e-02, -2.7093e-02, -1.0047e-01, -4.2749e-02, -8.1212e-02],\n                        [ 4.3222e-02,  3.6398e-02, -7.9690e-02, -1.1425e-02,  1.0339e-01],\n                        [-4.1680e-03, -2.0476e-02,  2.7874e-02, -1.1301e-01, -6.6453e-03]],\n              \n                       [[-3.6965e-02,  1.0848e-01,  7.9458e-02, -3.7953e-02,  2.9312e-02],\n                        [ 2.4172e-02, -2.2391e-02, -6.8862e-02,  4.8363e-02, -4.0756e-02],\n                        [ 6.3966e-02,  1.4107e-01,  9.5501e-02,  1.1994e-01, -1.0090e-02]],\n              \n                       [[ 9.6889e-02, -2.8273e-02, -1.4222e-01, -2.5860e-02,  8.2387e-02],\n                        [ 2.6418e-02, -7.9937e-02, -8.4296e-02, -1.8097e-02,  8.6080e-02],\n                        [-9.7499e-02, -2.4980e-02, -4.4682e-02,  1.4582e-02, -6.1521e-02]]],\n              \n              \n                      [[[-6.1976e-02, -1.0865e-01,  5.8927e-02, -1.2637e-01, -4.6379e-02],\n                        [ 8.0543e-02,  8.7526e-02, -4.2942e-03,  2.9118e-02,  1.9102e-03],\n                        [ 1.1777e-02, -1.7847e-02,  4.9790e-02,  6.2009e-02, -1.3621e-01]],\n              \n                       [[-1.2616e-01, -9.5161e-02, -6.5299e-02, -3.7696e-02,  1.9846e-03],\n                        [ 4.5571e-02,  1.0960e-01,  1.6201e-02,  1.1103e-01,  8.4787e-02],\n                        [-9.7590e-02,  5.7888e-02,  8.3123e-02,  2.7127e-02, -7.5202e-02]],\n              \n                       [[-4.6137e-02,  8.4653e-03,  2.6647e-02,  3.7128e-02, -1.3798e-01],\n                        [-9.8437e-02,  1.2108e-01, -1.5990e-02, -6.7666e-03, -1.3220e-01],\n                        [ 8.8731e-02,  4.2781e-02,  7.7687e-02,  7.2151e-02,  1.5482e-02]],\n              \n                       ...,\n              \n                       [[ 1.4456e-03, -3.6631e-02, -2.9077e-02,  2.8826e-02,  1.0278e-01],\n                        [ 9.8543e-02,  7.9639e-02, -7.8899e-02,  5.4153e-02, -1.9183e-02],\n                        [-1.3512e-02,  3.0363e-02, -8.2672e-03,  8.5722e-02,  9.1475e-02]],\n              \n                       [[ 3.6540e-02, -9.7726e-02, -7.8621e-02,  8.5543e-02, -2.7430e-02],\n                        [ 3.0430e-03,  4.9899e-02,  7.8540e-02, -3.2345e-03,  4.2506e-02],\n                        [ 2.3988e-02,  3.0833e-02,  3.1552e-02,  1.0561e-01, -2.6964e-02]],\n              \n                       [[ 6.2158e-02, -3.3684e-02, -5.0577e-02,  1.0741e-01,  9.8883e-02],\n                        [-3.5977e-02,  3.9654e-03, -5.7115e-02,  1.2492e-01, -1.6149e-02],\n                        [-9.0556e-02,  3.0641e-02, -3.1930e-02,  9.9266e-02, -7.7081e-02]]],\n              \n              \n                      [[[ 1.3555e-03,  7.3648e-02,  1.6722e-02, -3.8907e-02,  6.3197e-02],\n                        [ 2.3252e-03,  6.4055e-02,  6.7786e-02,  6.8929e-02,  9.5497e-02],\n                        [-1.6365e-01,  8.7059e-02,  1.3401e-03, -4.2696e-02, -3.0397e-02]],\n              \n                       [[-2.7516e-02,  1.6845e-03, -8.6232e-02,  5.5627e-02, -2.8923e-02],\n                        [-2.1910e-02,  1.8379e-02, -1.1829e-01,  7.2493e-02, -2.5094e-02],\n                        [-1.1807e-01, -5.3491e-02, -6.9172e-02,  6.6695e-02, -1.4652e-02]],\n              \n                       [[ 8.0773e-02,  7.9976e-02, -8.6481e-02,  4.8310e-02,  6.8006e-02],\n                        [-6.8861e-02, -7.7756e-02,  3.9420e-03, -2.1401e-02, -3.8910e-02],\n                        [ 4.7757e-02, -5.6168e-03, -1.2461e-01,  2.7006e-02, -1.6693e-02]],\n              \n                       ...,\n              \n                       [[ 3.8803e-02,  4.5468e-02, -5.3782e-02, -1.0296e-02,  1.9723e-02],\n                        [-4.1696e-03, -1.2175e-01, -1.2388e-01, -6.3185e-02,  6.5225e-02],\n                        [-1.1786e-01, -3.2494e-02, -8.3777e-02,  5.1228e-02,  3.1296e-02]],\n              \n                       [[ 2.1633e-02, -5.4134e-02, -1.5510e-01, -8.1564e-02,  1.2983e-02],\n                        [ 8.6361e-02, -1.2188e-01,  3.0394e-02, -9.2132e-02,  1.0218e-01],\n                        [-7.2612e-02,  7.0727e-02, -8.4068e-02, -1.2274e-01, -2.1421e-02]],\n              \n                       [[-2.4536e-02, -7.8648e-02,  1.9810e-02,  3.3846e-02,  4.3838e-02],\n                        [-7.6633e-02, -4.2282e-02, -2.9287e-02,  3.9019e-02, -6.4046e-02],\n                        [ 1.3143e-02, -8.0666e-03,  2.2448e-02, -4.9691e-02, -7.9446e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-4.5792e-02,  1.6529e-02, -1.0612e-01,  1.0753e-01,  4.1349e-02],\n                        [-9.7812e-03,  7.1285e-02,  2.2349e-02,  1.5435e-01,  9.6735e-04],\n                        [-5.3211e-02,  3.8857e-02, -9.8133e-02,  1.2520e-03, -4.3954e-03]],\n              \n                       [[-1.3315e-02,  6.2406e-03, -1.0415e-02,  1.1477e-01,  4.3809e-02],\n                        [-1.1949e-01, -8.2074e-02,  1.4351e-03, -3.2271e-02, -9.5584e-02],\n                        [ 9.0371e-03, -1.8883e-02, -6.5620e-02, -6.7759e-02,  4.3789e-02]],\n              \n                       [[-2.4408e-02, -8.4565e-02,  5.9678e-02,  1.3156e-01,  2.6893e-02],\n                        [-3.6768e-03, -1.0179e-01, -6.4473e-02,  7.1074e-02,  2.5500e-02],\n                        [-6.9174e-02, -2.2989e-02, -6.2722e-02,  5.8031e-03,  6.6271e-02]],\n              \n                       ...,\n              \n                       [[ 1.7532e-02,  1.2793e-01, -8.1012e-03, -5.8270e-02, -4.4962e-02],\n                        [ 4.3616e-02,  9.6507e-02, -1.8696e-04, -1.0397e-01,  5.7443e-02],\n                        [-6.2661e-02,  3.3185e-03,  8.0744e-02, -5.3503e-02,  7.9618e-02]],\n              \n                       [[-8.8653e-02, -9.5927e-02,  9.0721e-02,  6.6682e-02, -3.4917e-02],\n                        [ 3.5365e-02, -2.4282e-02, -1.9548e-02, -8.2538e-02, -6.5679e-04],\n                        [ 4.7327e-02,  2.8397e-02,  9.3435e-02,  1.1354e-01,  9.7047e-02]],\n              \n                       [[ 2.1799e-02, -3.5327e-02, -1.1081e-01,  1.2669e-01, -7.9269e-02],\n                        [-7.9258e-02,  6.1496e-02, -1.9413e-02,  7.0273e-02,  6.7347e-02],\n                        [ 5.6403e-02,  2.5257e-02, -1.3864e-03,  9.5660e-02, -3.9097e-02]]],\n              \n              \n                      [[[-5.5573e-03,  5.3081e-02, -5.9655e-02,  2.4098e-03, -3.9143e-02],\n                        [ 1.1106e-01, -2.9416e-02, -2.5911e-03, -1.7076e-02, -2.0487e-02],\n                        [ 7.4891e-02, -2.7223e-03, -4.6945e-02,  8.7730e-02,  1.4405e-02]],\n              \n                       [[-2.1925e-02, -6.4649e-02, -7.9553e-02,  7.5912e-02,  2.4144e-02],\n                        [ 4.4149e-02,  9.8598e-02, -5.7713e-03,  7.5914e-02,  6.6652e-02],\n                        [ 1.0675e-01, -6.8937e-02,  7.6814e-02,  6.7494e-02, -3.3026e-02]],\n              \n                       [[ 3.1835e-03,  1.1172e-01, -5.3917e-04,  6.7086e-02, -8.9532e-02],\n                        [ 8.8372e-03, -3.4900e-02,  1.2973e-01,  1.7669e-02, -3.2016e-02],\n                        [-5.6971e-02,  5.2145e-02,  7.7851e-02, -6.8882e-02,  1.3257e-02]],\n              \n                       ...,\n              \n                       [[-7.1199e-02, -8.2085e-02,  1.3153e-01, -4.2841e-02, -5.5108e-02],\n                        [-8.1553e-02,  2.8554e-02,  7.5111e-02, -7.1962e-02, -8.5293e-02],\n                        [-1.0285e-01, -7.9557e-02, -2.4543e-02, -1.3791e-01,  1.7650e-02]],\n              \n                       [[-6.2679e-02,  3.9053e-02,  2.6193e-02, -8.7024e-02, -2.4053e-02],\n                        [ 5.6440e-02, -9.7221e-02,  7.4051e-02, -1.0501e-01,  1.4497e-02],\n                        [-6.4405e-02, -9.7569e-02,  7.5483e-03,  1.1244e-01, -2.3161e-02]],\n              \n                       [[-4.5167e-03, -1.1438e-03,  1.2152e-01,  7.3533e-02, -1.3692e-01],\n                        [-3.9311e-02,  5.0402e-02, -5.1485e-04,  6.2823e-02, -4.5420e-03],\n                        [-5.0739e-02, -6.4395e-02,  7.8975e-02,  1.1973e-02, -1.9220e-03]]],\n              \n              \n                      [[[-8.5152e-02,  1.1870e-01, -6.1831e-02,  9.4697e-03, -7.0402e-02],\n                        [-3.7982e-02, -1.0812e-02, -8.7691e-03,  1.2663e-01,  3.3942e-02],\n                        [ 1.8176e-02, -1.0822e-01, -4.4661e-02,  4.9184e-02, -1.3654e-02]],\n              \n                       [[-1.1334e-01,  1.0158e-01,  3.1082e-02, -3.6515e-02, -7.6678e-02],\n                        [ 1.4795e-02, -1.6407e-02, -1.0425e-01,  2.7792e-02, -3.9716e-02],\n                        [ 8.3220e-02,  2.2818e-02,  3.2691e-02,  7.9559e-02, -5.3302e-02]],\n              \n                       [[ 3.8962e-02, -3.2127e-02,  1.0077e-01, -1.6247e-02,  1.6694e-01],\n                        [ 1.3560e-02,  5.0260e-02,  2.7927e-03, -7.6911e-02, -5.6391e-02],\n                        [-5.5853e-02, -6.5813e-02,  1.0497e-01, -6.7441e-02, -8.8877e-02]],\n              \n                       ...,\n              \n                       [[ 4.1503e-02, -4.0244e-02, -8.4493e-02, -7.3621e-02,  6.8342e-02],\n                        [ 1.1121e-01, -5.4660e-03,  8.4364e-02, -5.6848e-02, -1.1799e-01],\n                        [-2.5967e-02,  9.4160e-02, -9.0893e-02, -2.2264e-02, -1.1708e-01]],\n              \n                       [[-7.8666e-02,  3.8280e-05,  5.8967e-02,  5.9224e-02, -6.5580e-02],\n                        [-5.3728e-02, -1.2768e-01,  1.8180e-02,  4.9093e-02, -5.4575e-04],\n                        [ 3.5877e-02,  6.1919e-02,  4.8550e-03, -4.2855e-02,  7.0550e-02]],\n              \n                       [[ 2.9950e-02, -2.3182e-02, -2.9014e-02,  1.1162e-02, -7.1473e-02],\n                        [-7.8716e-02,  1.2302e-01,  6.0691e-02, -1.1833e-01, -1.9295e-02],\n                        [-9.6812e-02,  3.9818e-02, -7.1133e-02,  7.0229e-02, -1.1575e-01]]]])),\n             ('conv2.bias',\n              tensor([ 3.1759e-04,  6.5166e-03, -6.6575e-05,  2.1734e-03, -4.8690e-03,\n                      -6.2552e-03, -8.5927e-03,  1.0325e-02, -4.1003e-03, -1.0303e-02,\n                       1.0673e-02, -3.9682e-03, -1.3683e-02,  2.0080e-03,  1.5451e-03,\n                      -7.8563e-03])),\n             ('conv3.weight',\n              tensor([[[[-0.0035, -0.0185, -0.0716],\n                        [ 0.0509, -0.1168, -0.1158],\n                        [ 0.0685, -0.0701,  0.0467],\n                        [-0.0839,  0.0297,  0.0007]],\n              \n                       [[ 0.0402, -0.0628,  0.0773],\n                        [ 0.0936,  0.0533,  0.0244],\n                        [-0.0973,  0.0278, -0.0955],\n                        [ 0.0133, -0.0439,  0.0199]],\n              \n                       [[-0.0655, -0.0039,  0.0800],\n                        [ 0.0127,  0.0230, -0.0033],\n                        [-0.1082, -0.0289, -0.0259],\n                        [-0.0217,  0.0312,  0.0166]],\n              \n                       ...,\n              \n                       [[-0.0454, -0.0541,  0.0351],\n                        [ 0.0768,  0.0267, -0.0169],\n                        [ 0.0067, -0.0987, -0.0917],\n                        [ 0.0851, -0.0003, -0.1334]],\n              \n                       [[-0.0668, -0.0597, -0.0583],\n                        [-0.0727,  0.0683, -0.1240],\n                        [ 0.0432, -0.0189, -0.1035],\n                        [ 0.0549,  0.0167, -0.1000]],\n              \n                       [[ 0.0127, -0.0892,  0.0630],\n                        [ 0.0194, -0.0195,  0.0591],\n                        [ 0.0425,  0.0005, -0.0627],\n                        [ 0.0430, -0.1206, -0.1089]]],\n              \n              \n                      [[[-0.0629, -0.0653,  0.0392],\n                        [-0.0909, -0.0847, -0.1013],\n                        [ 0.0524,  0.0290, -0.0636],\n                        [-0.0587, -0.0853, -0.0706]],\n              \n                       [[-0.0804, -0.0531, -0.0939],\n                        [ 0.0505, -0.0619, -0.0612],\n                        [-0.0862,  0.0470, -0.0646],\n                        [-0.0286,  0.0728, -0.0026]],\n              \n                       [[-0.0198, -0.0403, -0.0617],\n                        [ 0.0815, -0.0177,  0.0387],\n                        [-0.0828,  0.1016,  0.0841],\n                        [ 0.0272, -0.0040,  0.0308]],\n              \n                       ...,\n              \n                       [[-0.0022, -0.0824,  0.0763],\n                        [-0.0647,  0.0352, -0.0770],\n                        [ 0.0602, -0.0911,  0.0186],\n                        [ 0.0031, -0.0368,  0.0326]],\n              \n                       [[ 0.0509,  0.0771, -0.0238],\n                        [-0.0733,  0.0649, -0.0918],\n                        [-0.0614, -0.0536,  0.0475],\n                        [-0.0273, -0.0003,  0.0094]],\n              \n                       [[ 0.0931, -0.1246, -0.0780],\n                        [ 0.0159, -0.0257, -0.0893],\n                        [ 0.0695,  0.0562, -0.0467],\n                        [ 0.0640, -0.1529,  0.0015]]],\n              \n              \n                      [[[-0.0336, -0.0541,  0.0077],\n                        [-0.1114, -0.0671,  0.0042],\n                        [-0.0734,  0.0446, -0.1157],\n                        [ 0.0446,  0.0368,  0.0329]],\n              \n                       [[-0.0757, -0.1205,  0.0616],\n                        [-0.1136, -0.0482, -0.0582],\n                        [-0.1028,  0.0651, -0.1045],\n                        [ 0.0379, -0.0243,  0.0580]],\n              \n                       [[-0.1267, -0.0782, -0.0613],\n                        [-0.0525, -0.0571,  0.0934],\n                        [ 0.0327,  0.0589,  0.0311],\n                        [-0.0345,  0.0362, -0.0669]],\n              \n                       ...,\n              \n                       [[ 0.0040, -0.0402, -0.0639],\n                        [-0.0022,  0.0773, -0.0950],\n                        [ 0.0935, -0.0089,  0.0631],\n                        [-0.0060, -0.0145, -0.0313]],\n              \n                       [[ 0.0332,  0.0622,  0.0955],\n                        [-0.0993, -0.0975,  0.0053],\n                        [-0.0756,  0.0411,  0.0395],\n                        [ 0.0835,  0.0497,  0.0532]],\n              \n                       [[-0.0945,  0.0780,  0.0214],\n                        [-0.0233,  0.0137,  0.0445],\n                        [-0.0681,  0.0220, -0.0347],\n                        [-0.0678,  0.0608,  0.0911]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.0529,  0.0359,  0.0189],\n                        [ 0.0534, -0.0415, -0.0098],\n                        [-0.0678, -0.0418,  0.0115],\n                        [-0.0158, -0.0468,  0.0544]],\n              \n                       [[ 0.0397, -0.0490,  0.0062],\n                        [-0.0817, -0.0486, -0.0650],\n                        [-0.0850, -0.0725, -0.1222],\n                        [-0.0532, -0.0394, -0.0506]],\n              \n                       [[-0.0781,  0.0585, -0.0825],\n                        [ 0.0415, -0.0304, -0.0431],\n                        [ 0.0017, -0.0236, -0.1006],\n                        [-0.0322, -0.0444,  0.0734]],\n              \n                       ...,\n              \n                       [[ 0.0064, -0.0419, -0.0310],\n                        [-0.0864, -0.0726,  0.0411],\n                        [-0.0404, -0.0147, -0.0525],\n                        [-0.0705,  0.0047, -0.0020]],\n              \n                       [[-0.0733, -0.0186, -0.1150],\n                        [-0.0030,  0.0622, -0.0119],\n                        [-0.1028, -0.0944, -0.0085],\n                        [ 0.0991, -0.0634, -0.0997]],\n              \n                       [[ 0.0011,  0.0111, -0.0595],\n                        [ 0.0074,  0.0204,  0.0430],\n                        [-0.1188,  0.0534, -0.0018],\n                        [ 0.0290, -0.0437, -0.0406]]],\n              \n              \n                      [[[-0.0754, -0.1006,  0.0892],\n                        [ 0.0406, -0.0531,  0.0533],\n                        [ 0.0918,  0.0127, -0.0096],\n                        [-0.1039,  0.0327,  0.0552]],\n              \n                       [[ 0.0684, -0.0138, -0.0291],\n                        [ 0.0967, -0.0006, -0.0845],\n                        [ 0.0808, -0.0622,  0.0624],\n                        [-0.0993, -0.0478, -0.0891]],\n              \n                       [[ 0.0726,  0.0570,  0.0490],\n                        [ 0.0644, -0.1041, -0.0284],\n                        [ 0.0454, -0.0641, -0.0451],\n                        [ 0.0314, -0.0939,  0.0487]],\n              \n                       ...,\n              \n                       [[-0.0134,  0.0072, -0.0227],\n                        [-0.0350,  0.0259, -0.0151],\n                        [ 0.0032, -0.1000, -0.0987],\n                        [-0.1051,  0.0596,  0.0136]],\n              \n                       [[-0.0831,  0.0626, -0.0047],\n                        [ 0.0390, -0.1081,  0.0620],\n                        [-0.0339, -0.0376, -0.0597],\n                        [-0.0450,  0.0391, -0.0286]],\n              \n                       [[-0.0607, -0.0298, -0.0487],\n                        [ 0.0603,  0.0754, -0.0714],\n                        [-0.0303,  0.0415, -0.0978],\n                        [ 0.0555,  0.0434, -0.0066]]],\n              \n              \n                      [[[-0.0092,  0.0481, -0.1045],\n                        [-0.0367,  0.0773, -0.0404],\n                        [-0.0822, -0.0222, -0.0449],\n                        [-0.0379, -0.0188, -0.0852]],\n              \n                       [[ 0.0378, -0.1219, -0.0700],\n                        [ 0.0333, -0.1370, -0.0002],\n                        [ 0.0769,  0.0637, -0.0678],\n                        [-0.0445, -0.1058,  0.0527]],\n              \n                       [[-0.0103,  0.0755, -0.0423],\n                        [-0.0924,  0.0922, -0.0380],\n                        [ 0.0697,  0.0433,  0.0066],\n                        [-0.0059, -0.0961, -0.0500]],\n              \n                       ...,\n              \n                       [[-0.0578,  0.0060, -0.0600],\n                        [ 0.0149,  0.0335, -0.0436],\n                        [ 0.0484, -0.0533, -0.0411],\n                        [-0.0610, -0.1167,  0.0908]],\n              \n                       [[ 0.0171, -0.0501, -0.0758],\n                        [-0.0500, -0.0920, -0.0834],\n                        [ 0.0768,  0.0145, -0.0674],\n                        [-0.0470, -0.0570,  0.0052]],\n              \n                       [[-0.0378, -0.0084, -0.0261],\n                        [ 0.0475, -0.1108, -0.0475],\n                        [ 0.0366, -0.1213,  0.0689],\n                        [-0.0999,  0.0056, -0.0220]]]])),\n             ('conv3.bias',\n              tensor([ 0.0047,  0.0119,  0.0125, -0.0035,  0.0135,  0.0113, -0.0101,  0.0030,\n                      -0.0130,  0.0009, -0.0004, -0.0058, -0.0099,  0.0001, -0.0006, -0.0027,\n                      -0.0031,  0.0102,  0.0112, -0.0127,  0.0139,  0.0059,  0.0133,  0.0035,\n                       0.0180,  0.0127,  0.0019,  0.0054, -0.0034, -0.0053,  0.0046, -0.0126])),\n             ('conv4.weight',\n              tensor([[[[ 0.0357,  0.0316, -0.0341],\n                        [ 0.0707, -0.0305, -0.0081],\n                        [-0.0280, -0.0904,  0.0463],\n                        [-0.0745, -0.0292, -0.0021]],\n              \n                       [[-0.1023,  0.0244,  0.0536],\n                        [ 0.0468,  0.0893,  0.0075],\n                        [ 0.0663, -0.0970, -0.0472],\n                        [ 0.0783, -0.0077, -0.0428]],\n              \n                       [[ 0.0654, -0.0194, -0.0034],\n                        [ 0.0729,  0.0610, -0.0604],\n                        [-0.0124, -0.0223,  0.0520],\n                        [-0.0741,  0.0782,  0.0999]],\n              \n                       ...,\n              \n                       [[ 0.0011,  0.0756,  0.0507],\n                        [-0.0689,  0.0047,  0.0183],\n                        [ 0.0845, -0.0102,  0.0208],\n                        [ 0.0830, -0.0378, -0.0355]],\n              \n                       [[-0.0396,  0.0550, -0.0593],\n                        [-0.0717, -0.0025, -0.0502],\n                        [-0.0627, -0.0371,  0.0016],\n                        [ 0.0500, -0.0107, -0.0786]],\n              \n                       [[-0.0119,  0.0656,  0.0017],\n                        [ 0.0525, -0.0375, -0.0512],\n                        [-0.0893,  0.0291,  0.0339],\n                        [ 0.0745, -0.0184,  0.0786]]],\n              \n              \n                      [[[-0.0458, -0.0752,  0.0868],\n                        [ 0.0762,  0.0643,  0.0604],\n                        [-0.0799, -0.0622, -0.0962],\n                        [ 0.0690,  0.0506, -0.0007]],\n              \n                       [[-0.0461, -0.0679,  0.0090],\n                        [-0.0922,  0.0421, -0.0496],\n                        [ 0.0822, -0.0582,  0.0239],\n                        [ 0.0107, -0.0392,  0.0552]],\n              \n                       [[-0.0461, -0.0525, -0.0148],\n                        [-0.0408, -0.0849,  0.0907],\n                        [ 0.0562,  0.0843, -0.0896],\n                        [-0.0088,  0.0504, -0.0975]],\n              \n                       ...,\n              \n                       [[-0.0405, -0.0542, -0.0821],\n                        [-0.0592, -0.0298,  0.0065],\n                        [-0.0414,  0.0372,  0.0201],\n                        [ 0.0112,  0.0726,  0.0569]],\n              \n                       [[ 0.0077, -0.0138,  0.0050],\n                        [-0.0941, -0.0501, -0.0706],\n                        [-0.0977, -0.0107,  0.0055],\n                        [-0.0459,  0.0795,  0.0378]],\n              \n                       [[-0.0028, -0.0576, -0.0763],\n                        [-0.0760,  0.0098,  0.0266],\n                        [ 0.0318,  0.0621,  0.0315],\n                        [ 0.0296, -0.0517,  0.0358]]],\n              \n              \n                      [[[-0.0357, -0.0237,  0.0488],\n                        [ 0.0029, -0.0498, -0.0323],\n                        [-0.0424, -0.0697, -0.0669],\n                        [-0.0828,  0.0634,  0.0498]],\n              \n                       [[-0.0320, -0.0493, -0.0861],\n                        [-0.0544,  0.0232, -0.0383],\n                        [ 0.0158, -0.0325, -0.0889],\n                        [ 0.0182,  0.0190,  0.0208]],\n              \n                       [[-0.0662, -0.0295,  0.0763],\n                        [ 0.0071, -0.0067,  0.0243],\n                        [ 0.0840,  0.0763,  0.0173],\n                        [ 0.0755, -0.0382,  0.0196]],\n              \n                       ...,\n              \n                       [[-0.0352, -0.0124, -0.0461],\n                        [-0.0255, -0.0238,  0.0573],\n                        [ 0.0247, -0.0076,  0.0142],\n                        [ 0.0347, -0.0335, -0.0893]],\n              \n                       [[ 0.0012, -0.1081,  0.0604],\n                        [ 0.0037,  0.0672, -0.0445],\n                        [-0.0523, -0.0533, -0.0068],\n                        [ 0.0522,  0.0031, -0.0568]],\n              \n                       [[-0.0639,  0.0746,  0.0083],\n                        [ 0.0478, -0.0918, -0.0075],\n                        [ 0.0070,  0.0539, -0.0209],\n                        [-0.0249,  0.0337, -0.0293]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.0244,  0.0395, -0.0472],\n                        [ 0.0157,  0.0895,  0.0467],\n                        [-0.1023,  0.0475, -0.0128],\n                        [-0.0636,  0.0719,  0.0133]],\n              \n                       [[-0.0917, -0.0511,  0.0259],\n                        [-0.0276, -0.0480, -0.0284],\n                        [-0.0902,  0.0943, -0.0327],\n                        [ 0.0482, -0.0009, -0.0700]],\n              \n                       [[ 0.1089, -0.0896,  0.0582],\n                        [ 0.0489,  0.0343, -0.0720],\n                        [-0.0833, -0.0099,  0.0018],\n                        [-0.0902, -0.0754,  0.0178]],\n              \n                       ...,\n              \n                       [[ 0.0731,  0.0263, -0.0231],\n                        [ 0.0101,  0.0293, -0.0039],\n                        [ 0.0639,  0.0853, -0.0518],\n                        [-0.0525, -0.0518, -0.0846]],\n              \n                       [[ 0.0091,  0.0486, -0.0135],\n                        [-0.0700, -0.0170,  0.0633],\n                        [-0.1457,  0.0080,  0.0037],\n                        [-0.0379, -0.0288,  0.0701]],\n              \n                       [[ 0.1058, -0.0204, -0.0729],\n                        [-0.0430,  0.0089, -0.0414],\n                        [-0.0305,  0.0146, -0.1287],\n                        [ 0.1090, -0.0362, -0.0532]]],\n              \n              \n                      [[[-0.0079,  0.0332,  0.0879],\n                        [ 0.0080,  0.0500, -0.0463],\n                        [ 0.0025, -0.0292, -0.0754],\n                        [-0.0757,  0.0028, -0.0267]],\n              \n                       [[ 0.0366, -0.0306,  0.0508],\n                        [-0.0419,  0.0809, -0.0978],\n                        [-0.0683,  0.0141, -0.0781],\n                        [-0.0560,  0.0026, -0.0152]],\n              \n                       [[-0.0421,  0.0211,  0.0445],\n                        [ 0.0634,  0.1110, -0.0451],\n                        [ 0.0470, -0.0612,  0.1031],\n                        [-0.0242, -0.0132,  0.0403]],\n              \n                       ...,\n              \n                       [[-0.0050, -0.0587, -0.0345],\n                        [ 0.0731, -0.0651, -0.0791],\n                        [ 0.0132,  0.0338,  0.0233],\n                        [ 0.0739, -0.0145,  0.0206]],\n              \n                       [[ 0.0254,  0.0055, -0.0351],\n                        [ 0.0130,  0.0132,  0.0213],\n                        [ 0.0122,  0.0309, -0.0220],\n                        [ 0.0143,  0.0311,  0.0414]],\n              \n                       [[-0.0889, -0.0190,  0.0721],\n                        [-0.0534,  0.0919, -0.0222],\n                        [-0.1250,  0.0367, -0.0644],\n                        [-0.1324, -0.0102,  0.0405]]],\n              \n              \n                      [[[ 0.0763, -0.0214,  0.0534],\n                        [-0.0616,  0.0251, -0.0121],\n                        [-0.0436, -0.0438,  0.0609],\n                        [ 0.0305, -0.0437,  0.0539]],\n              \n                       [[ 0.0545,  0.0455,  0.0679],\n                        [ 0.0373,  0.0026,  0.0067],\n                        [-0.0846,  0.0202, -0.0069],\n                        [ 0.0115, -0.0216, -0.0385]],\n              \n                       [[-0.0659, -0.0312,  0.1070],\n                        [ 0.0305, -0.0664,  0.0060],\n                        [-0.0309, -0.0988, -0.0398],\n                        [-0.0703, -0.0727,  0.0160]],\n              \n                       ...,\n              \n                       [[-0.0494,  0.0041,  0.1160],\n                        [ 0.0151, -0.0735, -0.0107],\n                        [-0.0333,  0.0644, -0.0054],\n                        [ 0.0222, -0.0665,  0.0590]],\n              \n                       [[ 0.0478, -0.0086,  0.0485],\n                        [-0.0382, -0.1017, -0.0058],\n                        [-0.0424,  0.0484, -0.0544],\n                        [ 0.0566, -0.0549, -0.0792]],\n              \n                       [[ 0.0313,  0.1039, -0.0748],\n                        [ 0.0411,  0.1074, -0.1273],\n                        [-0.0105, -0.0629, -0.0535],\n                        [ 0.0335, -0.0074, -0.0284]]]])),\n             ('conv4.bias',\n              tensor([ 0.0255,  0.0165, -0.0134,  0.0274, -0.0130,  0.0231,  0.0281,  0.0183,\n                      -0.0093,  0.0096,  0.0258,  0.0311, -0.0108,  0.0388,  0.0259,  0.0118,\n                      -0.0163,  0.0286, -0.0218, -0.0019,  0.0289,  0.0095,  0.0362,  0.0240,\n                       0.0403,  0.0352, -0.0027,  0.0049,  0.0213,  0.0242,  0.0130,  0.0243])),\n             ('fc.weight',\n              tensor([[ 0.0378, -0.0361,  0.0101,  ...,  0.0723, -0.0334,  0.0127],\n                      [ 0.0241, -0.0479,  0.0635,  ..., -0.0335,  0.0308, -0.0078],\n                      [ 0.0486, -0.0051, -0.0018,  ...,  0.0095,  0.0710,  0.0094],\n                      [-0.0301, -0.0349,  0.0094,  ..., -0.0699, -0.0441,  0.0760],\n                      [-0.0179,  0.0549,  0.0528,  ..., -0.0344,  0.0351, -0.0224],\n                      [ 0.0074,  0.0691,  0.0105,  ...,  0.0150,  0.0806,  0.0425]])),\n             ('fc.bias',\n              tensor([0.0380, 0.0419, 0.0425, 0.0358, 0.0127, 0.0341]))])"
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "5906a435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "\nHistory (client, train losses):\n\tmidstream: {1: 0.00011136518302693183, 2: 9.300731635975865e-05, 3: 7.146666680822972e-05, 4: 7.697379786022156e-05, 5: 9.065095554700146e-05}\n\tupstream: {1: 0.00011085080988589973, 2: 6.157865337341163e-05, 3: 5.460808473439775e-05, 4: 4.988717607802504e-05, 5: 4.173709135776142e-05}\n\tdownstream: {1: 0.0001315119847326985, 2: 0.00010988663469730169, 3: 7.605883262015336e-05, 4: 7.17156449611803e-05, 5: 6.438381042401674e-05}\n\nHistory (client, train metrics):\n\tmidstream: {1: {'MSE': 0.014151853, 'RMSE': 0.11896155924467672, 'MAE': 0.0936552, 'R^2': -0.09903742291344485, 'NRMSE': 0.9849771269000459}, 2: {'MSE': 0.011831862, 'RMSE': 0.10877436242357283, 'MAE': 0.07801473, 'R^2': 0.17629388375472513, 'NRMSE': 0.8081816479167916}, 3: {'MSE': 0.009109822, 'RMSE': 0.09544538805982739, 'MAE': 0.06751217, 'R^2': 0.4200972250579051, 'NRMSE': 0.7008272683008272}, 4: {'MSE': 0.009808536, 'RMSE': 0.09903805171561791, 'MAE': 0.06858187, 'R^2': 0.3307158447738638, 'NRMSE': 0.6990156408304301}, 5: {'MSE': 0.0115589565, 'RMSE': 0.10751258747986303, 'MAE': 0.07580292, 'R^2': 0.26171875958861496, 'NRMSE': 0.7041465529352043}}\n\tupstream: {1: {'MSE': 0.014061563, 'RMSE': 0.11858146025817402, 'MAE': 0.086664476, 'R^2': -0.3059621934588639, 'NRMSE': 1.0718621395091141}, 2: {'MSE': 0.007849209, 'RMSE': 0.08859576180947396, 'MAE': 0.062979825, 'R^2': -0.17388291869288028, 'NRMSE': 0.9893014167366349}, 3: {'MSE': 0.006964116, 'RMSE': 0.08345127854985576, 'MAE': 0.05863503, 'R^2': 0.044536464413937805, 'NRMSE': 0.9183226243053859}, 4: {'MSE': 0.006331474, 'RMSE': 0.07957056022553045, 'MAE': 0.058148667, 'R^2': -0.24208491438892735, 'NRMSE': 0.7687040487640601}, 5: {'MSE': 0.005309341, 'RMSE': 0.07286522496847353, 'MAE': 0.0489819, 'R^2': 0.3753893229195331, 'NRMSE': 0.7217694167103069}}\n\tdownstream: {1: {'MSE': 0.01673804, 'RMSE': 0.12937557873389127, 'MAE': 0.08778849, 'R^2': -0.03698708625751592, 'NRMSE': 1.1166527828139305}, 2: {'MSE': 0.0140117295, 'RMSE': 0.11837115150507907, 'MAE': 0.084103785, 'R^2': -0.004591917145265374, 'NRMSE': 1.0041282682767665}, 3: {'MSE': 0.009709249, 'RMSE': 0.0985355227853935, 'MAE': 0.06693856, 'R^2': 0.29665238331820487, 'NRMSE': 0.9521134435062043}, 4: {'MSE': 0.009130321, 'RMSE': 0.09555271551913583, 'MAE': 0.06328633, 'R^2': 0.34907033017551603, 'NRMSE': 0.9883633034913363}, 5: {'MSE': 0.0082069365, 'RMSE': 0.09059214386892978, 'MAE': 0.05858543, 'R^2': 0.41107339534027504, 'NRMSE': 0.911184840679664}}\n\nHistory (client, test losses):\n\tmidstream: {1: 0.0001560016182159669, 2: 0.00010777601579082062, 3: 6.533208585603804e-05, 4: 5.993714411055944e-05, 5: 6.337892946397855e-05}\n\tupstream: {1: 0.00012047470231654738, 2: 6.873955322805165e-05, 3: 6.0913455043290115e-05, 4: 4.6732939301511726e-05, 5: 5.2376439709704284e-05}\n\tdownstream: {1: 7.496257236101177e-05, 2: 5.5007447222787005e-05, 3: 5.1095616072416306e-05, 4: 3.933274398464136e-05, 5: 3.465448957495009e-05}\n\nHistory (client, test metrics):\n\tmidstream: {1: {'MSE': 0.019047974, 'RMSE': 0.1380143966313514, 'MAE': 0.10544771, 'R^2': -0.7954420998488887, 'NRMSE': 0.7133939938176809}, 2: {'MSE': 0.012752357, 'RMSE': 0.11292633412547214, 'MAE': 0.07842506, 'R^2': 0.02822224884435337, 'NRMSE': 0.6169411703956907}, 3: {'MSE': 0.00750386, 'RMSE': 0.08662482418125718, 'MAE': 0.05991578, 'R^2': 0.01076101167048408, 'NRMSE': 0.6230024714978473}, 4: {'MSE': 0.006653846, 'RMSE': 0.0815711087081233, 'MAE': 0.05612293, 'R^2': 0.14705771069409257, 'NRMSE': 0.5737296720898506}, 5: {'MSE': 0.0068906336, 'RMSE': 0.08300984043561269, 'MAE': 0.05733691, 'R^2': 0.1876405048209923, 'NRMSE': 0.5921176001091947}}\n\tupstream: {1: {'MSE': 0.014492985, 'RMSE': 0.12038681348526292, 'MAE': 0.08217926, 'R^2': -0.5641343557098814, 'NRMSE': 0.9315812150728621}, 2: {'MSE': 0.008219565, 'RMSE': 0.09066181812031268, 'MAE': 0.057329785, 'R^2': -0.09676783187173744, 'NRMSE': 0.7883915111282768}, 3: {'MSE': 0.0071924184, 'RMSE': 0.08480812723378406, 'MAE': 0.057928875, 'R^2': -0.4442875478649799, 'NRMSE': 1.1271111024743254}, 4: {'MSE': 0.005552262, 'RMSE': 0.07451350258708014, 'MAE': 0.049447194, 'R^2': -0.851981059326469, 'NRMSE': 0.6673133585631397}, 5: {'MSE': 0.006205451, 'RMSE': 0.0787746863471896, 'MAE': 0.05061212, 'R^2': 0.028556200261360815, 'NRMSE': 0.7713378987805366}}\n\tdownstream: {1: {'MSE': 0.009058434, 'RMSE': 0.09517580356828571, 'MAE': 0.06981634, 'R^2': -0.45894123363147116, 'NRMSE': 1.417860852136232}, 2: {'MSE': 0.0062651304, 'RMSE': 0.07915257645635712, 'MAE': 0.055121813, 'R^2': -0.20575864876042407, 'NRMSE': 1.2870656251270531}, 3: {'MSE': 0.006174984, 'RMSE': 0.07858106592807283, 'MAE': 0.0629574, 'R^2': -0.7143771383325287, 'NRMSE': 1.7907832400108454}, 4: {'MSE': 0.004679221, 'RMSE': 0.06840483312045627, 'MAE': 0.04851551, 'R^2': -0.24534118557507925, 'NRMSE': 1.0889738734159542}, 5: {'MSE': 0.004068945, 'RMSE': 0.06378828316286632, 'MAE': 0.04432987, 'R^2': -0.20468907962319935, 'NRMSE': 1.1443857240557402}}\n\nHistory (global averaged train losses):\n\t[0.0015395720352174964, 0.00024854410795093385, 0.00020803995674782054, 0.00013188586029582713, 0.00011886729213211852, 0.0001008075944899741]\n\nHistory (global averaged train metrics):\n\tMSE: [0.1951583971241428, 0.03160091131651445, 0.026464936604026726, 0.016776145154751627, 0.015133595905789356, 0.012835677239363928]\n\tRMSE: [0.44164381234569394, 0.1749017700774407, 0.1615861591627101, 0.12822136921923394, 0.11985834064790316, 0.11187103845367403]\n\tMAE: [0.3565680417760898, 0.134384741237075, 0.12342083132591394, 0.09636203438353269, 0.08744273744996287, 0.08239615967180099]\n\tR^2: [-13.088946974627678, -1.7214835391164596, -1.142707789797404, -0.5300679730275359, -0.38371557656249816, -0.1662141672964125]\n\tNRMSE: [1.43463811483058, 1.791670208613301, 1.2856305138789406, 1.311231253400706, 1.2479581808239997, 1.0922919543928582]\n\nHistory (global averaged test losses):\n\t[0.0014973638405266043, 0.00027003905775567906, 0.00021020635088731709, 0.00014013554683405455, 0.00012064255565497001, 0.00010451346307680789]\n\nHistory (global averaged test metrics):\n\tMSE: [0.17811404205449335, 0.032643797589916285, 0.02497020322998351, 0.016797536319495272, 0.014436998735258744, 0.012460191365699846]\n\tRMSE: [0.4218117810879179, 0.17695383083372748, 0.15659287448683404, 0.1286846705835335, 0.117484587207318, 0.11028009244606854]\n\tMAE: [0.34660633688198994, 0.13437158301738755, 0.11748843240439073, 0.09645118943036912, 0.08684661665644783, 0.08331191628802936]\n\tR^2: [-20.280578298586548, -6.768707278584008, -4.327252349937191, -4.234415426842329, -2.1657150784919557, -2.2273960942172475]\n\tNRMSE: [1.3645320704195445, 3.287192227806257, 2.4584361948774625, 2.60002033719065, 2.013148862950743, 1.8625707402395537]"
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f682fd52-1565-4823-80bf-5529f13d58f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "040bba0d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "a39106e1a9d6d153b7400628e7589ff266b5caee5b0db427f0903be982155882"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
