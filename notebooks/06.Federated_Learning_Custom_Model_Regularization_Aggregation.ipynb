{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "517dd5ed",
   "metadata": {},
   "source": [
    "### This example is almost equivalent to 05.Federated_Training_Inference. The ony difference is that in this notebook, we define our custom model, apply regularization during training and use a different federated aggregation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6d8cf9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6941fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "parent = Path(os.path.abspath(\"\")).resolve().parents[0]\n",
    "if parent not in sys.path:\n",
    "    sys.path.insert(0, str(parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "efb06673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import random\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a766f200",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.utils.data_utils import read_data, generate_time_lags, time_to_feature, handle_nans, to_Xy, \\\n",
    "    to_torch_dataset, to_timeseries_rep, assign_statistics, \\\n",
    "    to_train_val, scale_features, get_data_by_area, remove_identifiers, get_exogenous_data_by_area, handle_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66483b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.utils.train_utils import train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a440899",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.fl.defaults import create_regression_client\n",
    "from ml.fl.client_proxy import SimpleClientProxy\n",
    "from ml.fl.server.server import Server\n",
    "from ml.utils.helpers import accumulate_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "402ccb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    data_path='../dataset/full_dataset.csv', # dataset\n",
    "\n",
    "    test_size=0.2, # validation size \n",
    "    targets=['temp','pH','DissolvedOxygen','Conductivity','Turbidity','AmmoniaNitrogen'], # the target columns    num_lags=10, # the number of past observations to feed as input\n",
    "    num_lags=10, # the number of past observations to feed as input\n",
    "\n",
    "    identifier='District', # the column name that identifies a bs\n",
    "\n",
    "    nan_constant=0, # the constant to transform nan values\n",
    "    x_scaler='minmax', # x_scaler\n",
    "    y_scaler='minmax', # y_scaler\n",
    "    outlier_detection=True, # whether to perform flooring and capping\n",
    "\n",
    "    criterion='mse', # optimization criterion, mse or l1\n",
    "    fl_rounds=30, # the number of federated rounds\n",
    "    fraction=1., # the percentage of available client to consider for random selection\n",
    "    aggregation=\"fednova\", # federated aggregation algorithm\n",
    "    epochs=3, # the number of maximum local epochs\n",
    "    lr=0.001, # learning rate\n",
    "    optimizer='adam', # the optimizer, it can be sgd or adam\n",
    "    batch_size=128, # the batch size to use\n",
    "    local_early_stopping=False, # whether to use early stopping\n",
    "    local_patience=50, # patience value for the early stopping parameter (if specified)\n",
    "    \n",
    "    max_grad_norm=1.0, # whether to clip grad norm\n",
    "    reg1=1e-8, # l1 regularization\n",
    "    reg2=1e-6, # l2 regularization\n",
    "\n",
    "    cuda=True, # whether to use gpu\n",
    "    \n",
    "    seed=0, # reproducibility\n",
    "\n",
    "    assign_stats=None, # whether to use statistics as exogenous data, [\"mean\", \"median\", \"std\", \"variance\", \"kurtosis\", \"skew\"]\n",
    "    use_time_features=False # whether to use datetime features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "16dfd4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script arguments: Namespace(aggregation='fednova', assign_stats=None, batch_size=128, criterion='mse', cuda=True, data_path='../dataset/full_dataset.csv', epochs=3, fl_rounds=30, fraction=1.0, identifier='District', local_early_stopping=False, local_patience=50, lr=0.001, max_grad_norm=1.0, nan_constant=0, num_lags=10, optimizer='adam', outlier_detection=True, reg1=1e-08, reg2=1e-06, seed=0, targets=['temp', 'pH', 'DissolvedOxygen', 'Conductivity', 'Turbidity', 'AmmoniaNitrogen'], test_size=0.2, use_time_features=False, x_scaler='minmax', y_scaler='minmax')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Script arguments: {args}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e719537f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if args.cuda and torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8973eff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection specification\n",
    "if args.outlier_detection is not None:\n",
    "    outlier_columns = ['Conductivity', 'Turbidity', 'pH', 'DissolvedOxygen']\n",
    "    outlier_kwargs = {\"upstream\": (10, 90), \"midstream\": (10, 90), \"downstream\": (5, 95)}\n",
    "    args.outlier_columns = outlier_columns\n",
    "    args.outlier_kwargs = outlier_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "55014302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_all():\n",
    "    # ensure reproducibility\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6476b416",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0a7ed9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preprocessing():\n",
    "    \"\"\"Preprocess a given .csv\"\"\"\n",
    "    # read data\n",
    "    df = read_data(args.data_path)\n",
    "    # handle nans\n",
    "    df = handle_nans(train_data=df, constant=args.nan_constant,\n",
    "                     identifier=args.identifier)\n",
    "    # split to train/validation\n",
    "    train_data, val_data = to_train_val(df)\n",
    "    \n",
    "    # handle outliers (if specified)\n",
    "    if args.outlier_detection is not None:\n",
    "        train_data = handle_outliers(df=train_data, columns=args.outlier_columns,\n",
    "                                     identifier=args.identifier, kwargs=args.outlier_kwargs)\n",
    "    \n",
    "    # get X and y\n",
    "    X_train, X_val, y_train, y_val = to_Xy(train_data=train_data, val_data=val_data,\n",
    "                                          targets=args.targets)\n",
    "    \n",
    "    # scale X\n",
    "    X_train, X_val, x_scalers = scale_features(train_data=X_train, val_data=X_val,\n",
    "                                              scaler=args.x_scaler,\n",
    "                                              per_area=True, # the features are scaled locally\n",
    "                                              identifier=args.identifier)\n",
    "    # scale y\n",
    "    y_train, y_val, y_scalers = scale_features(train_data=y_train, val_data=y_val,\n",
    "                                              scaler=args.y_scaler, \n",
    "                                              per_area=True,\n",
    "                                              identifier=args.identifier)\n",
    "    \n",
    "    # generate time lags\n",
    "    X_train = generate_time_lags(X_train, args.num_lags)\n",
    "    X_val = generate_time_lags(X_val, args.num_lags)\n",
    "    y_train = generate_time_lags(y_train, args.num_lags, is_y=True)\n",
    "    y_val = generate_time_lags(y_val, args.num_lags, is_y=True)\n",
    "    \n",
    "    # get datetime features as exogenous data\n",
    "    date_time_df_train = time_to_feature(\n",
    "        X_train, args.use_time_features, identifier=args.identifier\n",
    "    )\n",
    "    date_time_df_val = time_to_feature(\n",
    "        X_val, args.use_time_features, identifier=args.identifier\n",
    "    )\n",
    "    \n",
    "    # get statistics as exogenous data\n",
    "    stats_df_train = assign_statistics(X_train, args.assign_stats, args.num_lags,\n",
    "                                       targets=args.targets, identifier=args.identifier)\n",
    "    stats_df_val = assign_statistics(X_val, args.assign_stats, args.num_lags, \n",
    "                                       targets=args.targets, identifier=args.identifier)\n",
    "    \n",
    "    # concat the exogenous features (if any) to a single dataframe\n",
    "    if date_time_df_train is not None or stats_df_train is not None:\n",
    "        exogenous_data_train = pd.concat([date_time_df_train, stats_df_train], axis=1)\n",
    "        # remove duplicate columns (if any)\n",
    "        exogenous_data_train = exogenous_data_train.loc[:, ~exogenous_data_train.columns.duplicated()].copy()\n",
    "        assert len(exogenous_data_train) == len(X_train) == len(y_train)\n",
    "    else:\n",
    "        exogenous_data_train = None\n",
    "    if date_time_df_val is not None or stats_df_val is not None:\n",
    "        exogenous_data_val = pd.concat([date_time_df_val, stats_df_val], axis=1)\n",
    "        exogenous_data_val = exogenous_data_val.loc[:, ~exogenous_data_val.columns.duplicated()].copy()\n",
    "        assert len(exogenous_data_val) == len(X_val) == len(y_val)\n",
    "    else:\n",
    "        exogenous_data_val = None\n",
    "        \n",
    "    return X_train, X_val, y_train, y_val, exogenous_data_train, exogenous_data_val, x_scalers, y_scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fb403e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO logger 2024-06-03 09:50:46,446 | data_utils.py:383 | Observations info in upstream\n",
      "INFO logger 2024-06-03 09:50:46,447 | data_utils.py:384 | \tTotal number of samples:  4863\n",
      "INFO logger 2024-06-03 09:50:46,447 | data_utils.py:385 | \tNumber of samples for training: 3891\n",
      "INFO logger 2024-06-03 09:50:46,447 | data_utils.py:386 | \tNumber of samples for validation:  972\n",
      "INFO logger 2024-06-03 09:50:46,450 | data_utils.py:383 | Observations info in midstream\n",
      "INFO logger 2024-06-03 09:50:46,450 | data_utils.py:384 | \tTotal number of samples:  4930\n",
      "INFO logger 2024-06-03 09:50:46,451 | data_utils.py:385 | \tNumber of samples for training: 3944\n",
      "INFO logger 2024-06-03 09:50:46,451 | data_utils.py:386 | \tNumber of samples for validation:  986\n",
      "INFO logger 2024-06-03 09:50:46,453 | data_utils.py:383 | Observations info in downstream\n",
      "INFO logger 2024-06-03 09:50:46,453 | data_utils.py:384 | \tTotal number of samples:  4920\n",
      "INFO logger 2024-06-03 09:50:46,453 | data_utils.py:385 | \tNumber of samples for training: 3936\n",
      "INFO logger 2024-06-03 09:50:46,454 | data_utils.py:386 | \tNumber of samples for validation:  984\n",
      "INFO logger 2024-06-03 09:50:46,456 | data_utils.py:389 | Observations info using all data\n",
      "INFO logger 2024-06-03 09:50:46,456 | data_utils.py:390 | \tTotal number of samples:  14713\n",
      "INFO logger 2024-06-03 09:50:46,456 | data_utils.py:391 | \tNumber of samples for training: 11771\n",
      "INFO logger 2024-06-03 09:50:46,457 | data_utils.py:392 | \tNumber of samples for validation:  2942\n",
      "INFO logger 2024-06-03 09:50:46,457 | data_utils.py:118 | Using Flooring and Capping and with params: {'upstream': (10, 90), 'midstream': (10, 90), 'downstream': (5, 95)}\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val, exogenous_data_train, exogenous_data_val, x_scalers, y_scalers = make_preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "40cd61b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                     TotalNitrogen_lag-10  TotalPhosphorus_lag-10  \\\ntime                                                                \n2020-11-10 16:00:00              0.053548                0.487918   \n2020-11-10 20:00:00              0.053548                0.487918   \n2020-11-11 00:00:00              0.052996                0.489781   \n2020-11-11 04:00:00              0.054376                0.489781   \n2020-11-11 08:00:00              0.055756                0.488850   \n\n                     AmmoniaNitrogen_lag-10  PermanganateIndex_lag-10  \\\ntime                                                                    \n2020-11-10 16:00:00                0.454747                  0.202132   \n2020-11-10 20:00:00                0.454747                  0.202132   \n2020-11-11 00:00:00                0.468944                  0.179582   \n2020-11-11 04:00:00                0.468944                  0.188602   \n2020-11-11 08:00:00                0.458740                  0.192292   \n\n                     Turbidity_lag-10  Conductivity_lag-10  \\\ntime                                                         \n2020-11-10 16:00:00          0.004721             0.104821   \n2020-11-10 20:00:00          0.099150             0.103444   \n2020-11-11 00:00:00          0.099150             0.103444   \n2020-11-11 04:00:00          0.042493             0.117218   \n2020-11-11 08:00:00          0.042493             0.117218   \n\n                     DissolvedOxygen_lag-10  pH_lag-10  temp_lag-10  \\\ntime                                                                  \n2020-11-10 16:00:00                0.206544   0.117647     0.671883   \n2020-11-10 20:00:00                0.063395   0.058823     0.663788   \n2020-11-11 00:00:00                0.059305   0.058823     0.658392   \n2020-11-11 04:00:00                0.241309   0.155462     0.677280   \n2020-11-11 08:00:00                0.337423   0.155462     0.685375   \n\n                     TotalNitrogen_lag-9  ...  TotalNitrogen_lag-1  \\\ntime                                      ...                        \n2020-11-10 16:00:00             0.053548  ...             0.054376   \n2020-11-10 20:00:00             0.052996  ...             0.052996   \n2020-11-11 00:00:00             0.054376  ...             0.057137   \n2020-11-11 04:00:00             0.055756  ...             0.052720   \n2020-11-11 08:00:00             0.055480  ...             0.052720   \n\n                     TotalPhosphorus_lag-1  AmmoniaNitrogen_lag-1  \\\ntime                                                                \n2020-11-10 16:00:00               0.490246               0.469831   \n2020-11-10 20:00:00               0.490246               0.463620   \n2020-11-11 00:00:00               0.492574               0.459184   \n2020-11-11 04:00:00               0.488850               0.458740   \n2020-11-11 08:00:00               0.488850               0.458740   \n\n                     PermanganateIndex_lag-1  Turbidity_lag-1  \\\ntime                                                            \n2020-11-10 16:00:00                 0.184912         0.240793   \n2020-11-10 20:00:00                 0.191472         0.080264   \n2020-11-11 00:00:00                 0.181222         0.023607   \n2020-11-11 04:00:00                 0.187782         0.075543   \n2020-11-11 08:00:00                 0.187782         0.108593   \n\n                     Conductivity_lag-1  DissolvedOxygen_lag-1  pH_lag-1  \\\ntime                                                                       \n2020-11-10 16:00:00            0.119973               0.214724  0.113445   \n2020-11-10 20:00:00            0.146143               0.355828  0.180672   \n2020-11-11 00:00:00            0.136501               0.249489  0.121849   \n2020-11-11 04:00:00            0.124105               0.167689  0.079832   \n2020-11-11 08:00:00            0.119973               0.122699  0.067227   \n\n                     temp_lag-1  District  \ntime                                       \n2020-11-10 16:00:00    0.661090  upstream  \n2020-11-10 20:00:00    0.679978  upstream  \n2020-11-11 00:00:00    0.666487  upstream  \n2020-11-11 04:00:00    0.658392  upstream  \n2020-11-11 08:00:00    0.655693  upstream  \n\n[5 rows x 91 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TotalNitrogen_lag-10</th>\n      <th>TotalPhosphorus_lag-10</th>\n      <th>AmmoniaNitrogen_lag-10</th>\n      <th>PermanganateIndex_lag-10</th>\n      <th>Turbidity_lag-10</th>\n      <th>Conductivity_lag-10</th>\n      <th>DissolvedOxygen_lag-10</th>\n      <th>pH_lag-10</th>\n      <th>temp_lag-10</th>\n      <th>TotalNitrogen_lag-9</th>\n      <th>...</th>\n      <th>TotalNitrogen_lag-1</th>\n      <th>TotalPhosphorus_lag-1</th>\n      <th>AmmoniaNitrogen_lag-1</th>\n      <th>PermanganateIndex_lag-1</th>\n      <th>Turbidity_lag-1</th>\n      <th>Conductivity_lag-1</th>\n      <th>DissolvedOxygen_lag-1</th>\n      <th>pH_lag-1</th>\n      <th>temp_lag-1</th>\n      <th>District</th>\n    </tr>\n    <tr>\n      <th>time</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2020-11-10 16:00:00</th>\n      <td>0.053548</td>\n      <td>0.487918</td>\n      <td>0.454747</td>\n      <td>0.202132</td>\n      <td>0.004721</td>\n      <td>0.104821</td>\n      <td>0.206544</td>\n      <td>0.117647</td>\n      <td>0.671883</td>\n      <td>0.053548</td>\n      <td>...</td>\n      <td>0.054376</td>\n      <td>0.490246</td>\n      <td>0.469831</td>\n      <td>0.184912</td>\n      <td>0.240793</td>\n      <td>0.119973</td>\n      <td>0.214724</td>\n      <td>0.113445</td>\n      <td>0.661090</td>\n      <td>upstream</td>\n    </tr>\n    <tr>\n      <th>2020-11-10 20:00:00</th>\n      <td>0.053548</td>\n      <td>0.487918</td>\n      <td>0.454747</td>\n      <td>0.202132</td>\n      <td>0.099150</td>\n      <td>0.103444</td>\n      <td>0.063395</td>\n      <td>0.058823</td>\n      <td>0.663788</td>\n      <td>0.052996</td>\n      <td>...</td>\n      <td>0.052996</td>\n      <td>0.490246</td>\n      <td>0.463620</td>\n      <td>0.191472</td>\n      <td>0.080264</td>\n      <td>0.146143</td>\n      <td>0.355828</td>\n      <td>0.180672</td>\n      <td>0.679978</td>\n      <td>upstream</td>\n    </tr>\n    <tr>\n      <th>2020-11-11 00:00:00</th>\n      <td>0.052996</td>\n      <td>0.489781</td>\n      <td>0.468944</td>\n      <td>0.179582</td>\n      <td>0.099150</td>\n      <td>0.103444</td>\n      <td>0.059305</td>\n      <td>0.058823</td>\n      <td>0.658392</td>\n      <td>0.054376</td>\n      <td>...</td>\n      <td>0.057137</td>\n      <td>0.492574</td>\n      <td>0.459184</td>\n      <td>0.181222</td>\n      <td>0.023607</td>\n      <td>0.136501</td>\n      <td>0.249489</td>\n      <td>0.121849</td>\n      <td>0.666487</td>\n      <td>upstream</td>\n    </tr>\n    <tr>\n      <th>2020-11-11 04:00:00</th>\n      <td>0.054376</td>\n      <td>0.489781</td>\n      <td>0.468944</td>\n      <td>0.188602</td>\n      <td>0.042493</td>\n      <td>0.117218</td>\n      <td>0.241309</td>\n      <td>0.155462</td>\n      <td>0.677280</td>\n      <td>0.055756</td>\n      <td>...</td>\n      <td>0.052720</td>\n      <td>0.488850</td>\n      <td>0.458740</td>\n      <td>0.187782</td>\n      <td>0.075543</td>\n      <td>0.124105</td>\n      <td>0.167689</td>\n      <td>0.079832</td>\n      <td>0.658392</td>\n      <td>upstream</td>\n    </tr>\n    <tr>\n      <th>2020-11-11 08:00:00</th>\n      <td>0.055756</td>\n      <td>0.488850</td>\n      <td>0.458740</td>\n      <td>0.192292</td>\n      <td>0.042493</td>\n      <td>0.117218</td>\n      <td>0.337423</td>\n      <td>0.155462</td>\n      <td>0.685375</td>\n      <td>0.055480</td>\n      <td>...</td>\n      <td>0.052720</td>\n      <td>0.488850</td>\n      <td>0.458740</td>\n      <td>0.187782</td>\n      <td>0.108593</td>\n      <td>0.119973</td>\n      <td>0.122699</td>\n      <td>0.067227</td>\n      <td>0.655693</td>\n      <td>upstream</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 91 columns</p>\n</div>"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7c9073de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                         temp        pH  DissolvedOxygen  Conductivity  \\\ntime                                                                     \n2020-11-10 16:00:00  0.679978  0.180672         0.355828      0.146143   \n2020-11-10 20:00:00  0.666487  0.121849         0.249489      0.136501   \n2020-11-11 00:00:00  0.658392  0.079832         0.167689      0.124105   \n2020-11-11 04:00:00  0.655693  0.067227         0.122699      0.119973   \n2020-11-11 08:00:00  0.652995  0.063025         0.110429      0.124105   \n\n                     Turbidity  AmmoniaNitrogen  District  \ntime                                                       \n2020-11-10 16:00:00   0.080264         0.463620  upstream  \n2020-11-10 20:00:00   0.023607         0.459184  upstream  \n2020-11-11 00:00:00   0.075543         0.458740  upstream  \n2020-11-11 04:00:00   0.108593         0.458740  upstream  \n2020-11-11 08:00:00   0.203022         0.462733  upstream  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>temp</th>\n      <th>pH</th>\n      <th>DissolvedOxygen</th>\n      <th>Conductivity</th>\n      <th>Turbidity</th>\n      <th>AmmoniaNitrogen</th>\n      <th>District</th>\n    </tr>\n    <tr>\n      <th>time</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2020-11-10 16:00:00</th>\n      <td>0.679978</td>\n      <td>0.180672</td>\n      <td>0.355828</td>\n      <td>0.146143</td>\n      <td>0.080264</td>\n      <td>0.463620</td>\n      <td>upstream</td>\n    </tr>\n    <tr>\n      <th>2020-11-10 20:00:00</th>\n      <td>0.666487</td>\n      <td>0.121849</td>\n      <td>0.249489</td>\n      <td>0.136501</td>\n      <td>0.023607</td>\n      <td>0.459184</td>\n      <td>upstream</td>\n    </tr>\n    <tr>\n      <th>2020-11-11 00:00:00</th>\n      <td>0.658392</td>\n      <td>0.079832</td>\n      <td>0.167689</td>\n      <td>0.124105</td>\n      <td>0.075543</td>\n      <td>0.458740</td>\n      <td>upstream</td>\n    </tr>\n    <tr>\n      <th>2020-11-11 04:00:00</th>\n      <td>0.655693</td>\n      <td>0.067227</td>\n      <td>0.122699</td>\n      <td>0.119973</td>\n      <td>0.108593</td>\n      <td>0.458740</td>\n      <td>upstream</td>\n    </tr>\n    <tr>\n      <th>2020-11-11 08:00:00</th>\n      <td>0.652995</td>\n      <td>0.063025</td>\n      <td>0.110429</td>\n      <td>0.124105</td>\n      <td>0.203022</td>\n      <td>0.462733</td>\n      <td>upstream</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "814ccd94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "({'upstream': MinMaxScaler(),\n  'midstream': MinMaxScaler(),\n  'downstream': MinMaxScaler()},\n {'upstream': MinMaxScaler(),\n  'midstream': MinMaxScaler(),\n  'downstream': MinMaxScaler()})"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scalers, y_scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4c4f4cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_postprocessing(X_train, X_val, y_train, y_val, exogenous_data_train, exogenous_data_val, x_scalers, y_scalers):\n",
    "    \"\"\"Make data ready to be fed into ml algorithms\"\"\"\n",
    "    # if there are more than one specified areas, get the data per area\n",
    "    if X_train[args.identifier].nunique() != 1:\n",
    "        area_X_train, area_X_val, area_y_train, area_y_val = get_data_by_area(X_train, X_val,\n",
    "                                                                              y_train, y_val, \n",
    "                                                                              identifier=args.identifier)\n",
    "    else:\n",
    "        area_X_train, area_X_val, area_y_train, area_y_val = None, None, None, None\n",
    "\n",
    "    # Get the exogenous data per area.\n",
    "    if exogenous_data_train is not None:\n",
    "        exogenous_data_train, exogenous_data_val = get_exogenous_data_by_area(exogenous_data_train,\n",
    "                                                                              exogenous_data_val)\n",
    "    # transform to np\n",
    "    if area_X_train is not None:\n",
    "        for area in area_X_train:\n",
    "            tmp_X_train, tmp_y_train, tmp_X_val, tmp_y_val = remove_identifiers(\n",
    "                area_X_train[area], area_y_train[area], area_X_val[area], area_y_val[area])\n",
    "            tmp_X_train, tmp_y_train = tmp_X_train.to_numpy(), tmp_y_train.to_numpy()\n",
    "            tmp_X_val, tmp_y_val = tmp_X_val.to_numpy(), tmp_y_val.to_numpy()\n",
    "            area_X_train[area] = tmp_X_train\n",
    "            area_X_val[area] = tmp_X_val\n",
    "            area_y_train[area] = tmp_y_train\n",
    "            area_y_val[area] = tmp_y_val\n",
    "    \n",
    "    if exogenous_data_train is not None:\n",
    "        for area in exogenous_data_train:\n",
    "            exogenous_data_train[area] = exogenous_data_train[area].to_numpy()\n",
    "            exogenous_data_val[area] = exogenous_data_val[area].to_numpy()\n",
    "    \n",
    "    # remove identifiers from features, targets\n",
    "    X_train, y_train, X_val, y_val = remove_identifiers(X_train, y_train, X_val, y_val)\n",
    "    assert len(X_train.columns) == len(X_val.columns)\n",
    "    \n",
    "    num_features = len(X_train.columns) // args.num_lags\n",
    "    \n",
    "    # to timeseries representation\n",
    "    X_train = to_timeseries_rep(X_train.to_numpy(), num_lags=args.num_lags,\n",
    "                                            num_features=num_features)\n",
    "    X_val = to_timeseries_rep(X_val.to_numpy(), num_lags=args.num_lags,\n",
    "                                          num_features=num_features)\n",
    "    \n",
    "    if area_X_train is not None:\n",
    "        area_X_train = to_timeseries_rep(area_X_train, num_lags=args.num_lags,\n",
    "                                                     num_features=num_features)\n",
    "        area_X_val = to_timeseries_rep(area_X_val, num_lags=args.num_lags,\n",
    "                                                   num_features=num_features)\n",
    "    \n",
    "    # transform targets to numpy\n",
    "    y_train, y_val = y_train.to_numpy(), y_val.to_numpy()\n",
    "    \n",
    "    if exogenous_data_train is not None:\n",
    "        exogenous_data_train_combined, exogenous_data_val_combined = [], []\n",
    "        for area in exogenous_data_train:\n",
    "            exogenous_data_train_combined.extend(exogenous_data_train[area])\n",
    "            exogenous_data_val_combined.extend(exogenous_data_val[area])\n",
    "        exogenous_data_train_combined = np.stack(exogenous_data_train_combined)\n",
    "        exogenous_data_val_combined = np.stack(exogenous_data_val_combined)\n",
    "        exogenous_data_train[\"all\"] = exogenous_data_train_combined\n",
    "        exogenous_data_val[\"all\"] = exogenous_data_val_combined\n",
    "    return X_train, X_val, y_train, y_val, area_X_train, area_X_val, area_y_train, area_y_val, exogenous_data_train, exogenous_data_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "938e146f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val, client_X_train, client_X_val, client_y_train, client_y_val, exogenous_data_train, exogenous_data_val = make_postprocessing(X_train, X_val, y_train, y_val, exogenous_data_train, exogenous_data_val, x_scalers, y_scalers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5b2619b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Client: upstream\n",
      "X_train shape: (3881, 10, 9, 1), y_train shape: (3881, 6)\n",
      "X_val shape: (962, 10, 9, 1), y_val shape: (962, 6)\n",
      "\n",
      "Client: midstream\n",
      "X_train shape: (3934, 10, 9, 1), y_train shape: (3934, 6)\n",
      "X_val shape: (976, 10, 9, 1), y_val shape: (976, 6)\n",
      "\n",
      "Client: downstream\n",
      "X_train shape: (3926, 10, 9, 1), y_train shape: (3926, 6)\n",
      "X_val shape: (974, 10, 9, 1), y_val shape: (974, 6)\n"
     ]
    }
   ],
   "source": [
    "for client in client_X_train:\n",
    "    print(f\"\\nClient: {client}\")\n",
    "    print(f\"X_train shape: {client_X_train[client].shape}, y_train shape: {client_y_train[client].shape}\")\n",
    "    print(f\"X_val shape: {client_X_val[client].shape}, y_val shape: {client_y_val[client].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "98cb5123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_dims(X_train, exogenous_data_train):\n",
    "    if args.model_name == \"mlp\":\n",
    "        input_dim = X_train.shape[1] * X_train.shape[2]\n",
    "    else:\n",
    "        input_dim = X_train.shape[2]\n",
    "    \n",
    "    if exogenous_data_train is not None:\n",
    "        if len(exogenous_data_train) == 1:\n",
    "            cid = next(iter(exogenous_data_train.keys()))\n",
    "            exogenous_dim = exogenous_data_train[cid].shape[1]\n",
    "        else:\n",
    "            exogenous_dim = exogenous_data_train[\"all\"].shape[1]\n",
    "    else:\n",
    "        exogenous_dim = 0\n",
    "    \n",
    "    return input_dim, exogenous_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4ef2bade",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_features=11, lags=10, out_dim=5,\n",
    "                 exogenous_dim: int = 0,\n",
    "                 in_channels=[1, 16],\n",
    "                 out_channels=[16, 32],\n",
    "                 kernel_sizes=[(2, 3), (5, 3)],\n",
    "                 pool_kernel_sizes=[(2, 1)]):\n",
    "        super(CNN, self).__init__()\n",
    "        assert len(in_channels) == len(out_channels) == len(kernel_sizes)\n",
    "        self.activation = torch.nn.Tanh()\n",
    "        self.num_lags = lags\n",
    "        self.num_features = num_features\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=in_channels[0], out_channels=out_channels[0],\n",
    "                                     kernel_size=kernel_sizes[0], padding=\"same\")\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=in_channels[1], out_channels=out_channels[1],\n",
    "                                     kernel_size=kernel_sizes[1], padding=\"same\")\n",
    "        self.pool = torch.nn.AvgPool2d(kernel_size=pool_kernel_sizes[0])\n",
    "        kernel0, kernel1 = pool_kernel_sizes[-1][0], pool_kernel_sizes[-1][1]\n",
    "        self.fc = torch.nn.Linear(\n",
    "            in_features=(out_channels[1] * int(lags / kernel0) * int(num_features / kernel1)) + exogenous_dim,\n",
    "            out_features=out_dim)\n",
    "\n",
    "    def forward(self, x, exogenous_data=None, device=None, y_hist=None):\n",
    "        if len(x.shape) > 2:\n",
    "            x = x.view(x.size(0), x.size(3), x.size(1), x.size(2))\n",
    "        else:\n",
    "            x = x.view(x.size(0), 1, self.num_lags, self.num_features,)\n",
    "        x = self.conv1(x)  # [batch_size]\n",
    "        x = self.activation(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # concatenate conv output with exogenous data\n",
    "        if exogenous_data is not None and len(exogenous_data) > 0:\n",
    "            x = torch.cat((x, exogenous_data), dim=1)\n",
    "\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8d338b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "83db8ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "CNN(\n  (activation): Tanh()\n  (conv1): Conv2d(1, 16, kernel_size=(2, 3), stride=(1, 1), padding=same)\n  (conv2): Conv2d(16, 32, kernel_size=(5, 3), stride=(1, 1), padding=same)\n  (pool): AvgPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0)\n  (fc): Linear(in_features=1760, out_features=5, bias=True)\n)"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "35f72279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, X_train, y_train, X_val, y_val, \n",
    "        exogenous_data_train=None, exogenous_data_val=None, \n",
    "        idxs=[0,1,3,7,8,2], # the indices of our targets in X\n",
    "        log_per=1,\n",
    "        client_creation_fn = None, # client specification\n",
    "        local_train_params=None, # local params\n",
    "        aggregation_params=None, # aggregation params\n",
    "        use_carbontracker=False\n",
    "       ):\n",
    "    # client creation definition\n",
    "    if client_creation_fn is None:\n",
    "        client_creation_fn = create_regression_client\n",
    "    # local params\n",
    "    if local_train_params is None:\n",
    "        local_train_params = {\n",
    "            \"epochs\": args.epochs, \"optimizer\": args.optimizer, \"lr\": args.lr,\n",
    "            \"criterion\": args.criterion, \"early_stopping\": args.local_early_stopping,\n",
    "            \"patience\": args.local_patience, \"device\": device,\n",
    "            \"reg1\": args.reg1, \"reg2\": args.reg2,\n",
    "            \"max_grad_norm\": args.max_grad_norm\n",
    "        }\n",
    "    \n",
    "    train_loaders, val_loaders = [], []\n",
    "    \n",
    "    # get data per client\n",
    "    for client in X_train:\n",
    "        if client == \"all\":\n",
    "            continue\n",
    "        if exogenous_data_train is not None:\n",
    "            tmp_exogenous_data_train = exogenous_data_train[client]\n",
    "            tmp_exogenous_data_val = tmp_exogenous_data_val[client]\n",
    "        else:\n",
    "            tmp_exogenous_data_train = None\n",
    "            tmp_exogenous_data_val = None\n",
    "    \n",
    "        num_features = len(X_train[client][0][0])\n",
    "        \n",
    "        # to torch loader\n",
    "        train_loaders.append(\n",
    "            to_torch_dataset(\n",
    "                X_train[client], y_train[client],\n",
    "                num_lags=args.num_lags,\n",
    "                num_features=num_features,\n",
    "                exogenous_data=tmp_exogenous_data_train,\n",
    "                indices=idxs,\n",
    "                batch_size=args.batch_size,\n",
    "                shuffle=False\n",
    "            )\n",
    "        )\n",
    "        val_loaders.append(\n",
    "            to_torch_dataset(\n",
    "                X_val[client], y_val[client],\n",
    "                num_lags=args.num_lags,\n",
    "                exogenous_data=tmp_exogenous_data_val,\n",
    "                indices=idxs,\n",
    "                batch_size=args.batch_size,\n",
    "                shuffle=False\n",
    "            )\n",
    "            \n",
    "        )\n",
    "        \n",
    "    # create clients with their local data\n",
    "    cids = [k for k in X_train.keys() if k != \"all\"]\n",
    "    clients = [\n",
    "        client_creation_fn(\n",
    "            cid=cid, # client id\n",
    "            model=model, # the global model\n",
    "            train_loader=train_loader, # the local train loader\n",
    "            test_loader=val_loader, # the local val loader\n",
    "            local_params=local_train_params # local parameters\n",
    "        )\n",
    "        for cid, train_loader, val_loader in zip(cids, train_loaders, val_loaders)\n",
    "    ]\n",
    "    \n",
    "    # represent clients to server\n",
    "    client_proxies = [\n",
    "        SimpleClientProxy(cid, client) for cid, client in zip(cids, clients)\n",
    "    ]\n",
    "    \n",
    "    # represent the server\n",
    "    server = Server(\n",
    "        client_proxies=client_proxies, # the client representations\n",
    "        aggregation=args.aggregation, # the aggregation algorithm\n",
    "        aggregation_params=aggregation_params, # aggregation specific params\n",
    "        local_params_fn=None, # we can change the local params on demand\n",
    "    )\n",
    "    # Note that the client manager instance will be initialized automatically. You can define your own client manager.\n",
    "\n",
    "    # train with FL\n",
    "    model_params, history = server.fit(args.fl_rounds, args.fraction, use_carbontracker=use_carbontracker)\n",
    "    \n",
    "    params_dict = zip(model.state_dict().keys(), model_params)\n",
    "    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "    model = copy.deepcopy(model)\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "de341140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# federated local params\n",
    "local_train_params = {\n",
    "            \"epochs\": args.epochs, \"optimizer\": args.optimizer, \"lr\": args.lr,\n",
    "            \"criterion\": args.criterion, \"early_stopping\": args.local_early_stopping,\n",
    "            \"patience\": args.local_patience, \"device\": device,\n",
    "            \"reg1\": args.reg1, \"reg2\": args.reg2,\n",
    "            \"max_grad_norm\": args.max_grad_norm\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ee6fef34",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO logger 2024-06-03 09:50:47,164 | server.py:62 | Initializing client manager...\n",
      "INFO logger 2024-06-03 09:50:47,166 | server.py:69 | Registering clients...\n",
      "INFO logger 2024-06-03 09:50:47,166 | client_manager.py:66 | Registered client with id: upstream\n",
      "INFO logger 2024-06-03 09:50:47,166 | client_manager.py:66 | Registered client with id: midstream\n",
      "INFO logger 2024-06-03 09:50:47,167 | client_manager.py:66 | Registered client with id: downstream\n",
      "INFO logger 2024-06-03 09:50:47,167 | server.py:73 | Client manager initialized!\n",
      "INFO logger 2024-06-03 09:50:47,168 | server.py:55 | Aggregation algorithm: FedNova(rho=0.0)\n",
      "INFO logger 2024-06-03 09:50:47,168 | client_manager.py:88 | Parameter c=0.0. Sampled 1 client(s): ['midstream']\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (128x1440 and 1760x5)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[52], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m global_model, history \u001B[38;5;241m=\u001B[39m \u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mclient_X_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mclient_y_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mclient_X_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mclient_y_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlocal_train_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_train_params\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[50], line 90\u001B[0m, in \u001B[0;36mfit\u001B[1;34m(model, X_train, y_train, X_val, y_val, exogenous_data_train, exogenous_data_val, idxs, log_per, client_creation_fn, local_train_params, aggregation_params, use_carbontracker)\u001B[0m\n\u001B[0;32m     81\u001B[0m server \u001B[38;5;241m=\u001B[39m Server(\n\u001B[0;32m     82\u001B[0m     client_proxies\u001B[38;5;241m=\u001B[39mclient_proxies, \u001B[38;5;66;03m# the client representations\u001B[39;00m\n\u001B[0;32m     83\u001B[0m     aggregation\u001B[38;5;241m=\u001B[39margs\u001B[38;5;241m.\u001B[39maggregation, \u001B[38;5;66;03m# the aggregation algorithm\u001B[39;00m\n\u001B[0;32m     84\u001B[0m     aggregation_params\u001B[38;5;241m=\u001B[39maggregation_params, \u001B[38;5;66;03m# aggregation specific params\u001B[39;00m\n\u001B[0;32m     85\u001B[0m     local_params_fn\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;66;03m# we can change the local params on demand\u001B[39;00m\n\u001B[0;32m     86\u001B[0m )\n\u001B[0;32m     87\u001B[0m \u001B[38;5;66;03m# Note that the client manager instance will be initialized automatically. You can define your own client manager.\u001B[39;00m\n\u001B[0;32m     88\u001B[0m \n\u001B[0;32m     89\u001B[0m \u001B[38;5;66;03m# train with FL\u001B[39;00m\n\u001B[1;32m---> 90\u001B[0m model_params, history \u001B[38;5;241m=\u001B[39m \u001B[43mserver\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfl_rounds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfraction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_carbontracker\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_carbontracker\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     92\u001B[0m params_dict \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mzip\u001B[39m(model\u001B[38;5;241m.\u001B[39mstate_dict()\u001B[38;5;241m.\u001B[39mkeys(), model_params)\n\u001B[0;32m     93\u001B[0m state_dict \u001B[38;5;241m=\u001B[39m OrderedDict({k: torch\u001B[38;5;241m.\u001B[39mTensor(v) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m params_dict})\n",
      "File \u001B[1;32mD:\\Work\\Federated_Time_Series_Forecasting_Water_Quality\\ml\\fl\\server\\server.py:84\u001B[0m, in \u001B[0;36mServer.fit\u001B[1;34m(self, num_rounds, fraction, fraction_args, use_carbontracker)\u001B[0m\n\u001B[0;32m     80\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Run federated rounds for num_rounds rounds.\"\"\"\u001B[39;00m\n\u001B[0;32m     82\u001B[0m history \u001B[38;5;241m=\u001B[39m History()\n\u001B[1;32m---> 84\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate_round\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfl_round\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhistory\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhistory\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     86\u001B[0m log(INFO, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStarting FL rounds\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     87\u001B[0m cb_tracker \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Work\\Federated_Time_Series_Forecasting_Water_Quality\\ml\\fl\\server\\server.py:217\u001B[0m, in \u001B[0;36mServer.evaluate_round\u001B[1;34m(self, fl_round, history)\u001B[0m\n\u001B[0;32m    215\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m cid, client_proxy \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient_manager\u001B[38;5;241m.\u001B[39mall()\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m--> 217\u001B[0m         num_train_instances, train_loss, train_eval_metrics \u001B[38;5;241m=\u001B[39m \u001B[43mclient_proxy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mglobal_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    218\u001B[0m \u001B[43m                                                                                    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    220\u001B[0m         num_train_examples\u001B[38;5;241m.\u001B[39mappend(num_train_instances)\n\u001B[0;32m    221\u001B[0m         train_losses[cid] \u001B[38;5;241m=\u001B[39m train_loss\n",
      "File \u001B[1;32mD:\\Work\\Federated_Time_Series_Forecasting_Water_Quality\\ml\\fl\\client_proxy.py:43\u001B[0m, in \u001B[0;36mSimpleClientProxy.evaluate\u001B[1;34m(self, data, model, params, method, verbose)\u001B[0m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mevaluate\u001B[39m(\u001B[38;5;28mself\u001B[39m, data: Optional[DataLoader] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m     40\u001B[0m              model: Optional[Union[torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mModule, List[np\u001B[38;5;241m.\u001B[39mndarray]]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m     41\u001B[0m              params: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, method: Optional[\u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, verbose: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m     42\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Global model evaluation.\"\"\"\u001B[39;00m\n\u001B[1;32m---> 43\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Work\\Federated_Time_Series_Forecasting_Water_Quality\\ml\\fl\\torch_client.py:144\u001B[0m, in \u001B[0;36mTorchRegressionClient.evaluate\u001B[1;34m(self, data, model, params, method, verbose)\u001B[0m\n\u001B[0;32m    141\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m data \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m method \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    142\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_loader\n\u001B[1;32m--> 144\u001B[0m loss, mse, rmse, mae, r2, nrmse \u001B[38;5;241m=\u001B[39m \u001B[43mtest\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnet\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcriterion\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    146\u001B[0m metrics \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMSE\u001B[39m\u001B[38;5;124m\"\u001B[39m: mse, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRMSE\u001B[39m\u001B[38;5;124m\"\u001B[39m: rmse, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMAE\u001B[39m\u001B[38;5;124m\"\u001B[39m: mae, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mR^2\u001B[39m\u001B[38;5;124m\"\u001B[39m: r2, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNRMSE\u001B[39m\u001B[38;5;124m\"\u001B[39m: nrmse}\n\u001B[0;32m    148\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m verbose:\n",
      "File \u001B[1;32mD:\\Work\\Federated_Time_Series_Forecasting_Water_Quality\\ml\\utils\\train_utils.py:161\u001B[0m, in \u001B[0;36mtest\u001B[1;34m(model, data, criterion, device)\u001B[0m\n\u001B[0;32m    159\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    160\u001B[0m     exogenous \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 161\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexogenous\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_hist\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    162\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m criterion \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    163\u001B[0m     loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m criterion(out, y)\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[1;32mD:\\Program\\anaconda\\envs\\fed\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Program\\anaconda\\envs\\fed\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[47], line 40\u001B[0m, in \u001B[0;36mCNN.forward\u001B[1;34m(self, x, exogenous_data, device, y_hist)\u001B[0m\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m exogenous_data \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(exogenous_data) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m     38\u001B[0m     x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat((x, exogenous_data), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m---> 40\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[1;32mD:\\Program\\anaconda\\envs\\fed\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Program\\anaconda\\envs\\fed\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Program\\anaconda\\envs\\fed\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    115\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 116\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (128x1440 and 1760x5)"
     ]
    }
   ],
   "source": [
    "global_model, history = fit(\n",
    "    model,\n",
    "    client_X_train,\n",
    "    client_y_train, \n",
    "    client_X_val, \n",
    "    client_y_val, \n",
    "    local_train_params=local_train_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fdc4fa",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de13ada4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de1695e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_preds(y_pred_train, y_pred_val):\n",
    "    if not isinstance(y_pred_train, np.ndarray):\n",
    "        y_pred_train = y_pred_train.cpu().numpy()\n",
    "    if not isinstance(y_pred_val, np.ndarray):\n",
    "        y_pred_val = y_pred_val.cpu().numpy()\n",
    "    return y_pred_train, y_pred_val\n",
    "\n",
    "def round_predictions(y_pred_train, y_pred_val, dims):\n",
    "    # round to closest integer\n",
    "    if dims is None or len(dims) == 0:\n",
    "        return y_pred_train, y_pred_val\n",
    "    for dim in dims:\n",
    "        y_pred_train[:, dim] = np.rint(y_pred_train[:, dim])\n",
    "        y_pred_val[:, dim] = np.rint(y_pred_val[:, dim])\n",
    "    return y_pred_train, y_pred_val\n",
    "\n",
    "def inverse_transform(y_train, y_val, y_pred_train, y_pred_val,\n",
    "                     y_scaler=None, \n",
    "                     round_preds=False, dims=None):\n",
    "    y_pred_train, y_pred_val = transform_preds(y_pred_train, y_pred_val)\n",
    "    \n",
    "    if y_scaler is not None:\n",
    "        y_train = y_scaler.inverse_transform(y_train)\n",
    "        y_val = y_scaler.inverse_transform(y_val)\n",
    "        y_pred_train = y_scaler.inverse_transform(y_pred_train)\n",
    "        y_pred_val = y_scaler.inverse_transform(y_pred_val)\n",
    "    \n",
    "    # to zeroes\n",
    "    y_pred_train[y_pred_train < 0.] = 0.\n",
    "    y_pred_val[y_pred_val < 0.] = 0.\n",
    "    \n",
    "    if round_preds:\n",
    "        y_pred_train, y_pred_val = round_predictions(y_pred_train, y_pred_val, dims)\n",
    "    \n",
    "    return y_train, y_val, y_pred_train, y_pred_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0a7e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plot(y_true, y_pred, \n",
    "              title, \n",
    "              feature_names=None, \n",
    "              client=None):\n",
    "    if feature_names is None:\n",
    "        feature_names = [f\"feature_{i}\" for i in range(y_pred.shape[1])]\n",
    "    assert len(feature_names) == y_pred.shape[1]\n",
    "\n",
    "    for i in range(y_pred.shape[1]):\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.ticklabel_format(style='plain')\n",
    "        plt.plot(y_true[:, i], label=\"Actual\")\n",
    "        plt.plot(y_pred[:, i], label=\"Predicted\")\n",
    "        if client is not None:\n",
    "            plt.title(f\"[{client} {title}] {feature_names[i]} prediction\")\n",
    "        else:\n",
    "            plt.title(f\"[{title}] {feature_names[i]} prediction\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "55843a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(\n",
    "    model, # the global model\n",
    "    client_X_train, # train data per client\n",
    "    client_y_train,\n",
    "    client_X_val, # val data per client\n",
    "    client_y_val,\n",
    "    exogenous_data_train, # exogenous data per client\n",
    "    exogenous_data_val,\n",
    "    y_scalers, # the scaler used to transform the targets\n",
    "    idxs=[0,1,3,7,8,2], # the indices of our targets in X\n",
    "    apply_round=True, # round to closest integer\n",
    "    round_dimensions=[0, 3, 4], # the dimensions to apply rounding\n",
    "    plot=True, # plot predictions\n",
    "):\n",
    "    # load per client data to torch\n",
    "    train_loaders, val_loaders = [], []\n",
    "    \n",
    "    # get data per client\n",
    "    for client in client_X_train:\n",
    "        if client == \"all\":\n",
    "            continue\n",
    "        assert client in list(y_scalers.keys())\n",
    "        if exogenous_data_train is not None:\n",
    "            tmp_exogenous_data_train = exogenous_data_train[client]\n",
    "            tmp_exogenous_data_val = exogenous_data_val[client]\n",
    "        else:\n",
    "            tmp_exogenous_data_train = None\n",
    "            tmp_exogenous_data_val = None\n",
    "    \n",
    "        num_features = len(client_X_train[client][0][0])\n",
    "        \n",
    "        # to torch loader\n",
    "        train_loaders.append(\n",
    "            to_torch_dataset(\n",
    "                client_X_train[client], client_y_train[client],\n",
    "                num_lags=args.num_lags,\n",
    "                num_features=num_features,\n",
    "                exogenous_data=tmp_exogenous_data_train,\n",
    "                indices=idxs,\n",
    "                batch_size=1,\n",
    "                shuffle=False\n",
    "            )\n",
    "        )\n",
    "        val_loaders.append(\n",
    "            to_torch_dataset(\n",
    "                client_X_val[client], client_y_val[client],\n",
    "                num_lags=args.num_lags,\n",
    "                exogenous_data=tmp_exogenous_data_val,\n",
    "                indices=idxs,\n",
    "                batch_size=1,\n",
    "                shuffle=False\n",
    "            )\n",
    "            \n",
    "        )\n",
    "        \n",
    "    # get client ids\n",
    "    cids = [k for k in client_X_train.keys() if k != \"all\"]\n",
    "        \n",
    "    # predict per client using the global model\n",
    "    y_preds_train, y_preds_val = dict(), dict()\n",
    "    for cid, train_loader, val_loader in zip(cids, train_loaders, val_loaders):\n",
    "        print(f\"Prediction on {cid}\")\n",
    "        train_mse, train_rmse, train_mae, train_r2, train_nrmse, y_pred_train = test(\n",
    "            model, train_loader, None, device=device\n",
    "        )\n",
    "        val_mse, val_rmse, val_mae, val_r2, val_nrmse, y_pred_val = test(\n",
    "            model, val_loader, None, device=device\n",
    "        )\n",
    "        y_preds_train[cid] = y_pred_train\n",
    "        y_preds_val[cid] = y_pred_val\n",
    "    \n",
    "    for cid in cids:\n",
    "        y_train, y_val = client_y_train[cid], client_y_val[cid]\n",
    "        y_pred_train, y_pred_val = y_preds_train[cid], y_preds_val[cid]\n",
    "        \n",
    "        y_scaler = y_scalers[cid]\n",
    "        y_train, y_val, y_pred_train, y_pred_val = inverse_transform(\n",
    "            y_train, y_val, y_pred_train, y_pred_val,\n",
    "            y_scaler, round_preds=apply_round, dims=round_dimensions\n",
    "        )\n",
    "        train_mse, train_rmse, train_mae, train_r2, train_nrmse, train_res_per_dim = accumulate_metric(\n",
    "            y_train, y_pred_train, True, return_all=True\n",
    "        )\n",
    "        val_mse, val_rmse, val_mae, val_r2, val_nrmse, val_res_per_dim = accumulate_metric(\n",
    "            y_val, y_pred_val, True, return_all=True\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nFinal Prediction on {cid} (Inference Stage)\")\n",
    "        print(f\"[Train]: mse: {train_mse}, \"\n",
    "              f\"rmse: {train_rmse}, mae {train_mae}, r2: {train_r2}, nrmse: {train_nrmse}\")\n",
    "        print(f\"[Val]: mse: {val_mse}, \"\n",
    "              f\"rmse: {val_rmse}, mae {val_mae}, r2: {val_r2}, nrmse: {val_nrmse}\\n\\n\")\n",
    "        \n",
    "        if plot:\n",
    "            make_plot(y_train, y_pred_train, title=\"Train\", feature_names=args.targets, client=cid)\n",
    "            make_plot(y_val, y_pred_val, title=\"Val\", feature_names=args.targets, client=cid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ca7aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(\n",
    "    global_model,\n",
    "    client_X_train, \n",
    "    client_y_train,\n",
    "    client_X_val, \n",
    "    client_y_val,\n",
    "    exogenous_data_train, \n",
    "    exogenous_data_val,\n",
    "    y_scalers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "25a4582d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<function __main__.inference(model, client_X_train, client_y_train, client_X_val, client_y_val, exogenous_data_train, exogenous_data_val, y_scalers, idxs=[0, 1, 3, 7, 8, 2], apply_round=True, round_dimensions=[0, 3, 4], plot=True)>"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8fe3b4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "a39106e1a9d6d153b7400628e7589ff266b5caee5b0db427f0903be982155882"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
